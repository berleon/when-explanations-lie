{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Explanations Lie: Why Modified BP Attribution fails\n",
    "\n",
    "This notebook produces the cosine similaries of the relevance vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install install packages\n",
    "# !pip install tensorflow-gpu==1.13.1\n",
    "# !pip install innvestigate seaborn tqdm deeplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "import innvestigate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import copy\n",
    "import contextlib\n",
    "import datetime\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage.measure import compare_ssim \n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "from innvestigate.analyzer.relevance_based.relevance_analyzer import LRP\n",
    "from innvestigate.analyzer.base import AnalyzerNetworkBase, ReverseAnalyzerBase\n",
    "from innvestigate.analyzer.deeptaylor import DeepTaylor\n",
    "from innvestigate.analyzer import DeepLIFTWrapper\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "from when_explanations_lie import *\n",
    "from monkey_patch_lrp_resnet import custom_add_bn_rule, get_custom_rule\n",
    "import deeplift_resnet  \n",
    "from deeplift_resnet import DeepLiftRelevanceReplacer\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to imagenet validation\n",
    "\n",
    "host = ! hostname\n",
    "host = host[0]\n",
    "\n",
    "imagenet_dir = {\n",
    " \"morty\": \"/mnt/ssd/data/imagenet/imagenet-raw\",\n",
    " \"snuffles\": \"/srv/public/leonsixt/data/imagenet\",\n",
    "}[host]\n",
    "#imagenet_val_dir = \"/home/leonsixt/tmp/imagenet/imagenet-raw/validation/\"\n",
    "# path to examplary image\n",
    "ex_image_path = \"n01534433/ILSVRC2012_val_00015410.JPEG\"\n",
    "# number of images to run the evaluation\n",
    "#n_selected_imgs = 10\n",
    "n_selected_imgs = 200\n",
    "\n",
    "load_weights = True\n",
    "model_names = ['cifar10', 'vgg16', 'resnet50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls -l cache/csc_200_2020-01-24T10:02:02.563084\n",
    "        \n",
    "cache_dir = 'cache/csc_200_2020-01-26T22:19:11.601426'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if 'cache_dir' not in globals():\n",
    "    cache_dir = 'cache/csc_200_' + datetime.datetime.utcnow().isoformat()\n",
    "    os.makedirs(cache_dir)\n",
    "    \n",
    "print(\"results will be saved in: \", cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_meta(model_name, load_weights=True, clear_session=True):\n",
    "    if clear_session:\n",
    "        keras.backend.clear_session()\n",
    "    if model_name in ['vgg16', 'resnet50']:\n",
    "        model, innv_net, color_conversion = load_model(model_name, load_weights) \n",
    "        meta = ImageNetMeta(model, model_name, innv_net, n_selected_imgs, \n",
    "                            imagenet_dir, ex_image_path)\n",
    "    elif model_name == 'cifar10':\n",
    "        model, _, _ = load_model('cifar10', load_weights)\n",
    "        meta = CIFAR10Meta(model, n_selected_imgs)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return model, meta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, meta = load_model_and_meta('vgg16', load_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmap_postprocess_wrapper(name):\n",
    "    return lambda x: heatmap_postprocess(name, x)\n",
    "\n",
    "input_range = (meta.ex_image.min(), meta.ex_image.max())\n",
    "analysers = get_analyser_params(input_range)\n",
    "\n",
    "attr_names = [n for (n, _, _, _, _) in analysers]\n",
    "    \n",
    "hmap_postprocessing = {\n",
    "    n: hmap_postprocess_wrapper(post_name) for n, _, post_name, _, _ in analysers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.9, 10).tolist() + [0.99, 0.999, 0.9999, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reversed(hidden):\n",
    "    return [h[1] for h in hidden[1:]]\n",
    "\n",
    "\n",
    "dead_neuron_mask = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    model, meta = load_model_and_meta(model_name)\n",
    "    analyser = innvestigate.create_analyzer(\n",
    "        \"gradient\", model, reverse_keep_tensors=True)\n",
    "    \n",
    "    analyser.analyze(np.concatenate([img for (img, _) in meta.images[:20]], 0))\n",
    "    \n",
    "    grad_hidden = parse_reversed(analyser._reversed_tensors) \n",
    "    dead_neuron_mask[model_name] = [(0 == np.mean(g, 0, keepdims=True)).all(-1, keepdims=True) for g in grad_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    plt.title(model_name + \" - inactive neurons\")\n",
    "    plt.plot([(m.sum(-1) / m.shape[-1] > 0.999999).mean() \n",
    "              for m in dead_neuron_mask[model_name]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities_from_relevances(relevance_per_layers):\n",
    "    cos_sims = []\n",
    "    for rel_per_layer in relevance_per_layers:\n",
    "        rel_per_layer = [conv_as_matrix(r[None]) for r in rel_per_layer]\n",
    "\n",
    "        cos_sims.append(pairwise_cosine_similarity(rel_per_layer).flatten())\n",
    "    return cos_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_as_matrix(x):\n",
    "    if len(x.shape) == 2:\n",
    "        return x\n",
    "    if len(x.shape) == 3:\n",
    "        x = x[None]\n",
    "    b, h, w, c = x.shape\n",
    "    return np.reshape(x, (b*h*w, c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replacement_class(analyser_cls):\n",
    "    assert issubclass(analyser_cls, ReverseAnalyzerBase)\n",
    "    class ReplaceBackward(analyser_cls):\n",
    "        def __init__(self, model, *args, **kwargs):\n",
    "            kwargs['reverse_keep_tensors'] = True\n",
    "            #kwargs['reverse_verbose'] = True\n",
    "            super().__init__(model, *args, **kwargs)\n",
    "        \n",
    "        def _create_analysis(self, *args, **kwargs):\n",
    "            outputs, relevances_per_layer = super()._create_analysis(*args, **kwargs)\n",
    "            # self._relevances_per_layer = relevances_per_layer[::-1]\n",
    "            # \n",
    "            # self._not_associated = set()\n",
    "            # \n",
    "            #     \n",
    "            # self._id_to_reversed_tensor = OrderedDict()\n",
    "            # for tens, info in self._reversed_tensors_raw.items():\n",
    "            #     self._id_to_reversed_tensor[info['id']] = info['final_tensor']\n",
    "       # \n",
    "    # \n",
    "            # self._layer_to_id_reversed_output = OrderedDict(\n",
    "            #     [reversed(i) for i in self._id_reversed_output_to_layer.items()])\n",
    "        # \n",
    "            # self._layer_to_id_reversed_input = OrderedDict()\n",
    "            # for _id, layers in self._id_reversed_input_to_layers.items():\n",
    "            #     for layer in layers:\n",
    "            #         self._layer_to_id_reversed_input[layer] = _id\n",
    "            return outputs, relevances_per_layer\n",
    "        \n",
    "        def _get_layer_idx(self, name):\n",
    "            layer = self._model.get_layer(name=name)\n",
    "            return self._model.layers.index(layer)\n",
    "        \n",
    "        def get_relevances(self, input_value, relevance_value,  \n",
    "                           set_layer, output_layers):\n",
    "            \"\"\"\n",
    "            return relevance values\n",
    "            \"\"\"\n",
    "            sess = keras.backend.get_session()\n",
    "            inp = self._analyzer_model.inputs[0]\n",
    "            \n",
    "            def parse_input_output(desc):\n",
    "                if type(desc) == tuple:\n",
    "                    layer_name, input_or_output = desc\n",
    "                else:\n",
    "                    layer_name = desc\n",
    "                    input_or_output = 'output'\n",
    "\n",
    "                if type(input_or_output) == str:\n",
    "                    input_or_output = (input_or_output, 0)\n",
    "                    return layer_name, input_or_output\n",
    "            \n",
    "            def get_rel_tensor(layer_name, input_or_output):\n",
    "                layer = self._model.get_layer(name=layer_name)\n",
    "                if input_or_output[0] == 'input':\n",
    "                    if type(layer.input) != list:\n",
    "                        forward_tens = layer.input\n",
    "                    else:\n",
    "                        forward_tens = layer.input[input_or_output[1]]\n",
    "                else:\n",
    "                    if type(layer.output) != list:\n",
    "                        forward_tens = layer.output\n",
    "                    else:\n",
    "                        forward_tens = layer.output[input_or_output[1]]\n",
    "                    \n",
    "                return self._reversed_tensors_raw[forward_tens]['final_tensor']\n",
    "            \n",
    "            set_layer_name, set_input_or_output = parse_input_output(set_layer)\n",
    "            \n",
    "            output_layers = [parse_input_output(n) for n in output_layers]\n",
    "            rel_tensor = get_rel_tensor(set_layer_name, set_input_or_output)\n",
    "            \n",
    "            output_rel_tensors = [get_rel_tensor(*o) for o in output_layers]\n",
    "        \n",
    "            output_relevances = sess.run(\n",
    "                output_rel_tensors,\n",
    "                feed_dict={ \n",
    "                    inp: input_value,\n",
    "                    rel_tensor: relevance_value\n",
    "                })\n",
    "            return output_relevances \n",
    "        \n",
    "    return ReplaceBackward \n",
    "\n",
    "\n",
    "def get_replacement_analyser(model, analyser_cls, **kwargs):\n",
    "    if type(analyser_cls) == str:\n",
    "        analyser_cls = innvestigate.analyzer.analyzers[analyser_cls]\n",
    "    replacement_cls = create_replacement_class(analyser_cls)\n",
    "    \n",
    "    return replacement_cls(model, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    for model_name in model_names:\n",
    "        print(model_name)\n",
    "        model, meta = load_model_and_meta(model_name)\n",
    "        from innvestigate.analyzer import GuidedBackprop\n",
    "        gb_repl = create_replacement_class(GuidedBackprop)(model)\n",
    "        gb_repl.create_analyzer_model()\n",
    "\n",
    "        for layer_name in meta.csc_replacement_layers:\n",
    "            layer = gb_repl._model.get_layer(name=layer_name)\n",
    "\n",
    "            relv_shape = layer.output.shape.as_list()\n",
    "\n",
    "            layer_idx = gb_repl._model.layers.index(layer)\n",
    "\n",
    "            n = 2\n",
    "            relvs = gb_repl.get_relevances(\n",
    "                input_value=np.repeat(meta.ex_image, n, axis=0), \n",
    "                relevance_value=np.random.normal(size=[n, ] + relv_shape[1:]),\n",
    "                set_layer=layer_name, \n",
    "                output_layers=[(model.layers[0].name, 'output')],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLiftRelevanceReplacer:\n",
    "    def __init__(self, deeplift_wrapper):\n",
    "        self.deeplift_wrapper = deeplift_wrapper\n",
    "        if not hasattr(self.deeplift_wrapper, \"_deep_lift_func\"): \n",
    "            self.deeplift_wrapper._create_deep_lift_func()\n",
    "        self.model = self.deeplift_wrapper._deeplift_model\n",
    "        self.layers = list(self.model._name_to_layer.values())\n",
    "        self.layer_names = list(self.model._name_to_layer.keys())\n",
    "        self.input_layer = self.layers[0]\n",
    "       \n",
    "    def _get_layer_idx(self, name):\n",
    "        deeplift_name = name + '_0'\n",
    "        layer_names = list(self.model._name_to_layer.keys())\n",
    "        return layer_names.index(deeplift_name)\n",
    "    \n",
    "    def get_relevances(self, input_value,  relevance_value,\n",
    "                       set_layer, output_layers, reference=None):\n",
    "        \n",
    "        def parse_input_output(desc):\n",
    "            if type(desc) == tuple:\n",
    "                layer_name, input_or_output = desc\n",
    "            else:\n",
    "                layer_name = desc\n",
    "                input_or_output = 'output'\n",
    "\n",
    "            if type(input_or_output) == str:\n",
    "                input_or_output = (input_or_output, 0)\n",
    "                return layer_name, input_or_output\n",
    "            \n",
    "        def run_single(single_image, single_relevance_value, single_reference):\n",
    "            sess = keras.backend.get_session()\n",
    "            return sess.run(\n",
    "                [self.layers[idx]._target_contrib_vars \n",
    "                 for idx in selected_layer_idxs], \n",
    "                feed_dict={\n",
    "                    self.input_layer.get_activation_vars(): single_image,\n",
    "                    self.input_layer.get_reference_vars(): single_reference,\n",
    "                    changed_layer._pos_mxts: single_relevance_value,\n",
    "                    changed_layer._neg_mxts: single_relevance_value, \n",
    "                })\n",
    "            \n",
    "        set_layer_idx = self._get_layer_idx(set_layer)\n",
    "        changed_layer = self.layers[set_layer_idx]\n",
    "        selected_layer_idxs = [self._get_layer_idx(name) for name in output_layers]\n",
    "        \n",
    "        if reference is None:\n",
    "            reference = np.zeros_like(input_value)\n",
    "            \n",
    "        self.layers[-1].set_active()\n",
    "        \n",
    "        aggregated_contribs = [[] for _ in selected_layer_idxs]\n",
    "        for i in range(len(input_value)):\n",
    "            contribs = run_single(\n",
    "                input_value[i:i+1],\n",
    "                relevance_value[i:i+1],\n",
    "                reference[i:i+1],\n",
    "            )\n",
    "            for i, cont in enumerate(contribs):\n",
    "                aggregated_contribs[i].append(cont)\n",
    "                \n",
    "        self.layers[-1].set_inactive()\n",
    "        \n",
    "        return [np.concatenate(contrib) for contrib in aggregated_contribs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift_resnet import monkey_patch_deeplift_neg_pos_mxts\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    for model_name in model_names[1:2]:\n",
    "        print(model_name)\n",
    "        model, meta = load_model_and_meta(model_name)\n",
    "        with monkey_patch_deeplift_neg_pos_mxts(cross_mxts=False):\n",
    "            dp_lift = DeepLIFTWrapper(model)\n",
    "            deeplift_csc = DeepLiftRelevanceReplacer(dp_lift)\n",
    "            n = 2\n",
    "            for layer_name in meta.csc_replacement_layers[:1]:\n",
    "\n",
    "                layer = model.get_layer(name=layer_name)\n",
    "\n",
    "                relv_shape = layer.output.shape.as_list()\n",
    "\n",
    "                relvs = deeplift_csc.get_relevances(\n",
    "                    input_value=np.repeat(meta.ex_image, n, axis=0), \n",
    "                    set_layer=layer_name, \n",
    "                    relevance_value=np.random.normal(size=[n, ] + relv_shape[1:]),\n",
    "                    output_layers=[meta.names.to_raw(n) for n in meta.names.nice_names()],\n",
    "                )\n",
    "                print([np.median(c) for c in cosine_similarities_from_relevances(relvs)])\n",
    "                # for i in range(len(relvs[0])):\n",
    "                #     plt.imshow(relvs[0][i].sum(-1))\n",
    "                #     plt.colorbar()\n",
    "                #     plt.show()\n",
    "                \n",
    "                plt.imshow(relvs[0][0].sum(-1))\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "                plt.imshow(relvs[0][1].sum(-1))\n",
    "                plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, innv_name, _, excludes, kwargs in analysers:\n",
    "    print(label, innv_name, get_custom_rule(innv_name, kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from when_explanations_lie import mpl_styles\n",
    "\n",
    "input_range = (meta.ex_image.min(), meta.ex_image.max())\n",
    "analysers = get_analyser_params(input_range)\n",
    "\n",
    "attr_names = [n for (n, _, _, _, _) in analysers]\n",
    "print(attr_names)\n",
    "\n",
    "for i, (name, style) in enumerate(mpl_styles.items()):\n",
    "    assert name in attr_names\n",
    "    plt.plot(np.arange(10), [20-i] * 10, \n",
    "             #markersize=5,\n",
    "             label=name, #+ \" m=\" + style['marker'], \n",
    "             **style)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacement_layers= {'vgg16': ['fc3'], 'resnet50': ['dense']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(attr_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_names = ['vgg16', 'resnet50', 'cifar10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift_resnet import monkey_patch_deeplift_neg_pos_mxts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement_analyser(model, analyser_cls, **kwargs):\n",
    "    if type(analyser_cls) == str:\n",
    "        analyser_cls = innvestigate.analyzer.analyzers[analyser_cls]\n",
    "    replacement_cls = create_replacement_class(analyser_cls)\n",
    "    \n",
    "    return replacement_cls(model, **kwargs)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def ctx_replacement_analyzer(model, meta, innv_name, kwargs):\n",
    "    if innv_name.startswith(\"pattern\"):\n",
    "        kwargs['patterns'] = meta.patterns\n",
    "\n",
    "    if innv_name == 'deep_lift.wrapper':\n",
    "        kwargs = copy.copy(kwargs)\n",
    "        cross_mxts = kwargs.pop('cross_mxts', True)\n",
    "        with monkey_patch_deeplift_neg_pos_mxts(cross_mxts):\n",
    "            analyser = DeepLIFTWrapper(model, **kwargs)\n",
    "            repl_analyser = DeepLiftRelevanceReplacer(analyser)\n",
    "            yield repl_analyser\n",
    "    else:\n",
    "        custom_rule = get_custom_rule(innv_name, kwargs)\n",
    "        with custom_add_bn_rule(custom_rule):\n",
    "            repl_analyser = get_replacement_analyser(\n",
    "                model, innv_name, **kwargs)\n",
    "            repl_analyser.create_analyzer_model()\n",
    "            yield repl_analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off\n",
    "# replacement_layer_indices = [22]\n",
    "n_sampled_v = 5\n",
    "\n",
    "cos_sim_histograms = OrderedDict()\n",
    "cos_mean = OrderedDict()\n",
    "selected_percentiles = [0, 1, 5, 10, 20, 50, 80, 95, 99, 100]\n",
    "cos_sim_percentiles = OrderedDict()\n",
    "override_results = False\n",
    "\n",
    "selected_attr_names = attr_names\n",
    "selected_attr_names = [\n",
    "    'PatternNet',\n",
    "    'DeepLIFT Abla.',\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    model, meta = load_model_and_meta(model_name, load_weights, clear_session=True)\n",
    "    input_range = (meta.ex_image.min(), meta.ex_image.max())\n",
    "    analysers = get_analyser_params(input_range)\n",
    "        \n",
    "    for attr_name, innv_name, _, excludes, kwargs in tqdm.tqdm_notebook(analysers[::-1]):\n",
    "        if attr_name not in selected_attr_names:\n",
    "            continue\n",
    "            \n",
    "        if 'exclude_cos_sim' in excludes:\n",
    "            continue\n",
    "        if 'exclude_' + model_name in excludes:\n",
    "            continue\n",
    "        \n",
    "        fname = os.path.join(cache_dir, \"csc_{}_{}.pickle\".format(model_name, attr_name))\n",
    "        if os.path.exists(fname) and not override_results:\n",
    "            warnings.warn(\"Results already exists at: \\n\"\n",
    "                          \"{}\\nUse override_results=True to replace\".format(fname))\n",
    "            continue\n",
    "        model, meta = load_model_and_meta(model_name, load_weights, clear_session=True)\n",
    "        \n",
    "        selected_layers = [meta.names.to_raw(nice_name) \n",
    "                           for nice_name in meta.names.nice_names()]\n",
    "        \n",
    "        with ctx_replacement_analyzer(model, meta, innv_name, kwargs) as repl_analyser:\n",
    "            for repl_layer_raw in meta.csc_replacement_layers:\n",
    "                repl_shape = model.get_layer(name=repl_layer_raw).output_shape\n",
    "\n",
    "                cos_per_img = OrderedDict()\n",
    "\n",
    "                lower_layers = list(itertools.takewhile(lambda n: n != repl_layer_raw, selected_layers))\n",
    "                relevance_layers = lower_layers + [repl_layer_raw]\n",
    "                for img_idx, (img, _) in tqdm.tqdm_notebook(\n",
    "                    zip(meta.image_indices, meta.images),  \n",
    "                    desc=\"[{}.{}] {}\".format(model_name, meta.names.to_nice(repl_layer_raw), attr_name), \n",
    "                    total=len(meta.images)): \n",
    "                    channels = repl_shape[-1]\n",
    "\n",
    "                    img_tiled = np.repeat(img, n_sampled_v, axis=0)\n",
    "                    random_relevance = np.random.normal(size=(n_sampled_v, ) + repl_shape[1:]) \n",
    "\n",
    "                    relevances = repl_analyser.get_relevances(\n",
    "                        img_tiled, random_relevance, \n",
    "                        set_layer=repl_layer_raw, \n",
    "                        output_layers=lower_layers)\n",
    "            \n",
    "                    cos_sim = cosine_similarities_from_relevances(\n",
    "                        relevances + [random_relevance])\n",
    "                    for layer_raw, cs_for_layer in zip(relevance_layers, cos_sim):\n",
    "                        cos_per_img[model_name, layer_raw, img_idx] = np.abs(cs_for_layer)\n",
    "\n",
    "                median_for_label = []\n",
    "                percentile_for_label = OrderedDict([(p, []) for p in selected_percentiles])\n",
    "                \n",
    "                mean_idx = (attr_name, model_name, repl_layer_raw)\n",
    "                cos_mean[mean_idx] = []\n",
    "                for layer_raw in relevance_layers:\n",
    "                    cos_per_layer = np.concatenate([cos_per_img[model_name, layer_raw, img_idx]  \n",
    "                                                    for img_idx in meta.image_indices])\n",
    "                    cos_per_layer = cos_per_layer.flatten()\n",
    "\n",
    "                    # we filter nans as they appear when the gradients are zero\n",
    "                    cos_mean[mean_idx].append(np.nanmean(cos_per_layer))\n",
    "\n",
    "                    cos_per_layer = cos_per_layer[~np.isnan(cos_per_layer)]\n",
    "\n",
    "                    if len(cos_per_layer) == 0:\n",
    "                        raise ValueError()\n",
    "                        #import pdb\n",
    "                        #pdb.set_trace()\n",
    "                        cos_per_layer = np.array([np.nan])\n",
    "\n",
    "\n",
    "                    perc_values = np.percentile(cos_per_layer,  selected_percentiles)\n",
    "                    for p, val in zip(selected_percentiles, perc_values):\n",
    "                        percentile_for_label[p].append(val)\n",
    "\n",
    "                    if layer_raw in meta.csc_histogram_layers:\n",
    "                        if len(cos_per_layer) > 50000:\n",
    "                            ridx = np.random.choice(len(cos_per_layer), 50000, replace=False)\n",
    "                            cos_per_layer_sel = cos_per_layer[ridx]\n",
    "                        else:\n",
    "                            cos_per_layer_sel = cos_per_layer\n",
    "\n",
    "                        idx = (attr_name, model_name, repl_layer_raw, layer_raw)\n",
    "                        cos_sim_histograms[idx] = np.histogram(cos_per_layer_sel, bins)\n",
    "\n",
    "\n",
    "                cos_mean[mean_idx] = np.array(cos_mean[mean_idx])\n",
    "                for p, values in percentile_for_label.items():\n",
    "                    cos_sim_percentiles[attr_name, model_name, repl_layer_raw, p] = np.array(values)\n",
    "                \n",
    "        assert not os.path.exists(fname)\n",
    "        with open(fname, 'wb') as f:\n",
    "            def filter_dict(dictionary):\n",
    "                return OrderedDict([\n",
    "                    (k, v) for k, v in dictionary.items()\n",
    "                    if k[0] == attr_name and k[1] == model_name])\n",
    "            \n",
    "            m = filter_dict(cos_mean)\n",
    "            perc = filter_dict(cos_sim_percentiles)\n",
    "            hist = filter_dict(cos_sim_histograms)\n",
    "            print('saving at {}, m{}, cs{}, h{}'.format(fname, len(m), len(perc), len(hist)))\n",
    "            pickle.dump((m, perc, hist), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False\n",
    "if save_results:\n",
    "    with open(os.path.join(cache_dir, 'all.pickle'), 'wb') as f:\n",
    "        pickle.dump((cos_mean, cos_sim_percentiles, cos_sim_histograms), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l  'cache/csc_200_2020-01-26T22:19:11.601426'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_results = True\n",
    "# cache_dir = 'cache/csc_200_2020-01-26T22:19:11.601426'\n",
    "\n",
    "if load_results:\n",
    "    if input(\"Do you really want to replace the current results?\") != \"y\":\n",
    "        raise Exception()\n",
    "    cos_sim_histograms = OrderedDict()\n",
    "    cos_mean = OrderedDict()\n",
    "    cos_sim_percentiles = OrderedDict()\n",
    "\n",
    "    for filename in tqdm.tqdm_notebook(os.listdir(cache_dir)):\n",
    "        with open(os.path.join(cache_dir, filename), 'rb') as f:\n",
    "            mean, prec, hist  = pickle.load(f)\n",
    "            cos_mean.update(mean)\n",
    "            cos_sim_percentiles.update(prec)\n",
    "            cos_sim_histograms.update(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(U, V):\n",
    "    v_norm = V / np.linalg.norm(V, axis=0, keepdims=True)\n",
    "    u_norm = U / np.linalg.norm(U, axis=0, keepdims=True)\n",
    "    return v_norm.T @ u_norm\n",
    "\n",
    "def get_sample_cos_sim_per_layer(output_shapes):\n",
    "    values = []\n",
    "    for layer_idx, shp in output_shapes.items():\n",
    "        ch = shp[-1]\n",
    "        n_samples = 1000\n",
    "        u = np.random.normal(size=(ch, n_samples))\n",
    "        v = np.random.normal(size=(ch, n_samples))\n",
    "        cos = cosine_similarity(v, u)\n",
    "        mask = np.tri(cos.shape[0])\n",
    "        values.append(np.median(np.abs(cos[mask == 1])))\n",
    "    return np.array(values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_baseline = {}\n",
    "\n",
    "model_metas = {}\n",
    "for model_name in model_names:\n",
    "    model, meta = load_model_and_meta(model_name)\n",
    "    model_metas[model_name] = meta\n",
    "    output_shapes = get_output_shapes(model)\n",
    "    print(len(output_shapes))\n",
    "    cos_sim_baseline[model_name] = get_sample_cos_sim_per_layer(output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_baseline['vgg16'].shape, cos_sim_baseline['resnet50'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cos_sim_percentiles.keys())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_in_cos_sim = set()\n",
    "attr_names_in_cos_sim = set()\n",
    "for (label, model_name, replacement_layer, percentile), values in cos_sim_percentiles.items():\n",
    "    if model_name == 'vgg16':\n",
    "        attr_names_in_cos_sim.add(label)\n",
    "    model_names_in_cos_sim.add(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_in_cos_sim, attr_names_in_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['vgg16', 'cifar10', 'resnet50']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metas['cifar10'].csc_replacement_layers\n",
    "\n",
    "model_metas['cifar10'].csc_replacement_layers\n",
    "\n",
    "\n",
    "csc_shown_layers = {}\n",
    "csc_shown_layers['cifar10'] = ['input', 'conv1', 'conv2', 'pool2', \n",
    "                               'conv3', 'conv4', 'pool4', 'fc5', 'fc6']\n",
    "\n",
    "cifar_nice = model_metas['cifar10'].names.nice_names()\n",
    "csc_shown_layers['cifar10'] = [(cifar_nice.index(name), name) for name in csc_shown_layers['cifar10']]\n",
    "# csc_shown_layers['cifar10'] = \n",
    "csc_shown_layers['resnet50'] = list(enumerate(model_metas['resnet50'].names.nice_names()))\n",
    "csc_shown_layers['vgg16'] = list(enumerate(model_metas['vgg16'].names.nice_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csc_shown_layers['cifar10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_order(attr_name):\n",
    "    if attr_name.startswith(\"LRP CMP\"):\n",
    "        return 1\n",
    "    elif attr_name.startswith(\"Pattern\"):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "analysers = get_analyser_params([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_save = True\n",
    "\n",
    "def plot_convergence(meta, replacement_layer, \n",
    "                     metrics, include_cos_sim_baseline=False, log=False, save=True, save_marker=None):\n",
    "    def handle_log(data):\n",
    "        if log:\n",
    "            return 1 - data\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    legend = OrderedDict()\n",
    "    model_name = meta.model_name\n",
    "    repl_idx = meta.names.raw_to_idx(replacement_layer)\n",
    "    start_layer = meta.n_layers - repl_idx \n",
    "\n",
    "\n",
    "    selected_layers = [name for _, name in csc_shown_layers[model_name]\n",
    "                       if meta.names.nice_to_idx(name) <= repl_idx][::-1]\n",
    "    selected_idx = [idx for idx, name in csc_shown_layers[model_name]\n",
    "                       if meta.names.nice_to_idx(name) <= repl_idx][::-1]\n",
    "    layer_idx = np.array([meta.names.nice_to_idx(name) \n",
    "                          for name in selected_layers])\n",
    "\n",
    "    plt.figure(figsize=(max(3, len(layer_idx) / 3.8), 2.7)) #3.5))\n",
    "\n",
    "\n",
    "    for i, (label, cos_sim_per_label) in enumerate(metrics):\n",
    "        cos_sim_per_label = cos_sim_per_label[selected_idx] #[::-1]\n",
    "\n",
    "        style = copy.copy(mpl_styles[label])\n",
    "        plt.plot(-0.5 + np.arange(len(cos_sim_per_label)), handle_log(cos_sim_per_label), \n",
    "                 label=label, zorder=draw_order(label), **style)\n",
    "\n",
    "        if label not in legend:\n",
    "            legend[label] = mpl_styles[label]\n",
    "\n",
    "    # Random Cos Similarity\n",
    "    # Cos Similarity Base.\n",
    "    if include_cos_sim_baseline:\n",
    "        label='Cos Similarity BL'\n",
    "        style = {'color': (0.25, 0.25, 0.25)}\n",
    "        plt.plot(-0.5 + np.arange(len(layer_idx)), cos_sim_baseline[model_name][layer_idx], \n",
    "                 # label='Cos. Sim. Baseline', \n",
    "                 label=label,\n",
    "                 **style)\n",
    "        if label not in legend:\n",
    "            legend[label] = style\n",
    "\n",
    "    #plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.ylabel('cosine similarity')\n",
    "    plt.xticks(np.arange(len(selected_layers)), selected_layers, rotation=90)\n",
    "    #plt.ylim(-0.05, 1.05)\n",
    "    plt.grid('on', alpha=0.35) #, axis=\"y\")\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "        print(plt.ylim())\n",
    "        ymin, ymax = plt.ylim()\n",
    "        ymin = max(ymin, 1e-8)\n",
    "        plt.ylim(ymin, 1.05)\n",
    "        locs, labels = plt.yticks()\n",
    "        locs = [l for l in locs if ymin < l < 1]\n",
    "        plt.yticks(locs, labels=[\"1 - {:.0e}\".format(l).replace(\"0\", \"\") for l in locs])\n",
    "        #            labels=[\"0\", \"1 - 1e-2\", \"1 - 1e-4\", \"1 - 1e-6\", \"1 - 1e-8\"])\n",
    "        plt.gca().invert_yaxis()\n",
    "    else:\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "    log_str = \"log\" if log else \"linear\"\n",
    "    \n",
    "    if save and global_save:\n",
    "        marker = model_name\n",
    "        if save_marker is not None:\n",
    "            marker += \"_\" + save_marker\n",
    "        outdir = \"./figures/cosine_similarity/{}/\".format(model_name)\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        fname = os.path.join(outdir, \"{}_layer_{}_{}.pdf\".format(\n",
    "            marker, repl_idx, log_str))\n",
    "        plt.savefig(fname, bbox_inches='tight', pad_inches=0.01)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        return legend, fname\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return legend, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.names.nice_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from when_explanations_lie import mpl_styles\n",
    "\n",
    "def filter_metric(metric, model_name, attr_names, replacement_layer):\n",
    "    metrics = []\n",
    "    for attr_name in attr_names:\n",
    "        try:\n",
    "            idx = (attr_name, model_name, replacement_layer)\n",
    "            metrics.append((attr_name, metric[idx]))\n",
    "        except KeyError:\n",
    "            warnings.warn(\"not found: \" + str(idx))\n",
    "            continue\n",
    "    return metrics\n",
    "\n",
    "meta = model_metas['cifar10']\n",
    "plot_convergence(meta, 'fc6', filter_metric(cos_mean, 'cifar10', attr_names, 'fc6'), log=True)\n",
    "\n",
    "\n",
    "meta = model_metas['vgg16']\n",
    "plot_convergence(meta, 'dense_1', filter_metric(cos_mean, 'vgg16', attr_names, 'dense_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([p for (l, m, r, p), v in cos_sim_percentiles.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cos_mean.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from when_explanations_lie import mpl_styles\n",
    "\n",
    "\n",
    "def plot_legend(legend, marker='all'):\n",
    "    plt.figure(figsize=(2.5, 3))\n",
    "    for label, style in legend.items():\n",
    "        plt.plot([], label=label, alpha=1, **legend[label])\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.legend(loc='center', labelspacing=0.33)\n",
    "    plt.savefig(\"./figures/cosine_similarity/cos_sim_legend_{}.pdf\".format(marker),\n",
    "                bbox_inches='tight', pad_inches=0.02,\n",
    "               )\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "os.makedirs('figures/cosine_similarity', exist_ok=True)\n",
    "\n",
    "cos_sim_median = {(l, m, r) : v for (l, m, r, p), v in cos_sim_percentiles.items()\n",
    "                  if p == 50}\n",
    "\n",
    "metric = cos_sim_median\n",
    "\n",
    "selected_attr_names = [n for n in attr_names if not n.startswith(\"LRP CMP\")]\n",
    "lrp_cmp_names = [\n",
    "    'LRP $\\\\alpha1\\\\beta0$',\n",
    "    'LRP $\\\\alpha2\\\\beta1$',\n",
    "    'LRP-z',\n",
    "    'LRP CMP $\\\\alpha1\\\\beta0$',\n",
    "    'LRP CMP $\\\\alpha2\\\\beta1$',\n",
    "    'Gradient'\n",
    "]\n",
    "\n",
    "figpaths = []\n",
    "for log in [True, False]:\n",
    "    include_cos_sim_baseline = not log\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        meta = model_metas[model_name]\n",
    "        replacement_layer = meta.csc_replacement_layers[0]\n",
    "\n",
    "        metrics = filter_metric(metric, model_name, lrp_cmp_names, replacement_layer)\n",
    "        legend, figpath = plot_convergence(meta, replacement_layer, metrics,  \n",
    "                                           log=log, save_marker='lrp_cmp')\n",
    "        print(legend)\n",
    "        plot_legend(legend, model_name + \"_lrp_cmp\")\n",
    "        figpaths.append(figpath)\n",
    "\n",
    "    all_model_legend = OrderedDict()\n",
    "    for model_name in model_names:\n",
    "        meta = model_metas[model_name]\n",
    "        print(\"meta\", meta.model_name)\n",
    "        for replacement_layer in meta.csc_replacement_layers:\n",
    "            metrics = filter_metric(metric, model_name, selected_attr_names, replacement_layer)\n",
    "            legend, figpath = plot_convergence(meta, replacement_layer, metrics, log=log)\n",
    "            figpaths.append(figpath)\n",
    "            all_model_legend.update(legend)\n",
    "        plot_legend(legend, model_name)\n",
    "\n",
    "    plot_legend(all_model_legend, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cos_mean.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = ! ls figures/cosine_similarity/\n",
    "print(figures)\n",
    "for figure in figures:\n",
    "    display(IFrame(\"figures/cosine_similarity/\" + figure, 800, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_keys(x, select_key_values):\n",
    "    for keys, val in x.items():\n",
    "        should_yield = True\n",
    "        for i, sel_key_value in enumerate(select_key_values):\n",
    "            if sel_key_value is None:\n",
    "                continue\n",
    "            if sel_key_value != keys[i]:\n",
    "                should_yield = False\n",
    "        if should_yield:\n",
    "            yield keys, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(select_keys(cos_sim_histograms, ['Gradient', 'vgg16', 'dense_1', None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_hist(x):\n",
    "    (attr_name, model_name, repl_layer, layer_name) = x[0]\n",
    "    return  attr_names.index(attr_name)\n",
    "\n",
    "prev_layer_name = None\n",
    "n_plots = 0\n",
    "\n",
    "\n",
    "\n",
    "def plot_histograms(histograms):\n",
    "    legend = OrderedDict()\n",
    "    attr_counts = []\n",
    "    labels = []\n",
    "    for (attr_name, model_name, repl_layer, layer_name), (counts, bins) in sorted(\n",
    "        histograms.items(), key=sort_hist, reverse=True):\n",
    "\n",
    "        lower_09 = counts[bins[:-1] < 0.9].sum()\n",
    "        #print(attr_name, counts.sum())\n",
    "        # print(attr_name, repl_layer, layer_name)\n",
    "        counts_collapsed = np.concatenate([lower_09[None], counts[bins[:-1] >= 0.9]])\n",
    "        bins_int = np.arange(len(counts_collapsed) + 1)\n",
    "        attr_counts.append(counts_collapsed)\n",
    "        labels.append(attr_name)\n",
    "        \n",
    "    plt.figure(figsize=(3., 2.5))\n",
    "    \n",
    "    \n",
    "    color = [mpl_styles[l]['color'] for l in labels] \n",
    "    _, _, patches = plt.hist(\n",
    "        [bins_int[:-1]] * len(attr_counts), bins_int, \n",
    "        weights=attr_counts, stacked=True, label=labels,  \n",
    "        color=color, \n",
    "        rwidth=0.9,\n",
    "        edgecolor='black', linewidth=1.2,\n",
    "    )\n",
    "    xticks = [\"{:.4g}\".format(b) for b in [0] + bins[bins >= 0.9].tolist()]\n",
    "    plt.xticks(bins_int, xticks, rotation=90)\n",
    "    print(bins_int, xticks)\n",
    "    \n",
    "    hatches = ['////',  '...', '\\\\\\\\\\\\\\\\',  'xxxx','OO', 'xxx', '**'] * 4\n",
    "    color_to_hatch = {}\n",
    "    for label, patch_set, color in reversed(list(zip(labels, patches, color))):\n",
    "        if color not in color_to_hatch:\n",
    "            hatch = None # hatches[0]\n",
    "            color_to_hatch[color] = 0\n",
    "        else:\n",
    "            hatch = hatches[color_to_hatch[color]]\n",
    "            color_to_hatch[color] += 1\n",
    "            \n",
    "        for patch in patch_set.patches:\n",
    "            patch.set_hatch(hatch)\n",
    "        legend[label] = {'hatch': hatch, 'color': color}\n",
    "    plt.yticks([])\n",
    "    #plt.legend(bbox_to_anchor=[1, 1])\n",
    "    sns.despine(left=True)\n",
    "    return legend\n",
    "    \n",
    "legend = OrderedDict()\n",
    "for model_name in model_names:\n",
    "    meta = model_metas[model_name]\n",
    "    repl_layer = meta.csc_replacement_layers[0]\n",
    "    hist_layer = meta.csc_histogram_layers[-2]\n",
    "    hists = OrderedDict(select_keys(\n",
    "        cos_sim_histograms, [None, model_name, repl_layer, hist_layer]))\n",
    "    \n",
    "    legend.update(plot_histograms(hists))\n",
    "    outdir = 'figures/csc_hists/'\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    \n",
    "    figpath =os.path.join(outdir, \"hist_{}_repl_{}_hist_{}.pdf\".format(\n",
    "        model_name, \n",
    "        meta.names.to_nice(repl_layer),\n",
    "        meta.names.to_nice(hist_layer)))\n",
    "    plt.savefig(figpath, bbox_inches='tight', pad_inches=0.01)\n",
    "    print(figpath)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2.5, 3))\n",
    "for label in attr_names:\n",
    "    if label not in legend:\n",
    "        continue\n",
    "    plt.hist([], label=label, alpha=1, **legend[label])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend(loc='center', fontsize='medium', frameon=True, handlelength=3, labelspacing=0.33)\n",
    "\n",
    "fname = 'figures/csc_hists/csc_hist_legend.pdf'\n",
    "plt.savefig(fname, bbox_inches='tight', pad_inches=0.01)\n",
    "print(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model, innv_net, _ = load_model('resnet50', load_weights=load_weights)\n",
    "model_output_shapes = get_output_shapes(model)\n",
    "\n",
    "selected_layers = [layer_names[model_name].to_raw(nice_name) \n",
    "                   for nice_name in layer_names[model_name].nice_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = DeepLIFTWrapper(model, **kwargs)\n",
    "repl_analyser = DeepLiftRelevanceReplacer(analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
