{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1670531162322445123, name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 17636946189283059842\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 16013109625234788861\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10330544538\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6116984006460521846\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From /home/mi/leonsixt/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "WARNING:tensorflow:From /home/mi/leonsixt/.conda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "10000/10000 [==============================] - 2s 227us/step\n",
      "Test loss: 10.512189151000976\n",
      "Test accuracy: 0.1342\n",
      "Saved trained model at /home/mi/leonsixt/sanity-checks-modified-bp/when-explanations-lie/saved_models/keras_cifar10_random_model_acc1342.h5 \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', name='conv1',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu', name='relu1'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', name='conv2'))\n",
    "model.add(Activation('relu', name='relu2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)), name='pool2')    #  8\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', name='conv3'))\n",
    "model.add(Activation('relu', name='relu3'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', name='conv4'))\n",
    "model.add(Activation('relu', name='relu1'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='pool2'))   \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5, name='dropout1'))\n",
    "model.add(Dense(1024, name='fc1'))\n",
    "model.add(Activation('relu', name='relu1'))\n",
    "model.add(Dropout(0.5, name='dropout2'))\n",
    "model.add(Dense(num_classes, name='fc2'))\n",
    "model.add(Activation('softmax', name='softmax'))\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "\n",
    "scores_without_point = \"{:.4f}\".format(scores[1])[2:]\n",
    "model_name = 'keras_cifar10_random_model_acc{}.h5'.format(\n",
    "    scores_without_point)\n",
    "\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_1\n",
      "activation_1\n",
      "conv2d_2\n",
      "activation_2\n",
      "max_pooling2d_1\n",
      "conv2d_3\n",
      "activation_3\n",
      "conv2d_4\n",
      "activation_4\n",
      "max_pooling2d_2\n",
      "flatten_1\n",
      "dropout_1\n",
      "dense_1\n",
      "activation_5\n",
      "dropout_2\n",
      "dense_2\n",
      "activation_6\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "WARNING:tensorflow:From /home/mi/leonsixt/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 1.5332 - acc: 0.4412 - val_loss: 1.1802 - val_acc: 0.5745\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.1519 - acc: 0.5880 - val_loss: 0.9087 - val_acc: 0.6760\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.0120 - acc: 0.6411 - val_loss: 0.8185 - val_acc: 0.7133\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.9398 - acc: 0.6688 - val_loss: 0.8002 - val_acc: 0.7224\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.8889 - acc: 0.6907 - val_loss: 0.7221 - val_acc: 0.7479\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.8521 - acc: 0.7035 - val_loss: 0.7118 - val_acc: 0.7508\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.8251 - acc: 0.7116 - val_loss: 0.7618 - val_acc: 0.7375\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.8013 - acc: 0.7223 - val_loss: 0.6806 - val_acc: 0.7692\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.7837 - acc: 0.7282 - val_loss: 0.6599 - val_acc: 0.7779\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.7626 - acc: 0.7337 - val_loss: 0.6947 - val_acc: 0.7764\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7596 - acc: 0.7350 - val_loss: 0.6532 - val_acc: 0.7780\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.7430 - acc: 0.7424 - val_loss: 0.6401 - val_acc: 0.7848\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7329 - acc: 0.7462 - val_loss: 0.6560 - val_acc: 0.7815\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.7234 - acc: 0.7501 - val_loss: 0.6284 - val_acc: 0.7872\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.7111 - acc: 0.7557 - val_loss: 0.6041 - val_acc: 0.7959\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.7012 - acc: 0.7571 - val_loss: 0.5977 - val_acc: 0.8001\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7058 - acc: 0.7598 - val_loss: 0.5999 - val_acc: 0.7962\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6978 - acc: 0.7625 - val_loss: 0.5851 - val_acc: 0.8023\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6936 - acc: 0.7606 - val_loss: 0.6947 - val_acc: 0.7646\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6817 - acc: 0.7690 - val_loss: 0.6126 - val_acc: 0.7936\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6781 - acc: 0.7663 - val_loss: 0.5793 - val_acc: 0.8063\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6824 - acc: 0.7650 - val_loss: 0.6637 - val_acc: 0.7784\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6741 - acc: 0.7702 - val_loss: 0.5916 - val_acc: 0.8073\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6720 - acc: 0.7693 - val_loss: 0.6483 - val_acc: 0.7842\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6660 - acc: 0.7705 - val_loss: 0.5806 - val_acc: 0.8081\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6664 - acc: 0.7699 - val_loss: 0.5874 - val_acc: 0.8019\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6579 - acc: 0.7727 - val_loss: 0.6087 - val_acc: 0.8015\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6531 - acc: 0.7770 - val_loss: 0.6144 - val_acc: 0.7942\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6637 - acc: 0.7728 - val_loss: 0.5541 - val_acc: 0.8157\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6473 - acc: 0.7785 - val_loss: 0.5755 - val_acc: 0.8094\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6494 - acc: 0.7780 - val_loss: 0.5633 - val_acc: 0.8104\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6470 - acc: 0.7794 - val_loss: 0.5761 - val_acc: 0.8096\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6364 - acc: 0.7812 - val_loss: 0.5534 - val_acc: 0.8167\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6392 - acc: 0.7819 - val_loss: 0.5701 - val_acc: 0.8061\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6377 - acc: 0.7821 - val_loss: 0.5710 - val_acc: 0.8117\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6434 - acc: 0.7798 - val_loss: 0.5711 - val_acc: 0.8149\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6358 - acc: 0.7830 - val_loss: 0.5440 - val_acc: 0.8196\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6379 - acc: 0.7824 - val_loss: 0.5826 - val_acc: 0.8058\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6442 - acc: 0.7797 - val_loss: 0.5750 - val_acc: 0.8073\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6278 - acc: 0.7870 - val_loss: 0.5525 - val_acc: 0.8186\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6361 - acc: 0.7811 - val_loss: 0.5864 - val_acc: 0.8055\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 15s 10ms/step - loss: 0.6335 - acc: 0.7857 - val_loss: 0.5277 - val_acc: 0.8268\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6230 - acc: 0.7895 - val_loss: 0.6144 - val_acc: 0.7947\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6252 - acc: 0.7874 - val_loss: 0.5833 - val_acc: 0.8052\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6232 - acc: 0.7873 - val_loss: 0.5616 - val_acc: 0.8199\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6209 - acc: 0.7916 - val_loss: 0.5735 - val_acc: 0.8152\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6238 - acc: 0.7875 - val_loss: 0.5730 - val_acc: 0.8156\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6236 - acc: 0.7887 - val_loss: 0.5302 - val_acc: 0.8235\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6246 - acc: 0.7897 - val_loss: 0.5822 - val_acc: 0.8089\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6269 - acc: 0.7872 - val_loss: 0.5617 - val_acc: 0.8130\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6235 - acc: 0.7886 - val_loss: 0.5969 - val_acc: 0.8031\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6201 - acc: 0.7890 - val_loss: 0.5835 - val_acc: 0.8124\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6245 - acc: 0.7877 - val_loss: 0.5607 - val_acc: 0.8145\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6180 - acc: 0.7909 - val_loss: 0.5613 - val_acc: 0.8143\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6224 - acc: 0.7890 - val_loss: 0.5561 - val_acc: 0.8226\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6149 - acc: 0.7912 - val_loss: 0.5323 - val_acc: 0.8291\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6131 - acc: 0.7924 - val_loss: 0.5738 - val_acc: 0.8155\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6142 - acc: 0.7907 - val_loss: 0.5568 - val_acc: 0.8201\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6200 - acc: 0.7895 - val_loss: 0.5692 - val_acc: 0.8150\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6083 - acc: 0.7940 - val_loss: 0.5606 - val_acc: 0.8200\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6107 - acc: 0.7948 - val_loss: 0.5399 - val_acc: 0.8267\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6096 - acc: 0.7934 - val_loss: 0.5595 - val_acc: 0.8158\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6102 - acc: 0.7936 - val_loss: 0.5131 - val_acc: 0.8356\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6151 - acc: 0.7931 - val_loss: 0.5457 - val_acc: 0.8190\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6133 - acc: 0.7917 - val_loss: 0.5447 - val_acc: 0.8256\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6150 - acc: 0.7925 - val_loss: 0.5438 - val_acc: 0.8233\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6031 - acc: 0.7940 - val_loss: 0.5082 - val_acc: 0.8358\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6042 - acc: 0.7950 - val_loss: 0.5396 - val_acc: 0.8251\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6114 - acc: 0.7948 - val_loss: 0.5497 - val_acc: 0.8225\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6192 - acc: 0.7899 - val_loss: 0.6183 - val_acc: 0.7964\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6171 - acc: 0.7933 - val_loss: 0.5380 - val_acc: 0.8325\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6062 - acc: 0.7960 - val_loss: 0.5742 - val_acc: 0.8118\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6172 - acc: 0.7943 - val_loss: 0.5674 - val_acc: 0.8138\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6025 - acc: 0.7959 - val_loss: 0.5678 - val_acc: 0.8175\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6045 - acc: 0.7960 - val_loss: 0.5472 - val_acc: 0.8240\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6148 - acc: 0.7913 - val_loss: 0.5475 - val_acc: 0.8218\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6190 - acc: 0.7918 - val_loss: 0.5368 - val_acc: 0.8238\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6052 - acc: 0.7965 - val_loss: 0.5509 - val_acc: 0.8217\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6034 - acc: 0.7955 - val_loss: 0.5345 - val_acc: 0.8338\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6083 - acc: 0.7933 - val_loss: 0.5687 - val_acc: 0.8142\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6033 - acc: 0.7960 - val_loss: 0.5262 - val_acc: 0.8290\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.5998 - acc: 0.7979 - val_loss: 0.5652 - val_acc: 0.8139\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6104 - acc: 0.7971 - val_loss: 0.5300 - val_acc: 0.8308\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.5985 - acc: 0.7988 - val_loss: 0.5413 - val_acc: 0.8233\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6026 - acc: 0.7976 - val_loss: 0.5431 - val_acc: 0.8279\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6084 - acc: 0.7966 - val_loss: 0.5679 - val_acc: 0.8140\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6041 - acc: 0.7970 - val_loss: 0.5547 - val_acc: 0.8251\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6095 - acc: 0.7971 - val_loss: 0.5641 - val_acc: 0.8153\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.6034 - acc: 0.7979 - val_loss: 0.5321 - val_acc: 0.8275\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6071 - acc: 0.7960 - val_loss: 0.5256 - val_acc: 0.8338\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6033 - acc: 0.7969 - val_loss: 0.5356 - val_acc: 0.8330\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.5969 - acc: 0.7995 - val_loss: 0.5440 - val_acc: 0.8280\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6033 - acc: 0.7965 - val_loss: 0.5150 - val_acc: 0.8333\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6020 - acc: 0.7988 - val_loss: 0.5726 - val_acc: 0.8097\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6075 - acc: 0.7975 - val_loss: 0.5329 - val_acc: 0.8256\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 15s 10ms/step - loss: 0.6059 - acc: 0.7964 - val_loss: 0.5235 - val_acc: 0.8313\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6074 - acc: 0.7967 - val_loss: 0.5363 - val_acc: 0.8306\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6022 - acc: 0.7982 - val_loss: 0.5391 - val_acc: 0.8298\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6052 - acc: 0.7978 - val_loss: 0.5683 - val_acc: 0.8133\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.6038 - acc: 0.7984 - val_loss: 0.5878 - val_acc: 0.8089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    hist = model.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size), \n",
    "        epochs=epochs, \n",
    "        steps_per_epoch=len(x_train) // batch_size,\n",
    "        validation_data=(x_test, y_test), workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 127us/step\n",
      "Test loss: 0.5877557454109192\n",
      "Test accuracy: 0.8089\n",
      "Saved trained model at /home/mi/leonsixt/sanity-checks-modified-bp/when-explanations-lie/saved_models/keras_cifar10_trained_model_acc8089.h5 \n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "\n",
    "scores_without_point = \"{:.4f}\".format(scores[1])[2:]\n",
    "model_name = 'keras_cifar10_trained_model_acc{}.h5'.format(\n",
    "    scores_without_point)\n",
    "\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
