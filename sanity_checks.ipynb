{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# When Explanations Lie: Why Many Modified BP Attributions fails\n",
    "\n",
    "\n",
    "## Sanity Checks & Random Logits\n",
    "\n",
    "This notebook produces the saliency figures and the ssim results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu device\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "import innvestigate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import copy\n",
    "import json\n",
    "import contextlib\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage.measure import compare_ssim \n",
    "import pickle\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "from innvestigate.analyzer.relevance_based.relevance_analyzer import LRP\n",
    "from innvestigate.analyzer.base import AnalyzerNetworkBase, ReverseAnalyzerBase\n",
    "from innvestigate.analyzer.deeptaylor import DeepTaylor\n",
    "from innvestigate.analyzer import DeepLIFTWrapper\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from when_explanations_lie import *\n",
    "\n",
    "import deeplift_resnet\n",
    "from deeplift_resnet import monkey_patch_deeplift_neg_pos_mxts\n",
    "from monkey_patch_lrp_resnet import custom_add_bn_rule, get_custom_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to imagenet validation\n",
    "host, = ! hostname\n",
    "\n",
    "with open('imagenet_dir.json') as f:\n",
    "    imagenet_dir = json.load(f)[host]\n",
    "\n",
    "# path to examplary image\n",
    "ex_image_path = \"n01534433/ILSVRC2012_val_00015410.JPEG\"\n",
    "# number of images to run the evaluation\n",
    "n_selected_imgs = 200\n",
    "\n",
    "model_names = ['resnet50', 'vgg16']\n",
    "\n",
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_meta(model_name, load_weights=True, clear_session=True):\n",
    "    if clear_session:\n",
    "        keras.backend.clear_session()\n",
    "    if model_name in ['vgg16', 'resnet50']:\n",
    "        model, innv_net, color_conversion = load_model(model_name, load_weights) \n",
    "        meta = ImageNetMeta(model, model_name, innv_net, n_selected_imgs, \n",
    "                            imagenet_dir, ex_image_path)\n",
    "    elif model_name == 'cifar10':\n",
    "        model, _, _ = load_model('cifar10', load_weights)\n",
    "        meta = CIFAR10Meta(model, n_selected_imgs)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return model, meta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metas = OrderedDict()\n",
    "for model_name in model_names:\n",
    "    model, meta = load_model_and_meta(model_name)\n",
    "    metas[model_name] = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shapes = get_output_shapes(model)\n",
    "\n",
    "print_output_shapes = False \n",
    "if print_output_shapes: \n",
    "    print(\"{:3}{:20}{:20}{}\".format(\"l\", \"layer\", \"input_at_0\", \"output_shape\"))\n",
    "    for i in range(len(model.layers)):\n",
    "        layer = model.get_layer(index=i)\n",
    "        print(\"{:3}: {:20}  {:20}  {}\".format(\n",
    "            i, layer.name, str(layer.get_input_shape_at(0)), str(output_shapes[i])))\n",
    "        #print(\"{:3}: {:20}  {:20}  {}\".format(i, type(layer).__name__, layer.name, output_shapes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmap_postprocess_wrapper(name):\n",
    "    return lambda x: heatmap_postprocess(name, x)\n",
    "\n",
    "input_range = (meta.ex_image.min(), meta.ex_image.max())\n",
    "analysers = get_analyser_params(input_range)\n",
    "\n",
    "attr_names = [n for (n, _, _, _, _) in analysers]\n",
    "    \n",
    "hmap_postprocessing = {\n",
    "    n: hmap_postprocess_wrapper(post_name) for n, _, post_name, _, _ in analysers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(attr_names):\n",
    "    style = mpl_styles[name]\n",
    "    plt.plot(np.arange(10), [20-i] * 10, \n",
    "             #markersize=5,\n",
    "             label=name + \" m=\" + style['marker'], **style)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks: Random Parameters & Logit Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, meta = load_model_and_meta('resnet50', clear_session=True)\n",
    "model_cascading, _ = load_model_and_meta('resnet50', clear_session=False)\n",
    "model_random, _ = load_model_and_meta('resnet50', \n",
    "                                         load_weights=False, clear_session=False)\n",
    "model_cascading.set_weights(model.get_weights())\n",
    "out = model.predict(meta.ex_image)\n",
    "out_cascading = model_cascading.predict(meta.ex_image)\n",
    "print(\"mean-l1 distance of the outputs of the trained model and when weights are from trained model [should be 0]:\", np.abs(out_cascading - out).mean())\n",
    "\n",
    "n_layers = len(model_random.layers)\n",
    "copy_weights(model_cascading, model_random, range(n_layers - 3, n_layers))\n",
    "\n",
    "out = model.predict(meta.ex_image)\n",
    "out_cascading = model_cascading.predict(meta.ex_image)\n",
    "print( \"mean-l1 distance of the outputs of the trained model when the last 2 layers are random [should not be 0]:\", np.abs(out_cascading - out).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to select a specific cache dir\n",
    "# hmap_output_dir = 'cache/2020-01-26T18:07:39.494420'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'hmap_output_dir' not in globals():\n",
    "    hmap_output_dir = 'cache/' + datetime.datetime.now().isoformat()\n",
    "    os.makedirs(hmap_output_dir)\n",
    "    print(\"Created new output dir:\", hmap_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving heatmaps to: \", hmap_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def ctx_analyzer(model, meta, innv_name, kwargs):\n",
    "    if innv_name.startswith(\"pattern\"):\n",
    "        kwargs['patterns'] = meta.patterns\n",
    "\n",
    "    if innv_name == 'deep_lift.wrapper':\n",
    "        kwargs = copy.copy(kwargs)\n",
    "        cross_mxts = kwargs.pop('cross_mxts', True)\n",
    "        print(\"CROSS MIXTS\", cross_mxts)\n",
    "        with monkey_patch_deeplift_neg_pos_mxts(cross_mxts):\n",
    "            analyzer = DeepLIFTWrapper(model, **kwargs)\n",
    "            analyzer.create_analyzer_model()\n",
    "            yield analyzer\n",
    "    else:\n",
    "        custom_rule = get_custom_rule(innv_name, kwargs)\n",
    "        with custom_add_bn_rule(custom_rule):\n",
    "            analyzer = innvestigate.create_analyzer(innv_name, model, **kwargs)\n",
    "            analyzer.create_analyzer_model()\n",
    "            yield analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmaps are saved in those dicts\n",
    "\n",
    "selected_attr_names = attr_names\n",
    "# selected_attr_names = ['DeepLIFT Abla.', ]\n",
    "\n",
    "recreate_analyser = False\n",
    "for model_name in tqdm.tqdm_notebook(['vgg16']):\n",
    "#for model_name in tqdm.tqdm_notebook(model_names):\n",
    "    model, meta = load_model_and_meta(model_name,  clear_session=True)\n",
    "    input_range = (meta.ex_image.min(), meta.ex_image.max())\n",
    "    analysers = get_analyser_params(input_range)\n",
    "    \n",
    "    for i, (attr_name, innv_name, _, excludes, analyser_kwargs) in enumerate(tqdm.tqdm_notebook(\n",
    "        analysers, desc=model_name)):\n",
    "        if attr_name not in selected_attr_names:\n",
    "            continue\n",
    "            \n",
    "        hmap_original = OrderedDict()\n",
    "        hmap_random_weights = OrderedDict()\n",
    "        hmap_random_target = OrderedDict()\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            # clear session from time to time to not OOM\n",
    "            keras.backend.clear_session()\n",
    "\n",
    "            model, innv_net, _ = load_model(model_name)\n",
    "            model_cascading, _, _ = load_model(model_name)\n",
    "            model_random, _, _ = load_model(model_name, load_weights=False)\n",
    "            model_cascading.set_weights(model.get_weights())\n",
    "            \n",
    "        if \"exclude_\" + model_name in excludes:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        fname = hmap_output_dir + '/heatmap_{}_{}.pickle'.format(model_name, attr_name)\n",
    "        if os.path.exists(fname):\n",
    "            warnings.warn(\"File already exsists: \" + fname)\n",
    "            # continue\n",
    "            \n",
    "            \n",
    "        cascading_heatmaps = {}\n",
    "        cascading_outputs = {}\n",
    "        model_cascading.set_weights(model.get_weights())\n",
    "        kwargs_w_idx = copy.copy(analyser_kwargs)\n",
    "        kwargs_w_idx['neuron_selection_mode'] = \"index\"\n",
    "        \n",
    "        original_idx = len(model.layers)\n",
    "        with ctx_analyzer(model_cascading, meta, innv_name, kwargs_w_idx) as analyzer_cascading:\n",
    "            for img_idx, (img_pp, target) in zip(meta.image_indices, meta.images):\n",
    "                random_target = get_random_target(target)\n",
    "                random_hmap = analyzer_cascading.analyze(img_pp, neuron_selection=random_target)[0]\n",
    "                idx = model_name, attr_name, img_idx\n",
    "                hmap_random_target[idx] = (random_target, random_hmap)\n",
    "                \n",
    "            selected_layers = [('original', original_idx)] +  [\n",
    "                (name, meta.names.nice_to_idx(name)) \n",
    "                 for name in meta.randomization_layers[::-1]\n",
    "            ]\n",
    "            \n",
    "            for layer_name, layer_idx in tqdm.tqdm_notebook(selected_layers, desc=attr_name):\n",
    "                copy_weights(model_cascading, model_random, range(layer_idx, original_idx))\n",
    "                if recreate_analyser or innv_name.startswith('deep_lift'):\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                    model, innv_net, _ = load_model(model_name)\n",
    "                    model_cascading, _, _ = load_model(model_name)\n",
    "                    model_random, _, _ = load_model(model_name, load_weights=False)\n",
    "                    copy_weights(model_cascading, model_random, range(layer_idx, original_idx))\n",
    "                    \n",
    "                    with ctx_analyzer(model_cascading, meta, innv_name, kwargs_w_idx) as analyzer_cascading:\n",
    "                        pass\n",
    "\n",
    "                for img_idx, (img_pp, target) in zip(meta.image_indices, meta.images):\n",
    "                    hmap = analyzer_cascading.analyze(img_pp, neuron_selection=target)[0]\n",
    "                    if layer_idx == original_idx:\n",
    "                        hmap_original[model_name, attr_name, img_idx] = hmap\n",
    "                        print('o', end='')\n",
    "                    else:\n",
    "                        hmap_random_weights[model_name, attr_name, img_idx, layer_idx] =  hmap\n",
    "\n",
    "            #assert not os.path.exists(fname)\n",
    "            with open(fname, 'wb') as f:\n",
    "                pickle.dump((hmap_original, hmap_random_weights, hmap_random_target), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = hmap_output_dir + '/heatmap_{}_{}.pickle'.format('vgg16', \"PatternAttr.\")\n",
    "if not os.path.exists(outpath):\n",
    "    warnings.warn(\"not found: \" + outpath)\n",
    "    raise\n",
    "with open(outpath, 'rb') as f:\n",
    "    hmap_original, hmap_random_weights, hmap_random_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh {hmap_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the sanity check ssim scores\n",
    "\n",
    "ssim_random_weights = OrderedDict()\n",
    "l2_random_weights = OrderedDict()\n",
    "\n",
    "last_idx = len(model.layers)\n",
    "for model_name in model_names:\n",
    "    for attr_name in attr_names:\n",
    "        outpath = hmap_output_dir + '/heatmap_{}_{}.pickle'.format(model_name, attr_name)\n",
    "        if not os.path.exists(outpath):\n",
    "            warnings.warn(\"not found: \" + outpath)\n",
    "            continue\n",
    "        with open(outpath, 'rb') as f:\n",
    "            hmap_original, hmap_random_weights, hmap_random_target = pickle.load(f)\n",
    "        \n",
    "        for (model_name, name, img_idx, layer_idx), heatmap in tqdm_notebook(\n",
    "            hmap_random_weights.items(), desc=\"{}.{}\".format(model_name, attr_name)):\n",
    "            original_heatmap = hmap_original[model_name, name, img_idx]\n",
    "            postprocess = hmap_postprocessing[name]\n",
    "            original_heatmap = postprocess(original_heatmap)\n",
    "            heatmap = postprocess(heatmap)\n",
    "            \n",
    "            if attr_name == 'RectGrad':\n",
    "                percentile = 100\n",
    "            else:\n",
    "                percentile = 99.5\n",
    "            ssim_random_weights[model_name, name, img_idx, layer_idx] = ssim_flipped(\n",
    "                heatmap, original_heatmap, percentile=percentile)\n",
    "            # l2_random_weights[model_name, name, img_idx, layer_idx] = l2_flipped(heatmap, original_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the random target ssim scores\n",
    "ssim_random_target = OrderedDict()\n",
    "\n",
    "for model_name in model_names:\n",
    "    for attr_name in attr_names:\n",
    "        outpath = hmap_output_dir + '/heatmap_{}_{}.pickle'.format(model_name, attr_name)\n",
    "        if not os.path.exists(outpath):\n",
    "            warnings.warn(\"not found: \" + outpath)\n",
    "            continue\n",
    "        with open(outpath, 'rb') as f:\n",
    "            hmap_original, hmap_random_weights, hmap_random_target = pickle.load(f)\n",
    "        for (model_name, attr_name, img_idx), (_, hmap_random) in tqdm.tqdm_notebook(\n",
    "            hmap_random_target.items()):\n",
    "            if (model_name, attr_name) not in ssim_random_target:\n",
    "                ssim_random_target[model_name, attr_name] = []\n",
    "\n",
    "            postprocess = hmap_postprocessing[attr_name]\n",
    "            hmap = postprocess(hmap_original[model_name, attr_name, img_idx])\n",
    "            hmap_random = postprocess(hmap_random)\n",
    "            \n",
    "            if attr_name == 'RectGrad':\n",
    "                percentile = 100\n",
    "            else:\n",
    "                percentile = 99.5\n",
    "            ssim_random_target[model_name, attr_name].append(\n",
    "                ssim_flipped(hmap, hmap_random, percentile=percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    with sns.axes_style('ticks', {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(3.9, 2.3), squeeze=True)\n",
    "\n",
    "\n",
    "        xlabels =  [n for (m, n) in ssim_random_target.keys() if m == model_name]\n",
    "        bars = ax.boxplot([ssim_random_target[model_name, n] for n in xlabels]) \n",
    "        ax.set_ylabel('SSIM')\n",
    "        #ax.set_xticks(np.arange(len(xlabels)))\n",
    "        ax.set_xticklabels(xlabels, rotation=90)\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "        \n",
    "        os.makedirs('figures/sanity_checks/', exist_ok=True)\n",
    "        figpath = 'figures/sanity_checks/random-logit-boxplot-{}.pdf'.format(model_name)\n",
    "        fig.savefig(figpath,  bbox_inches='tight', pad_inches=0)\n",
    "        display(IFrame(figpath, 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layers = [\n",
    "    (name, meta.names.nice_to_idx(name))\n",
    "     for name in meta.randomization_layers[::-1]\n",
    "]\n",
    "selected_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from when_explanations_lie import mpl_styles\n",
    "\n",
    "metrics = [('SSIM', ssim_random_weights)]\n",
    "\n",
    "ssim_reduce = 'median'\n",
    "confidence_intervals = True\n",
    "confidence_percentile = 99.5\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    meta = metas[model_name]\n",
    "    selected_layers = [\n",
    "        (name, meta.names.nice_to_idx(name))\n",
    "         for name in meta.randomization_layers[::-1]\n",
    "    ]\n",
    "    print(selected_layers)\n",
    "    \n",
    "    with sns.axes_style(\"ticks\", {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "        fig, axes = plt.subplots(1, len(metrics), figsize=(4.5 * len(metrics), 3.5), squeeze=False)\n",
    "        axes = axes[0]\n",
    "        for ax, (ylabel, metric) in zip(axes, metrics): \n",
    "            for (name, _, _, excludes, _) in analysers:\n",
    "                if 'exclude_' + model_name in excludes:\n",
    "                    continue\n",
    "                    \n",
    "                metric_per_layer = []\n",
    "                layer_idx = selected_layers[-1][1]\n",
    "                if (model_name, name, meta.image_indices[0], layer_idx) not in metric:\n",
    "                    warnings.warn(\"cound not find: \" + str((model_name, name, meta.image_indices[0], layer_idx)))\n",
    "                    continue\n",
    "                lower_conf = []\n",
    "                upper_conf = []\n",
    "                for (_, layer_idx) in selected_layers[::-1]:\n",
    "                    metric_per_layer.append(\n",
    "                        [metric[model_name, name, img_idx, layer_idx] for img_idx in meta.image_indices]\n",
    "                    )\n",
    "                        \n",
    "                    if confidence_intervals:\n",
    "                        vals = np.array(metric_per_layer[-1])\n",
    "                        ridx = np.random.choice(len(vals), (10000, len(vals)), replace=True)\n",
    "                        resample = vals[ridx]\n",
    "                        stats = np.median(resample, 1)\n",
    "                        lower_conf.append(np.percentile(stats, 100 - confidence_percentile))\n",
    "                        upper_conf.append(np.percentile(stats, confidence_percentile))\n",
    "\n",
    "                metric_per_layer = np.array(metric_per_layer)\n",
    "\n",
    "                if ssim_reduce == 'mean':\n",
    "                    ssims_reduced = metric_per_layer.mean(1)\n",
    "                elif ssim_reduce == 'median':\n",
    "                    ssims_reduced = np.median(metric_per_layer, 1)\n",
    "\n",
    "                ticks = np.arange(len(ssims_reduced))\n",
    "                ax.plot(ticks, ssims_reduced[::-1], label=name, **mpl_styles[name])\n",
    "                ax.fill_between(ticks, lower_conf[::-1], upper_conf[::-1], \n",
    "                                color=mpl_styles[name]['color'],\n",
    "                                alpha=0.25\n",
    "                               )\n",
    "                #ax.plot(ticks, lower_conf, color=linestyles[name]['color'])\n",
    "                #ax.plot(ticks, upper_conf, color=linestyles[name]['color'])\n",
    "\n",
    "            xlabels = [layer_name for layer_name, _ in selected_layers] \n",
    "            ax.set_ylim([0, 1.05])\n",
    "            ax.set_xticks(np.arange(len(xlabels)))\n",
    "            ax.set_xticklabels(xlabels, rotation=90)\n",
    "            ax.set_ylabel(ylabel)\n",
    "        axes[-1].legend(bbox_to_anchor=(1.0, 1.00), labelspacing=0.33)\n",
    "        plt.savefig('figures/sanity_checks/ssim-random-weights-{}.pdf'.format(model_name),  bbox_inches='tight', pad_inches=0)\n",
    "        plt.show()\n",
    "        display(IFrame('figures/sanity_checks/ssim-random-weights-{}.pdf'.format(model_name), 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_grid(heatmaps, cols, row_labels=[], column_labels=[], \n",
    "                      fig_path=None, figsize=None, labelpad=45):\n",
    "    mpl.rcParams['font.family'] = 'serif'\n",
    "    rows = len(heatmaps) // cols\n",
    "    \n",
    "    if figsize is None:\n",
    "        figsize = (cols, rows)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)\n",
    "    fontsize = 9\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05, top=1, bottom=0, left=0, right=1)\n",
    "    for label, ax in zip(row_labels, axes[:, 0]):\n",
    "        ax.set_ylabel(label, fontsize=fontsize + 1, labelpad=labelpad, rotation=0)\n",
    "        \n",
    "    print(axes.shape, column_labels, row_labels)\n",
    "    for label, ax in zip(column_labels, axes[0, :]):\n",
    "        ax.set_title(label, fontsize=fontsize)\n",
    "        \n",
    "        \n",
    "    for ax, heatmap in zip(axes.flatten(), heatmaps):\n",
    "        ax.imshow(heatmap, cmap='seismic', vmin=-1, vmax=1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    if fig_path is not None:\n",
    "        plt.savefig(fig_path, bbox_inches='tight', pad_inches=0, dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_neg(x):\n",
    "#     vmax = np.percentile(x, 99)\n",
    "#     vmin = np.percentile(x, 1)\n",
    "#     vmax\n",
    "#     x_pos = x * (x > 0)\n",
    "#     x_neg = x * (x < 0)\n",
    "#     \n",
    "#     x_pos = x_pos / vmax\n",
    "#     x_neg = - x_neg / vmin\n",
    "#     return np.clip(x_pos + x_neg, -1, 1)\n",
    "\n",
    "\n",
    "def load_examplary_heatmap():\n",
    "    hmap_loaded = OrderedDict()\n",
    "    for model_name in model_names:\n",
    "        meta = metas[model_name]\n",
    "        rnd_layers = meta.randomization_layers[::-1]\n",
    "        for (attr_name, _, _, excludes, _) in tqdm.tqdm_notebook(analysers):\n",
    "            if 'exclude_' + model_name in excludes:\n",
    "                print(attr_name)\n",
    "                continue\n",
    "            try:\n",
    "                outpath = hmap_output_dir + '/heatmap_{}_{}.pickle'.format(model_name, attr_name)\n",
    "                if not os.path.exists(outpath):\n",
    "                    warnings.warn(\"not found: \" + outpath)\n",
    "                    continue\n",
    "                with open(outpath, 'rb') as f:\n",
    "                    hmap_original, hmap_random_weights, hmap_random_target = pickle.load(f)\n",
    "                if attr_name in ['GuidedBP', 'Deconv']:\n",
    "                    postp = hmap_postprocessing[attr_name]\n",
    "                else:\n",
    "                    postp = hmap_postprocessing[attr_name]\n",
    "                \n",
    "                postp = lambda x: x\n",
    "                for img_idx in meta.image_indices[:1]:\n",
    "                    hmap_loaded[model_name, attr_name, 'image'] =norm_image(meta.images[0][0][0])\n",
    "                    hmap_loaded[model_name, attr_name, 'original'] = hmap_original[\n",
    "                        model_name, attr_name, img_idx]\n",
    "                    for layer_name in rnd_layers:\n",
    "                        layer_idx = meta.names.nice_to_idx(layer_name)\n",
    "                        hmap_loaded[model_name, attr_name, layer_name] = hmap_random_weights[\n",
    "                            model_name, attr_name, img_idx, layer_idx]\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    return hmap_loaded\n",
    "\n",
    "hmap_examplary = load_examplary_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('figures/sanity_checks', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_visual_equal(x, percentile=99):\n",
    "    \"\"\"\n",
    "    for visualization we normalize pos and neg attribution separatly.\n",
    "    \"\"\"\n",
    "    \n",
    "    if (x > 0).all() or (x < 0).all():\n",
    "        # special case\n",
    "        x_abs = np.abs(x)\n",
    "        vmax = np.percentile(x_abs, percentile)\n",
    "        return np.sign(x.mean()) * x_abs / vmax\n",
    "    \n",
    "    vmax = np.percentile(x, percentile)\n",
    "    vmin = np.percentile(x, 100 - percentile)\n",
    "    \n",
    "    x_pos = x * (x >= 0)\n",
    "    x_neg = x * (x < 0)\n",
    "    \n",
    "    absmax = max(np.abs(vmax), np.abs(vmin))\n",
    "    if np.abs(vmax) > 0:\n",
    "        x_pos = x_pos / absmax\n",
    "    if np.abs(vmin) > 0:\n",
    "        x_neg = x_neg / absmax\n",
    "    return np.clip(x_pos + x_neg, -1, 1)\n",
    "\n",
    "def postprocess_sanity(attr_name, hmap, visual=True):\n",
    "    hmap_sum = hmap_postprocessing[attr_name](hmap)\n",
    "    if attr_name == \"RectGrad\":\n",
    "        percentile = 100\n",
    "    else:\n",
    "        percentile = 99.5\n",
    "    return normalize_sanity(hmap_sum, percentile) \n",
    "\n",
    "def postprocess_visual(attr_name, hmap, visual=True):\n",
    "    mean = [103.939, 116.779, 123.68]\n",
    "    hmap_sum = hmap_postprocessing[attr_name](hmap)\n",
    "    if attr_name == \"RectGrad\":\n",
    "        percentile = 100\n",
    "    else:\n",
    "        percentile = 99.5\n",
    "    \n",
    "    \n",
    "    if False and attr_name in ['GuidedBP', \"DeepLIFT Abla.\", \"PatternNet\", \"Deconv\"]:\n",
    "        return image(hmap)#[..., ::-1]\n",
    "    else:\n",
    "        return normalize_visual_equal(hmap_sum, percentile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure 1\n",
    "selected_analysers = ['GuidedBP',\n",
    " 'RectGrad',\n",
    " 'DTD',\n",
    " 'LRP $\\\\alpha1\\\\beta0$',\n",
    " 'LRP $\\\\alpha2\\\\beta1$',                      \n",
    " 'PatternAttr.',\n",
    " 'DeepLIFT Resc.',\n",
    " 'Gradient',\n",
    "]\n",
    "\n",
    "selected_layers = [\n",
    "    \"image\",\n",
    "    \"original\",\n",
    "    \"fc3\",\n",
    "    \"conv5_3\",\n",
    "    \"conv4_1\",\n",
    "    \"conv2_1\",\n",
    "    \"conv1_1\",\n",
    "]\n",
    "\n",
    "selected_layers\n",
    "selected_hmaps = []\n",
    "\n",
    "for attr_name in selected_analysers:\n",
    "    for layer_name in selected_layers:\n",
    "        hmap = hmap_examplary['vgg16', attr_name, layer_name]\n",
    "        if layer_name != 'image':\n",
    "            hmap = postprocess_visual(attr_name, hmap)\n",
    "        selected_hmaps.append(hmap)\n",
    "    \n",
    "plot_heatmap_grid(\n",
    "    selected_hmaps, len(selected_layers), row_labels=selected_analysers, \n",
    "    column_labels=selected_layers,\n",
    "    figsize=(3.9, 0.55*len(selected_analysers) + 0.1),\n",
    "    fig_path='figures/sanity_checks/heatmap_grid_figure1.pdf',\n",
    "    labelpad=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('figures/sanity_checks/heatmap_grid_figure1.pdf', width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metas['resnet50'].randomization_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_analysers = [\n",
    "     'LRP CMP $\\\\alpha1\\\\beta0$',\n",
    "     'LRP CMP $\\\\alpha2\\\\beta1$'\n",
    "]\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    selected_hmaps = []\n",
    "    selected_layers = {\n",
    "        'vgg16': [\n",
    "            \"image\",\n",
    "            \"original\",\n",
    "            \"fc3\",\n",
    "            \"conv5_3\",\n",
    "            \"conv5_1\",\n",
    "            \"conv4_1\",\n",
    "            \"conv1_1\",\n",
    "        ],\n",
    "        'resnet50': [\n",
    "            \"image\",\n",
    "            \"original\",\n",
    "            \"dense\",\n",
    "            \"block5_2\",\n",
    "            \"block4_1\",\n",
    "            \"block3_3\",\n",
    "            \"conv1\",\n",
    "        ]}[model_name]\n",
    "    \n",
    "    for attr_name in selected_analysers:\n",
    "        for layer_name in selected_layers:\n",
    "            hmap = hmap_examplary[model_name, attr_name, layer_name]\n",
    "\n",
    "            if layer_name != 'image':\n",
    "                hmap = postprocess_visual(attr_name, hmap)\n",
    "            selected_hmaps.append(hmap)\n",
    "\n",
    "    outname = 'figures/sanity_checks/heatmap_grid_{}_lrp_cmp.pdf'.format(model_name)\n",
    "    print(len(selected_hmaps))\n",
    "    plot_heatmap_grid(\n",
    "        selected_hmaps, len(selected_layers), row_labels=selected_analysers, \n",
    "        column_labels=selected_layers,\n",
    "        figsize=(3.99, 1.1),\n",
    "        fig_path=outname\n",
    "    )\n",
    "    display(IFrame(outname, width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for selected_model_name in model_names[::-1]:\n",
    "    print(selected_model_name)\n",
    "    attr_for_model = OrderedDict()\n",
    "    layer_names = OrderedDict()\n",
    "    hmap_plot = []\n",
    "    for (model_name, attr_name, layer_name), hmap in hmap_examplary.items():\n",
    "        if model_name != selected_model_name:\n",
    "            continue\n",
    "        attr_for_model[attr_name] = attr_name\n",
    "        layer_names[layer_name] = layer_name\n",
    "        if layer_name != 'image':\n",
    "            if attr_name in ['GuidedBP', \"Deconv\", \"DeepLIFT Abla.\", \"PatternNet\"]:\n",
    "                hmap = image(hmap) #[..., [2, 1, 0]]\n",
    "            else:\n",
    "                hmap = postprocess_visual(attr_name, hmap)\n",
    "        hmap_plot.append(hmap)\n",
    "    \n",
    "    #hmap_plot = [normalize_visual(h) for h in hmap_plot]\n",
    "    plot_heatmap_grid(\n",
    "        hmap_plot, len(layer_names), \n",
    "        row_labels=attr_for_model.keys(), \n",
    "        column_labels=layer_names.keys(),\n",
    "        figsize=(0.6*len(layer_names), 0.6*len(attr_for_model)),\n",
    "        #figsize=(1*len(layer_names), 1*len(attr_for_model)),\n",
    "        fig_path='figures/sanity_checks/heatmap_image_grid_{}.pdf'.format(selected_model_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('figures/sanity_checks/heatmap_image_grid_vgg16.pdf', width=1000, height=600))\n",
    "display(IFrame('figures/sanity_checks/heatmap_image_grid_resnet50.pdf', width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for selected_model_name in model_names[::-1]:\n",
    "    print(selected_model_name)\n",
    "    attr_for_model = OrderedDict()\n",
    "    layer_names = OrderedDict()\n",
    "    hmap_plot = []\n",
    "    for (model_name, attr_name, layer_name), hmap in hmap_examplary.items():\n",
    "        if model_name != selected_model_name:\n",
    "            continue\n",
    "        attr_for_model[attr_name] = attr_name\n",
    "        layer_names[layer_name] = layer_name\n",
    "        if layer_name != 'image':\n",
    "            hmap = postprocess_visual(attr_name, hmap)\n",
    "        hmap_plot.append(hmap)\n",
    "    \n",
    "    #hmap_plot = [normalize_visual(h) for h in hmap_plot]\n",
    "    plot_heatmap_grid(\n",
    "        hmap_plot, len(layer_names), \n",
    "        row_labels=attr_for_model.keys(), \n",
    "        column_labels=layer_names.keys(),\n",
    "        figsize=(0.6*len(layer_names), 0.6*len(attr_for_model)),\n",
    "        #figsize=(1*len(layer_names), 1*len(attr_for_model)),\n",
    "        fig_path='figures/sanity_checks/heatmap_visual_grid_{}.pdf'.format(selected_model_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('figures/sanity_checks/heatmap_visual_grid_vgg16.pdf', width=1000, height=600))\n",
    "display(IFrame('figures/sanity_checks/heatmap_visual_grid_resnet50.pdf', width=1000, height=600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
