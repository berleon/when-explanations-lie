{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Explanations Lie: Why Modified BP Attribution fails\n",
    "\n",
    "This notebook produces the cosine similaries of the relevance vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install install packages\n",
    "# !pip install tensorflow-gpu=1.13.1\n",
    "# !pip install innvestigate seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "import innvestigate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import copy\n",
    "import contextlib\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage.measure import compare_ssim \n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "from innvestigate.analyzer.relevance_based.relevance_analyzer import LRP\n",
    "from innvestigate.analyzer.base import AnalyzerNetworkBase, ReverseAnalyzerBase\n",
    "from innvestigate.analyzer.deeptaylor import DeepTaylor\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_model(self, model):\n",
    "    return super(DeepTaylor, self)._prepare_model(model)\n",
    "\n",
    "# otherwise DTD does not work on negative outputs\n",
    "DeepTaylor._prepare_model = _prepare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to imagenet validation\n",
    "imagenet_val_dir = \"/mnt/ssd/data/imagenet/imagenet-raw/validation\"\n",
    "# path to examplary image\n",
    "ex_image_path = \"n01534433/ILSVRC2012_val_00015410.JPEG\"\n",
    "# number of images to run the evaluation\n",
    "n_selected_imgs = 200\n",
    "\n",
    "model_names = ['resnet50', 'vgg16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "def load_image(path, size):\n",
    "    ret = PIL.Image.open(path)\n",
    "    ret = ret.resize((size, size))\n",
    "    ret = np.asarray(ret, dtype=np.uint8).astype(np.float32)\n",
    "    if ret.ndim == 2:\n",
    "        ret.resize((size, size, 1))\n",
    "        ret = np.repeat(ret, 3, axis=-1)\n",
    "    return ret\n",
    "\n",
    "def preprocess(X, net):\n",
    "    X = X.copy()\n",
    "    X = net[\"preprocess_f\"](X)\n",
    "    return X\n",
    "\n",
    "def image(X):\n",
    "    X = X.copy()\n",
    "    return ivis.project(X, absmax=255.0)\n",
    "\n",
    "def to_heatmap(saliency):\n",
    "    return ivis.heatmap(\n",
    "        np.abs(\n",
    "            ivis.clip_quantile(saliency.copy(), 0.5))\n",
    "        )\n",
    "\n",
    "def copy_weights(model_to, model_from, idxs):\n",
    "    idxs = set(idxs)\n",
    "    all_weights = []\n",
    "    for i in range(len(model_to.layers)):\n",
    "        if i in idxs:\n",
    "            all_weights.extend(model_from.layers[i].get_weights())\n",
    "        else:\n",
    "            all_weights.extend(model_to.layers[i].get_weights())\n",
    "    model_to.set_weights(all_weights)\n",
    "        \n",
    "def get_random_target(target_exclude):\n",
    "    return np.random.choice([i for i in range(1000) if i != target_exclude])\n",
    "\n",
    "def normalize(x, percentile=99):\n",
    "    \"\"\"\n",
    "    all heatmap are normalized\n",
    "    \"\"\"\n",
    "    vmin = np.percentile(x, percentile)\n",
    "    vmax = np.percentile(x, 100 - percentile)\n",
    "    return np.clip((x - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "def ssim_flipped(x, y, win_size=5, **kwargs):\n",
    "    norm_x = normalize(x)\n",
    "    norm_y = normalize(y)\n",
    "    norm_y_flip = normalize(-y)\n",
    "    \n",
    "    ssim = compare_ssim(norm_x, norm_y, win_size=win_size, **kwargs)\n",
    "    ssim_flip = compare_ssim(norm_x, norm_y_flip, win_size=win_size, **kwargs)\n",
    "    return max(ssim, ssim_flip)\n",
    "\n",
    "\n",
    "def l2_flipped(x, y):\n",
    "    norm_x = normalize(x)\n",
    "    norm_y = normalize(y)\n",
    "    norm_y_flip = normalize(-y)\n",
    "    l2 = np.mean(np.sqrt((norm_x - norm_y)**2))\n",
    "    l2_flipped = np.mean(np.sqrt((norm_x - norm_y_flip)**2))\n",
    "    return max(l2, l2_flipped)\n",
    "\n",
    "\n",
    "def norm_image(x):\n",
    "    mi = x.min()\n",
    "    ma = x.max()\n",
    "    return (x - mi) / (ma - mi)\n",
    "\n",
    "def load_image_paths(validation_dir):\n",
    "    val_filepaths = ! find {validation_dir} -name '*.JPEG' \n",
    "    val_filepaths = sorted(val_filepaths)\n",
    "    val_targets = ! ls {validation_dir}\n",
    "    val_targets = sorted(val_targets)\n",
    "    \n",
    "    val_path_with_target = []\n",
    "    for path in val_filepaths:\n",
    "        synnet = path.split('/')[-2]\n",
    "        target = val_targets.index(synnet)\n",
    "        val_path_with_target.append((path, target))\n",
    "    return val_path_with_target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model='vgg16', load_weights=True, load_patterns=\"relu\"):\n",
    "    load_func = getattr(innvestigate.applications.imagenet, model)\n",
    "    net = load_func(load_weights=load_weights, load_patterns=load_patterns)\n",
    "    model = keras.models.Model(inputs=net[\"in\"], outputs=net[\"sm_out\"])\n",
    "    model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
    "    \n",
    "    channels_first = keras.backend.image_data_format() == \"channels_first\"\n",
    "    color_conversion = \"BGRtoRGB\" if net[\"color_coding\"] == \"BGR\" else None\n",
    "    return model_wo_softmax, net, color_conversion\n",
    "\n",
    "def get_output_shapes(model, input_shape=(None, 224, 224, 3)):\n",
    "    model_output_shapes = OrderedDict()\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        model_output_shapes[i] = layer.get_output_shape_at(0)\n",
    "    return model_output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nice_resnet_layers(model):\n",
    "    def get_conv_name(layer):\n",
    "        block, branch = layer.name.split('_')\n",
    "        block_spec = block.lstrip('res')\n",
    "        block_idx = block_spec[0]\n",
    "        subblock_idx = ' abcdefg'.index(block_spec[1])\n",
    "        layer_idx = branch[-1]\n",
    "        if layer_idx == '1':\n",
    "            layer_idx = 's'\n",
    "            nice_name = \"conv{}_skip\".format(block_idx)\n",
    "        else:\n",
    "            nice_name = \"conv{}_{}{}\".format(block_idx, subblock_idx, layer_idx) \n",
    "        return nice_name\n",
    "        \n",
    "    nice_layer_names_resnet = OrderedDict()\n",
    "    for i in range(len(model.layers)):\n",
    "        layer = model.get_layer(index=i)\n",
    "        if type(layer) == keras.layers.Conv2D:\n",
    "            if layer.name == 'conv1':\n",
    "                nice_layer_names_resnet[i] = 'conv1'\n",
    "                continue\n",
    "\n",
    "            nice_layer_names_resnet[i] = get_conv_name(layer)\n",
    "            #print(block_spec, block_idx, subblock_idx, layer.name, nice_name)\n",
    "        elif type(layer) == keras.layers.MaxPool2D:\n",
    "            nice_layer_names_resnet[i] = 'maxpool'\n",
    "        elif type(layer) == keras.layers.GlobalAveragePooling2D:\n",
    "            nice_layer_names_resnet[i] = 'avgpool'\n",
    "        elif type(layer) == keras.layers.Add:\n",
    "            prev_layer = model.get_layer(index=i-1)\n",
    "            \n",
    "            block, branch = prev_layer.name.split('_')\n",
    "            block_spec = block.lstrip('bn')\n",
    "            block_idx = block_spec[0]\n",
    "            subblock_idx = ' abcdefg'.index(block_spec[1])\n",
    "            nice_name = 'block{}_{}'.format(block_idx, subblock_idx)\n",
    "            # print(prev_layer.name, nice_name, block_idx, subblock_idx, \n",
    "            #       model.get_layer(index=i+1).name)\n",
    "            print(nice_name)\n",
    "            # name the following activation layers\n",
    "            nice_layer_names_resnet[i+1] = nice_name\n",
    "        elif type(layer) == keras.layers.Dense:\n",
    "            nice_layer_names_resnet[i] = 'dense'\n",
    "    return nice_layer_names_resnet\n",
    "\n",
    "\n",
    "def get_layer_idx(model_name, layer_name):\n",
    "    for key, l in nice_layer_names[model_name].items():\n",
    "        if l == layer_name:\n",
    "            return key\n",
    "        \n",
    "    raise ValueError(\"no layer: \" + layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_paths = load_image_paths(imagenet_val_dir)\n",
    "\n",
    "ex_image_full_path, ex_target = [\n",
    "    (path, target) for (path, target) in val_paths \n",
    "    if path.endswith(ex_image_path)][0]\n",
    "ex_image_idx = val_paths.index((ex_image_full_path, ex_target))\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "selected_img_idxs = [ex_image_idx] + np.random.choice([idx for idx in range(len(val_paths))\n",
    "                                                       if idx != ex_image_idx], n_selected_imgs - 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model definition.\n",
    "\n",
    "model, innv_net, color_conversion = load_model('resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_layer_names = {\n",
    "    'vgg16': OrderedDict([\n",
    "        (0, 'input'),\n",
    "        (1, 'conv1_1'),\n",
    "        (2, 'conv1_2'),\n",
    "        (3, 'pool1'),\n",
    "        (4, 'conv2_1'),\n",
    "        (5, 'conv2_2'),\n",
    "        (6, 'pool2'),\n",
    "        (7, 'conv3_1'),\n",
    "        (8, 'conv3_2'),\n",
    "        (9, 'conv3_3'),\n",
    "        (10, 'pool3'),\n",
    "        (11, 'conv4_1'),\n",
    "        (12, 'conv4_2'),\n",
    "        (13, 'conv4_3'),\n",
    "        (14, 'pool4'),\n",
    "        (15, 'conv5_1'),\n",
    "        (16, 'conv5_2'),\n",
    "        (17, 'conv5_3'),\n",
    "        (18, 'pool5'),\n",
    "        (19, 'flatten'),\n",
    "        (20, 'fc1'),\n",
    "        (21, 'fc2'),\n",
    "        (22, 'fc3') \n",
    "    ]),\n",
    "    'resnet50': get_nice_resnet_layers(model)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = {'vgg16': 22, 'resnet50': 177}\n",
    "\n",
    "replacement_layers = {\n",
    "    'vgg16':  ['fc3', 'fc1', 'conv4_3', 'conv3_3', 'conv2_2'],\n",
    "    'resnet50': ['dense', 'block5_1', 'block4_2', 'block3_4', 'block3_2', 'block2_2'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_image = preprocess(load_image(os.path.join(imagenet_val_dir, ex_image_path), 224),\n",
    "                      innv_net)[np.newaxis]\n",
    "ex_target = [target for (path, target) in val_paths \n",
    "             if path.endswith(ex_image_path)][0]\n",
    "\n",
    "val_images = [(\n",
    "    preprocess(\n",
    "        load_image(val_paths[idx][0], 224),\n",
    "        innv_net\n",
    "    )[np.newaxis],\n",
    "    val_paths[idx][1]) for idx in selected_img_idxs]\n",
    "\n",
    "print(\"Loaded {} images\".format(len(val_images)))\n",
    "\n",
    "\n",
    "output_shapes = get_output_shapes(model)\n",
    "\n",
    "print_output_shapes = False \n",
    "if print_output_shapes: \n",
    "    print(\"{:3}{:20}{:20}{}\".format(\"l\", \"layer\", \"input_at_0\", \"output_shape\"))\n",
    "    for i in range(len(model.layers)):\n",
    "        layer = model.get_layer(index=i)\n",
    "        print(\"{:3}: {:20}  {:20}  {}\".format(\n",
    "            i, layer.name, str(layer.get_input_shape_at(0)), str(output_shapes[i])))\n",
    "        #print(\"{:3}: {:20}  {:20}  {}\".format(i, type(layer).__name__, layer.name, output_shapes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cascading, _, _ = load_model('resnet50')\n",
    "model_random, _, _ = load_model('resnet50', load_weights=False)\n",
    "model_cascading.set_weights(model.get_weights())\n",
    "out = model.predict(ex_image)\n",
    "out_cascading = model_cascading.predict(ex_image)\n",
    "print(\"mean-l1 distance of the outputs of the trained model and when weights are from trained model [should be 0]:\", np.abs(out_cascading - out).mean())\n",
    "\n",
    "n_layers = len(model_random.layers)\n",
    "copy_weights(model_cascading, model_random, range(n_layers - 3, n_layers))\n",
    "\n",
    "out = model.predict(ex_image)\n",
    "out_cascading = model_cascading.predict(ex_image)\n",
    "print( \"mean-l1 distance of the outputs of the trained model when the last 2 layers are random [should not be 0]:\", np.abs(out_cascading - out).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(name, x):\n",
    "    if name == 'abs.max':\n",
    "        return np.abs(x).max(-1)\n",
    "    elif name == 'abs.sum':\n",
    "        return np.abs(x).sum(-1)\n",
    "    elif name == 'sum':\n",
    "        return x.sum(-1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "input_range = (ex_image.min(), ex_image.max())\n",
    "noise_scale = 0.15 * (ex_image.max() - ex_image.min())\n",
    "\n",
    "# label, innv_name, postprocess, exclude, kwargs\n",
    "\n",
    "\n",
    "attr_names = [n for (n, _, _, _, _) in analysers]\n",
    "\n",
    "hmap_postprocesisng = {\n",
    "    n: lambda x: postprocessing(post_name, x) for n, _, post_name, _, _ in analysers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl_linestyle = ['solid', 'dashed', 'dotted']\n",
    "#    \"o\", #circle\n",
    "#    \"v\", #triangle_down\n",
    "#    \"^\", #triangle_up\n",
    "#    \"X\", #star\n",
    "#    \"s\", #square\n",
    "#    \"<\", #triangle_left\n",
    "#    \"P\", #plus (filled)\n",
    "#    \"D\", #diamond\n",
    "#    \"$O$\", \n",
    "#    \">\",\n",
    "#    \"$V$\", #hexagon2\n",
    "#    \"$P$\", \n",
    "#    \"$S$\"\n",
    "\n",
    "color_palette = sns.color_palette('colorblind', n_colors=5)\n",
    "hatches = [None, '//']\n",
    "styles = list(itertools.product(mpl_linestyle, zip(mpl_markers, itertools.cycle(color_palette))))\n",
    "\n",
    "mpl_styles = OrderedDict([\n",
    "    ('GuidedBP',                   {'marker': 'X',   'color': colors[0]}),\n",
    "    ('Deconv',                     {'marker': '$D$', 'color': colors[1]}),\n",
    "    ('LRP-z',                      {'marker': 'D',   'color': colors[2]}),\n",
    "    ('DTD',                        {'marker': '$T$', 'color': colors[3]}),\n",
    "    ('PatternAttr.',               {'marker': '$P$', 'color': colors[4]}),\n",
    "    ('LRP-$\\\\alpha=1, \\\\beta=0$',  {'marker': '<',   'color': colors[0]}),\n",
    "    ('LRP-$\\\\alpha=2, \\\\beta=1$',  {'marker': '>',   'color': colors[1]}),\n",
    "    ('LRP-$\\\\alpha=5, \\\\beta=4$',  {'marker': '^',   'color': colors[2]}),\n",
    "    ('LRP-cmp-$\\\\alpha=1$',        {'marker': 's',   'color': colors[3]}),\n",
    "    ('LRP-cmp-$\\\\alpha=2$',        {'marker': 'P',   'color': colors[4]}),\n",
    "    ('SmoothGrad',                 {'marker': 'o',   'color': colors[5]}),\n",
    "    ('Gradient',                   {'marker': 'v',   'color': 'black'}),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "print(len(styles), len(attr_names))\n",
    "assert len(styles) >= len(attr_names)\n",
    "\n",
    "linestyles = OrderedDict(\n",
    "    [(name, {'linestyle': 'solid', 'marker': marker_dict[name], 'color': c}) \n",
    "     for name, (l, (m, c)) in zip(attr_names, styles)]\n",
    ")\n",
    "if 'Gradient' in linestyles:\n",
    "    linestyles['Gradient']['color'] = 'black'\n",
    "    \n",
    "for i, (name, style) in enumerate(mpl_styles.items()):\n",
    "    plt.plot(np.arange(10), [20-i] * 10, \n",
    "             #markersize=5,\n",
    "             label=name + \" m=\" + linestyle['marker'], **linestyle)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replacement_class(analyser_cls):\n",
    "    assert issubclass(analyser_cls, ReverseAnalyzerBase)\n",
    "    class ReplaceBackward(analyser_cls):\n",
    "        def __init__(self, model, replace_layer, replace_shape, *args, **kwargs):\n",
    "            kwargs['reverse_keep_tensors'] = True\n",
    "            super().__init__(model, *args, **kwargs)\n",
    "            self._replace_shape = replace_shape\n",
    "            self._replace_layer = replace_layer\n",
    "            self._replace_tensor = None\n",
    "\n",
    "        def _prepare_model(self, model):\n",
    "            model, analysis_inputs, stop_analysis_at_tensor = super()._prepare_model(model)\n",
    "            self._replace_tensor = keras.layers.Input(name='replace_backward', \n",
    "                                                      batch_shape=self._replace_shape)   \n",
    "            self._replace_inputs = analysis_inputs + [self._replace_tensor]\n",
    "            return model, analysis_inputs + [self._replace_tensor], stop_analysis_at_tensor\n",
    "\n",
    "        def _create_analysis(self, *args, **kwargs):\n",
    "            def check_layer(layer):\n",
    "                return layer == self._replace_layer\n",
    "\n",
    "            def replace_backward(Xs, Ys, Rs, reverse_state):               \n",
    "                return [keras.layers.Lambda(lambda x: 2*x / 2)(self._replace_tensor)]\n",
    "\n",
    "            self._add_conditional_reverse_mapping(check_layer, replace_backward)\n",
    "            outputs, intermediate = super()._create_analysis(*args, **kwargs)\n",
    "            self._create_cos_model(intermediate)\n",
    "            return outputs, intermediate\n",
    "        \n",
    "        def get_cosine(self, X, replacement, intermediate_values):\n",
    "            sess = keras.backend.get_session()\n",
    "            feed_dict = OrderedDict([\n",
    "                (t, v) for t, v in zip(self._intermediate_references, intermediate_values)\n",
    "            ])\n",
    "            feed_dict[self._replace_tensor] = replacement\n",
    "            \n",
    "            tf_X = repl_analyser._analyzer_model.inputs[0]\n",
    "            feed_dict[tf_X] = X\n",
    "            return sess.run(self._cosine_similarities, feed_dict=feed_dict)\n",
    "            \n",
    "        def get_cosine_grad(self, X, replacement, intermediate_values):\n",
    "            sess = keras.backend.get_session()\n",
    "            feed_dict = OrderedDict([\n",
    "                (t, v) for t, v in zip(self._intermediate_references, intermediate_values)\n",
    "            ])\n",
    "            feed_dict[self._replace_tensor] = replacement\n",
    "            \n",
    "            tf_X = repl_analyser._analyzer_model.inputs[0]\n",
    "            feed_dict[tf_X] = X\n",
    "            return sess.run(self._cosine_grads, feed_dict=feed_dict)\n",
    "            \n",
    "            \n",
    "        def _create_cos_model(self, intermediate_tensors):\n",
    "            self._intermediate_tensors = intermediate_tensors[1:]\n",
    "            self._intermediate_references = [\n",
    "                keras.layers.Input(name='intermediate_{}'.format(i), batch_shape=tens.shape)\n",
    "                for i, tens in enumerate(self._intermediate_tensors)\n",
    "            ]\n",
    "            self._replace_inputs \n",
    "            \n",
    "            self._cosine_similarities = [] \n",
    "            \n",
    "            for v, r in zip(self._intermediate_tensors, self._intermediate_references):\n",
    "                \n",
    "                r_norm = tf.nn.l2_normalize(r, axis=-1)\n",
    "                v_norm = tf.nn.l2_normalize(v, axis=-1)\n",
    "                cos = 1 - tf.losses.cosine_distance(r_norm, v_norm, axis=-1, reduction='none')\n",
    "                self._cosine_similarities.append(cos)\n",
    "                \n",
    "            cos_sim = self._cosine_similarities[-1]\n",
    "            self._cosine_grads = [g for g in tf.gradients(tf.reduce_mean(cos_sim), self._intermediate_tensors) if g is not None]\n",
    "            \n",
    "    return ReplaceBackward \n",
    "\n",
    "\n",
    "def get_replacement_analyser(model, analyser_cls, replacement_layer_idx, model_output_shapes=None, **kwargs):\n",
    "    if type(analyser_cls) == str:\n",
    "        analyser_cls = innvestigate.analyzer.analyzers[analyser_cls]\n",
    "    replacement_cls = create_replacement_class(analyser_cls)\n",
    "    \n",
    "    if model_output_shapes is None:\n",
    "        model_output_shapes = get_output_shapes(model)\n",
    "    \n",
    "    replacement_shape = model_output_shapes[replacement_layer_idx - 1]\n",
    "    replace_layer = model.layers[replacement_layer_idx]\n",
    "    return replacement_cls(model, replace_layer, replacement_shape, **kwargs), replacement_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = {'vgg16': 23, 'resnet50': 177}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement_layer_indices = [22]\n",
    "n_sampled_v = 5\n",
    "\n",
    "cos_sim_median = OrderedDict()\n",
    "selected_percentiles = [0, 1, 5, 10, 20, 50, 100]\n",
    "cos_sim_percentiles = OrderedDict()\n",
    "for label, innv_name, _, excludes, kwargs in tqdm.tqdm_notebook(analysers):\n",
    "    if 'exclude_cos_sim' in excludes:\n",
    "        continue\n",
    "        \n",
    "    for model_name in ['resnet50', 'vgg16']:\n",
    "        if 'exclude_' + model_name in excludes:\n",
    "            continue\n",
    "        keras.backend.clear_session()\n",
    "        model_wo_softmax, innv_net, _ = load_model(model_name, load_weights=True)\n",
    "        if innv_name == \"pattern.attribution\":\n",
    "            kwargs['patterns'] = innv_net['patterns']\n",
    "\n",
    "        for replacement_layer in replacement_layers[model_name][:]:\n",
    "            replacement_layer_idx = get_layer_idx(model_name, replacement_layer)\n",
    "            repl_analyser, repl_shape = get_replacement_analyser(\n",
    "                model_wo_softmax, innv_name,  \n",
    "                replacement_layer_idx=replacement_layer_idx,\n",
    "                **kwargs)\n",
    "            repl_analyser.create_analyzer_model()\n",
    "            cos_per_img = OrderedDict()\n",
    "            for img_idx, (img, _) in tqdm.tqdm_notebook(\n",
    "                zip(selected_img_idxs, val_images), \n",
    "                desc=\"[{}.{}] {}\".format(model_name, replacement_layer, label)):\n",
    "                channels = repl_shape[-1]\n",
    "                if label == \"$\\\\alpha=100, \\\\beta=99$-LRP\":\n",
    "                    # a=100,b=99 sufferes numerical instabilities with std = 1\n",
    "                    std = 1 / np.sqrt(channels)\n",
    "                else:\n",
    "                    std = 1\n",
    "\n",
    "                replacement = std*np.random.normal(size=(1, ) + repl_shape[1:]) \n",
    "                hmap = repl_analyser.analyze([img, replacement])\n",
    "                intermediate_values = repl_analyser._reversed_tensors\n",
    "\n",
    "                replacement = std * np.random.normal(size=(n_sampled_v,) + repl_shape[1:]) \n",
    "                outs = repl_analyser.get_cosine(np.tile(img, (n_sampled_v, 1, 1, 1)), replacement, [v for (_, v) in intermediate_values[1:][::-1]])\n",
    "                for layer_idx, o in enumerate(outs):\n",
    "                    cos_per_img[model_name, layer_idx, img_idx] = np.abs(o)\n",
    "\n",
    "            median_for_label = []\n",
    "            percentile_for_label = OrderedDict([(p, []) for p in selected_percentiles])\n",
    "            for layer_idx in range(n_layers[model_name]):\n",
    "                cos_per_layer = np.concatenate([cos_per_img[model_name, layer_idx, img_idx]  for img_idx in selected_img_idxs])\n",
    "                perc_values = np.percentile(cos_per_layer.flatten(),  selected_percentiles)\n",
    "                for p, val in zip(selected_percentiles, perc_values):\n",
    "                    percentile_for_label[p].append(val)\n",
    "\n",
    "            for p, values in percentile_for_label.items():\n",
    "                cos_sim_percentiles[label, model_name, replacement_layer_idx, p] = np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = True\n",
    "if save_results:\n",
    "    with open('cache/cos_sim_with_resnet.pickle', 'wb') as f:\n",
    "        pickle.dump((cos_sim_percentiles), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(U, V):\n",
    "    v_norm = V / np.linalg.norm(V, axis=0, keepdims=True)\n",
    "    u_norm = U / np.linalg.norm(U, axis=0, keepdims=True)\n",
    "    return v_norm.T @ u_norm\n",
    "\n",
    "def get_sample_cos_sim_per_layer(output_shapes):\n",
    "    values = []\n",
    "    for layer_idx, shp in output_shapes.items():\n",
    "        ch = shp[-1]\n",
    "        n_samples = 1000\n",
    "        u = np.random.normal(size=(ch, n_samples))\n",
    "        v = np.random.normal(size=(ch, n_samples))\n",
    "        cos = cosine_similarity(u, u)\n",
    "        mask = np.tri(cos.shape[0])\n",
    "        values.append(np.median(np.abs(cos[mask == 1])))\n",
    "    return np.array(values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for (n, _, _, _, _) in analysers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = OrderedDict()\n",
    "for model_name in model_names[::-1]:\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    model, _, _ = load_model(model_name)\n",
    "    output_shapes = get_output_shapes(model)\n",
    "\n",
    "    sample_cos_per_layer = get_sample_cos_sim_per_layer(output_shapes)\n",
    "    for replacement_layer in replacement_layers[model_name]:\n",
    "        repl_idx = get_layer_idx(model_name, replacement_layer)\n",
    "        start_layer = n_layers[model_name] - repl_idx\n",
    "        \n",
    "        layer_names = [name for idx, name in nice_layer_names[model_name].items()\n",
    "                       if idx <= repl_idx][::-1]\n",
    "        layer_idxs = np.array([idx for idx, name in nice_layer_names[model_name].items()\n",
    "                       if idx < repl_idx][::-1])\n",
    "        # print(layer_idxs)\n",
    "        displayed_layers = n_layers[model_name] - layer_idxs\n",
    "        plt.figure(figsize=(max(3, len(displayed_layers) / 4), 3.5))\n",
    "        for i, (label, _, _, _, _) in enumerate(analysers):\n",
    "            idx = (label, model_name, repl_idx, 50)\n",
    "            if idx not in cos_sim_percentiles:\n",
    "                warnings.warn(\"not found: \" + str(idx))\n",
    "                continue\n",
    "            #print(len(cos_sim_percentiles[idx]), repl_idx, displayed_layers)\n",
    "            cos_sim_per_label = cos_sim_percentiles[idx][displayed_layers]\n",
    "            plt.plot(0.5 + np.arange(len(cos_sim_per_label)), cos_sim_per_label, label=label, **linestyles[label])\n",
    "            if label not in legend:\n",
    "                legend[label] = linestyles[label]\n",
    "            \n",
    "        layer_names = [name for idx, name in nice_layer_names[model_name].items()\n",
    "                       if idx <= repl_idx][::-1]\n",
    "        \n",
    "        \n",
    "        # Random Cos Similarity\n",
    "        # Cos Similarity Base.\n",
    "        label='Cos Similarity BL'\n",
    "        style = {'color': (0.25, 0.25, 0.25)}\n",
    "        plt.plot(0.5 + np.arange(len(displayed_layers)), sample_cos_per_layer[::-1][displayed_layers], \n",
    "                 # label='Cos. Sim. Baseline', \n",
    "                 label=label,\n",
    "                 **style)\n",
    "        if label not in legend:\n",
    "            legend[label] = style\n",
    "        \n",
    "        #plt.legend(bbox_to_anchor=(1, 1))\n",
    "        plt.ylabel('cosine similarity')\n",
    "        plt.xticks(np.arange(len(layer_names)), layer_names, rotation=90)\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid('on', alpha=0.35) #, axis=\"y\")\n",
    "        plt.savefig(\"./figures/consine_similarity_{}_layer_{}.pdf\".format(model_name, repl_idx),  \n",
    "                    bbox_inches='tight', pad_inches=0)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for replacement_layer_idx in replacement_layer_indices:\n",
    "    display(IFrame(\"./figures/consine_similarity_vgg16_layer_{}.pdf\".format(replacement_layer_idx), 800, 600))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2, 1))\n",
    "for label, style in legend.items():\n",
    "    plt.plot([], [], label=label, **style)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend(loc='center')\n",
    "#plt.tight_layout()\n",
    "plt.savefig('figures/cos_sim_legend.pdf', \n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame(\"./figures/cos_sim_legend.pdf\", 800, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PatternAttribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "layers_w_kernel = ([l for l in model.layers if type(l) == keras.layers.Conv2D]\n",
    "                    + [l for l in model.layers if type(l) == keras.layers.Dense])\n",
    "for i in range(len(patterns)):\n",
    "    w = layers_w_kernel[i].get_weights()[0]\n",
    "    pw = patterns[i] * w\n",
    "    print(i, (patterns[i] < 0).sum() / patterns[i].size, (w < 0).sum() / patterns[i].size, (pw > 0).sum() / pw.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does GBP and Deconv converge SOOOO fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gdb = ('GuidedBP', \n",
    "       \"guided_backprop\", \n",
    "         lambda x: np.abs(x).sum(-1), \n",
    "         {})\n",
    "deconv = ('Deconv', \n",
    "         \"deconvnet\", \n",
    "         lambda x: np.abs(x).sum(-1), \n",
    "         {})\n",
    "\n",
    "replacement_layer_idx = 22\n",
    "n_sampled_v = 10\n",
    "cos_sim = OrderedDict()\n",
    "for label, innv_name, postpro, kwargs in tqdm.tqdm_notebook([gdb, deconv]):\n",
    "    repl_analyser, repl_shape = get_replacement_analyser(model_wo_softmax, innv_name, \n",
    "                                                         replacement_layer_idx=replacement_layer_idx,\n",
    "                                                         **kwargs)\n",
    "    repl_analyser.create_analyzer_model()\n",
    "    \n",
    "    \n",
    "    analyser = innvestigate.create_analyzer(innv_name, model_wo_softmax, \n",
    "                                             reverse_keep_tensors= True, \n",
    "                                             **kwargs)\n",
    "    analyser.create_analyzer_model()\n",
    "    for img_idx, (img, _) in tqdm.tqdm_notebook(zip(selected_img_idxs, val_images), desc=label):\n",
    "        channels = repl_shape[-1]\n",
    "        if label == \"$\\\\alpha=100, \\\\beta=99$-LRP\":\n",
    "            # a=100,b=99 sufferes numerical instabilities with std = 1\n",
    "            std = 1 / np.sqrt(channels)\n",
    "        else:\n",
    "            std = 30\n",
    "\n",
    "        replacement = std*np.random.normal(size=(5, ) + repl_shape[1:]) \n",
    "        img_tiled = np.tile(img, (5, 1, 1, 1))\n",
    "        hmap = repl_analyser.analyze([img_tiled, replacement])\n",
    "        \n",
    "        repl_hidden = [h for (_, h) in repl_analyser._reversed_tensors[1:]]\n",
    "        _ = analyser.analyze(img)\n",
    "        plain_hidden = [h for (_, h) in analyser._reversed_tensors[1:]]\n",
    "\n",
    "        #replacement = std * np.random.normal(size=(n_sampled_v,) + repl_shape[1:]) \n",
    "        #outs = repl_analyser.get_cosine(np.tile(img, (10, 1, 1, 1)), replacement, [v for (_, v) in intermediate_values[1:][::-1]])\n",
    "        #cos_sim[label, replacement_layer_idx, img_idx] = [np.abs(o) for o in outs]\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(U, V):\n",
    "    v_norm =  V / np.linalg.norm(V, axis=0, keepdims=True)\n",
    "    u_norm = U / np.linalg.norm(U, axis=0, keepdims=True)\n",
    "    return (v_norm.T @ u_norm)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_as_matrix(w):\n",
    "    kh, kw, cin, cout = w.shape\n",
    "    return w.reshape(kh*kw*cin, cout) \n",
    "\n",
    "for layer_name, layer in zip(nice_layer_names, model_wo_softmax.layers):\n",
    "    try:\n",
    "        w, b = layer.get_weights()\n",
    "    except ValueError:\n",
    "        continue\n",
    "    if len(w.shape) == 4:\n",
    "        w = kernel_as_matrix(w)\n",
    "        \n",
    "    #s = np.linalg.eigvals(w.T)\n",
    "    if False:\n",
    "        u, s, v = np.linalg.svd(w.T)\n",
    "        plt.title(layer_name)\n",
    "        plt.plot(s[np.argsort(s)])\n",
    "        plt.show()\n",
    "        print(layer_name, s.min(), s.max())\n",
    "\n",
    "    cin, cout = w.shape\n",
    "    print(w.shape)\n",
    "    plt.title(layer_name)\n",
    "    plt.hist(((w @ relu(np.random.normal(size=(cout, 100))))).flatten(), bins=25)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(layer_name)\n",
    "    plt.hist((relu(w @ relu(np.random.normal(size=(cout, 100))))).flatten(), bins=25)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = model_wo_softmax.layers[-2].get_weights()\n",
    "plt.hist((w.T @ plain_hidden[-2].T).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(relu(w.T @ relu(plain_hidden[-2].T)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(w.T @ relu(plain_hidden[-2].T).flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(relu(w.T @ plain_hidden[-2].T).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hidden_values[-3][:, :100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hidden_values[-2][:, :100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((w.T @ relu(hidden_values[-2].T)).T[:, :100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((w.T @ relu(hidden_values[-2].T)).T[:, :100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((w.T @ (hidden_values[-2].T + b[:, None])).T[:, :100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(b)), b, marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = model.layers[-2].get_weights()\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_values[0].max(-1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hidden_values[0].max(-1)[4])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_eig = np.linalg.eigvals(w)\n",
    "plt.plot(w_eig[np.argsort(w_eig)[::-1]])\n",
    "print(w_eig[np.argsort(w_eig)][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cosine_similarity(hidden_values[-3].T, hidden_values[-3].T))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.abs(o).mean() for o in outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmaps = []\n",
    "repr2 = []\n",
    "for _ in range(128):\n",
    "    replacement = np.random.normal(size=repl_shape)\n",
    "    hmap = repl_analyser.analyze([ex_image, replacement])\n",
    "    hmaps.append(hmap[0])\n",
    "    repr2.append(repl_analyser._reversed_tensors[2][1])\n",
    "    #print(hmap.min(), hmap.max())\n",
    "    #plt.imshow(hmap[0].mean(-1), cmap='seismic')\n",
    "    #plt.colorbar()\n",
    "hmaps = np.stack(hmaps)\n",
    "repr2 = np.stack(repr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ten in repl_analyser._reversed_tensors:\n",
    "    print(ten[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_analyser._reverse_tensors_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(hmaps[:, 100, 100].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(repr2[:, 0, 10, 10])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(np.corrcoef(repr2[:, 0, 10, 10], repr2[:, 0, 10, 10].mean(0, keepdims=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(np.corrcoef(repr2[:, 0, 10, 10], repr2[:, 0, 10, 10].mean(0, keepdims=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(np.corrcoef(repr2[:, 0, 100, 100].T)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_neg(x):\n",
    "    vmax = np.percentile(x, 99)\n",
    "    return np.clip(x / vmax, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(np.corrcoef(repr2[:, 0, 90, 90].T)), vmin=0, vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median(v):\n",
    "    v = tf.reshape(v, [-1])\n",
    "    mid = v.get_shape()[0]//2 + 1\n",
    "    return tf.nn.top_k(v, mid).values[-1]\n",
    "\n",
    "class ReplacementGradientAnalyzer:\n",
    "    def __init__(self, analyzer, with_norm=True, is_conv=True):\n",
    "        if not hasattr(analyzer, '_analyzer_model'):\n",
    "            analyzer.create_analyzer_model()\n",
    "        self.analyzer = analyzer\n",
    "        analyzer_model = self.analyzer._analyzer_model\n",
    "        self.input_img, self.rk = analyzer_model.inputs\n",
    "        r1 = analyzer_model.outputs[0]\n",
    "        \n",
    "            \n",
    "        if with_norm:\n",
    "            r1_perm = tf.stack([r1[:, :, :, i] for i in [1, 2, 0]], axis=-1)\n",
    "            r1_reduced = tf.reduce_mean(tf.norm(r1 - r1_perm, axis=-1) / tf.norm(r1, axis=-1))\n",
    "        else:\n",
    "            r1_reduced = tf.reduce_mean(np.abs(r1))\n",
    "\n",
    "        self.grad_rk, = tf.gradients([r1_reduced], [self.rk])\n",
    "        \n",
    "        if is_conv:\n",
    "            grad_mean = tf.reduce_mean(self.grad_rk, (1, 2))\n",
    "        else:\n",
    "            grad_mean = self.grad_rk\n",
    "            \n",
    "        grad_flat = tf.reshape(grad_mean, (-1,))\n",
    "        self.grad_l2 = tf.norm(grad_flat, 2)\n",
    "        self.grad_l1 = tf.norm(grad_flat, 1)\n",
    "        self.grad_max = tf.reduce_max(grad_flat)\n",
    "        self.grad_median = get_median(grad_flat) \n",
    "        \n",
    "    def r1_wrt_rk(self, input_img, relevance_k):\n",
    "        sess = keras.backend.get_session() \n",
    "        return sess.run([self.grad_l2, self.grad_l1, self.grad_max, self.grad_median], {\n",
    "            self.input_img: input_img, \n",
    "            self.rk: relevance_k,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_median_r1_rk = OrderedDict()\n",
    "grads_max_r1_rk = OrderedDict()\n",
    "grads_l2_r1_rk = OrderedDict()\n",
    "grads_l1_r1_rk = OrderedDict()\n",
    "replacement_shapes = OrderedDict()\n",
    "for name, analyser_str, _, analyser_kwargs in analysers:\n",
    "    replacements = OrderedDict()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        keras.backend.set_session(sess)\n",
    "        _, model_wo_softmax, _ = load_model(load_weights=True)\n",
    "        for layer_name, layer_idx in tqdm.tqdm_notebook(selected_layers[::-1], desc=name):\n",
    "            replacement_layer_idx = layer_idx\n",
    "            if layer_idx == 1:\n",
    "                replacement_layer_idx = 2\n",
    "            analyzer, replacement_shape = get_replacement_analyser(\n",
    "                model_wo_softmax, analyser_str,  replacement_layer_idx=layer_idx, \n",
    "                **analyser_kwargs)\n",
    "            if layer_idx not in replacement_shapes:\n",
    "                replacement_shapes[layer_idx] = replacement_shape\n",
    "            grad_analyzer = ReplacementGradientAnalyzer(analyzer, is_conv=len(replacement_shape) == 4)\n",
    "            for img_idx, (img_pp, target) in zip(selected_img_idxs, val_images):\n",
    "\n",
    "                if (layer_idx, img_idx) not in replacements:\n",
    "                    if layer_idx >= 18:\n",
    "                        replacements[layer_idx, img_idx] = np.random.normal(size=replacement_shape)\n",
    "                    else:\n",
    "                        shp = np.array(replacement_shape)\n",
    "                        shp[:-1] = 1\n",
    "                        repeat = np.array(replacement_shape)\n",
    "                        repeat[-1] = 1\n",
    "                        replacements[layer_idx, img_idx] = np.tile(np.random.normal(size=shp), repeat)\n",
    "\n",
    "                grad_l2, grad_l1, grad_max, grad_median = grad_analyzer.r1_wrt_rk(\n",
    "                    img_pp, replacements[layer_idx, img_idx])\n",
    "                grads_median_r1_rk[name, layer_idx, img_idx] = grad_median\n",
    "                grads_max_r1_rk[name, layer_idx, img_idx] = grad_max\n",
    "                grads_l2_r1_rk[name, layer_idx, img_idx] = grad_l2\n",
    "                grads_l1_r1_rk[name, layer_idx, img_idx] = grad_l1\n",
    "\n",
    "            del analyzer\n",
    "            del grad_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(grads_max_r1_rk.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '$\\\\alpha=1, \\\\beta=0$-LRP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grads_l2_r1_rk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx_2_name = OrderedDict([(i, n) for n, i in selected_layers])\n",
    "layer_idx_2_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(layer_idx_2_name[i], np.prod(shp)) for i, shp in replacement_shapes.items()]\n",
    "n_neurons_per_layer = np.array([np.prod(shp) for i, shp in replacement_shapes.items()])\n",
    "n_neurons_per_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in attr_names:\n",
    "    grads = np.array([[grads_l1_r1_rk[name, layer_idx, img_idx] \n",
    "                       for img_idx in selected_img_idxs] \n",
    "                      for (_, layer_idx) in selected_layers])\n",
    "    #_ = plt.plot(grads * n_neurons_per_layer[::-1, None])\n",
    "    _ = plt.plot(grads)\n",
    "    #plt.ylim(0, 0.0001)\n",
    "    plt.title(name)\n",
    "    plt.xticks(ticks=np.arange(len(selected_layers)), labels=[n for (n, _) in selected_layers], rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(grads_median_r1_rk.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(grads_l2_r1_rk.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplacePatternNet = create_replacement_class(PatternNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern_net = PatternNet(model_wo_softma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_net\n",
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_layer_idx = 10\n",
    "replacement_shape = model_output_shapes[replacement_layer_idx - 1]\n",
    "\n",
    "replace_lrp = ReplaceLRPAlpha1Beta0(model_wo_softmax, model_wo_softmax.layers[replacement_layer_idx], \n",
    "                                     model_output_shapes[replacement_layer_idx - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pdb off\n",
    "replace_pattern_net = ReplacePatternNet(model_wo_softmax, model_wo_softmax.layers[replacement_layer_idx], \n",
    "                                        model_output_shapes[replacement_layer_idx - 1], patterns=patterns)\n",
    "\n",
    "replace_pattern_net.create_analyzer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_model = replace_pattern_net._analyzer_model\n",
    "r1 = analyzer_model.outputs[0]\n",
    "rl = analyzer_model.inputs[1]\n",
    "r1_mean = tf.reduce_mean(r1 / tf.norm(r1, axis=-1, keepdims=True))\n",
    "#r1_mean = tf.reduce_mean(np.abs(r1))\n",
    "grad_rl, = tf.gradients([r1_mean], [rl])\n",
    "sess = keras.backend.get_session() \n",
    "tf_input, rl = analyzer_model.inputs\n",
    "\n",
    "grad = sess.run([grad_rl], {\n",
    "    tf_input: ex_image, \n",
    "    rl: np.random.normal(size=replacement_shape)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Weight & Logit Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cascading.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "model_cascading_wo_softmax = iutils.keras.graph.model_wo_softmax(model_cascading)\n",
    "\n",
    "# heatmaps are saved in those dicts\n",
    "hmap_original = OrderedDict()\n",
    "hmap_random_weights = OrderedDict()\n",
    "hmap_random_target = OrderedDict()\n",
    "\n",
    "for name, analyser, _, analyser_kwargs in analysers:\n",
    "    cascading_heatmaps = {}\n",
    "    cascading_outputs = {}\n",
    "    model_cascading.set_weights(model.get_weights())\n",
    "    \n",
    "    last_idx = len(model.layers)\n",
    "    \n",
    "    analyzer_cascading = create_analyzer(analyser, model_cascading_wo_softmax, \n",
    "                                                      neuron_selection_mode=\"index\", \n",
    "                                                      **analyser_kwargs)\n",
    "    \n",
    "    for img_idx, (img_pp, target) in zip(selected_img_idxs, val_images):\n",
    "        random_target = get_random_target(target)\n",
    "        hmap_random_target[(name, img_idx)] = (\n",
    "            random_target, analyzer_cascading.analyze(img_pp, neuron_selection=random_target)[0])\n",
    "    for layer_name, layer_idx in tqdm.tqdm_notebook([('original', last_idx)] + selected_layers[::-1], desc=name):\n",
    "        copy_weights(model_cascading, model_random, range(layer_idx, last_idx))\n",
    "        last_idx = layer_idx\n",
    "        if recreate_analyser:\n",
    "            analyzer_cascading = create_analyzer(\n",
    "                analyser, model_cascading_wo_softmax, \n",
    "                neuron_selection_mode=\"index\",  **analyser_kwargs)\n",
    "            \n",
    "        for img_idx, (img_pp, target) in zip(selected_img_idxs, val_images):\n",
    "            hmap = analyzer_cascading.analyze(img_pp, neuron_selection=target)[0]\n",
    "            if layer_idx == n_layers:\n",
    "                hmap_original[(name, img_idx)] = hmap\n",
    "            else:\n",
    "                hmap_random_weights[(name, img_idx, layer_idx)] =  hmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_heatmaps = False\n",
    "\n",
    "if dump_heatmaps:\n",
    "    with open('heatmaps.pickle', 'wb') as f:\n",
    "        pickle.dump((hmap_original, hmap_random_weights, hmap_random_target, analysers), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of dump\n",
    "! ls -lh 'heatmaps.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_heatmaps = False\n",
    "if load_heatmaps:\n",
    "    with open('heatmaps.pickle', 'rb') as f:\n",
    "        hmap_original, hmap_random_weights, hmap_random_target, analysers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = sorted(set([n for (n, _) in hmap_original.keys()]))\n",
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorting(n):\n",
    "    attr_names_sorting = {\n",
    "     'DTD': -1,\n",
    "     'GuidedBP': -1,\n",
    "     'SmoothGrad': -1,\n",
    "     '$\\\\alpha=1, \\\\beta=0$-LRP': 1,\n",
    "     '$\\\\alpha=2, \\\\beta=1$-LRP': 2,\n",
    "     '$\\\\alpha=100, \\\\beta=99$-LRP': 3,\n",
    "    }\n",
    "    if n in attr_names_sorting:\n",
    "        return attr_names_sorting[n]\n",
    "    elif \"epsilon\" in n:\n",
    "        return 0\n",
    "    elif \"cmp-LRP\" in n:\n",
    "        return 10\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "attr_names = sorted(attr_names, key=lambda x: (get_sorting(x), x))\n",
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim = OrderedDict()\n",
    "l2_random_weights = OrderedDict()\n",
    "\n",
    "last_idx = len(model.layers)\n",
    "for (name, img_idx, layer_idx), heatmap in tqdm.tqdm_notebook(hmap_random_weights.items()):\n",
    "    original_heatmap = hmap_original[(name, img_idx)]\n",
    "    postprocess = hmap_postprocesisng[name]\n",
    "    original_heatmap = postprocess(original_heatmap)\n",
    "    heatmap = postprocess(heatmap)\n",
    "    ssim[(name, img_idx, layer_idx)] = ssim_flipped(heatmap, original_heatmap)\n",
    "    l2_random_weights[(name, img_idx, layer_idx)] = l2_flipped(heatmap, original_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_random_target = OrderedDict()\n",
    "for (name, img_idx), (_, hmap_random) in tqdm.tqdm_notebook(hmap_random_target.items()):\n",
    "    if name not in ssim_random_target:\n",
    "        ssim_random_target[name] = []\n",
    "        \n",
    "    postprocess = hmap_postprocesisng[name]\n",
    "    \n",
    "    hmap = hmap_original[name, img_idx]\n",
    "    original_heatmap = postprocess(original_heatmap)\n",
    "    hmap = postprocess(hmap)\n",
    "    hmap_random = postprocess(hmap_random)\n",
    "    ssim_random_target[name].append(\n",
    "        ssim_flipped(hmap, hmap_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid', {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3.0), squeeze=True)\n",
    "    mean_ssim = [np.mean(s) for s in ssim_random_target.values()]\n",
    "    \n",
    "    names = ssim_random_target.keys()\n",
    "    bars = ax.bar(names, mean_ssim, \n",
    "           color=[linestyles[name]['color'] for name in attr_names])\n",
    "    \n",
    "    xlabels = ssim_random_target.keys()\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_xticks(np.arange(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    \n",
    "    \n",
    "    fig.savefig('check-random-logit.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    display(IFrame('check-random-logit.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_random_target.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('ticks', {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3.0), squeeze=True)\n",
    "    \n",
    "    \n",
    "    xlabels = attr_names\n",
    "    bars = ax.boxplot([ssim_random_target[n] for n in attr_names]) \n",
    "    ax.set_ylabel('SSIM')\n",
    "    #ax.set_xticks(np.arange(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    fig.savefig('check-random-logit-boxplot.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    display(IFrame('check-random-logit-boxplot.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_reduce = 'median'\n",
    "confidence_intervals = True\n",
    "confidence_percentile = 99.5\n",
    "with sns.axes_style(\"ticks\", {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    # metrics = [('SSIM', ssim), ('MSE', l2_random_weights)]\n",
    "    metrics = [('SSIM', ssim)]\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(4.5 * len(metrics), 3.5), squeeze=False)\n",
    "    axes = axes[0]\n",
    "    for ax, (ylabel, metric) in zip(axes, metrics): \n",
    "        for name in attr_names:\n",
    "            metric_per_layer = []\n",
    "            \n",
    "            lower_conf = []\n",
    "            upper_conf = []\n",
    "            for (_, layer_idx) in selected_layers[::-1]:\n",
    "                metric_per_layer.append(\n",
    "                    [metric[(name, img_idx, layer_idx)] for img_idx in selected_img_idxs]\n",
    "                )\n",
    "                if confidence_intervals:\n",
    "                    vals = np.array(metric_per_layer[-1])\n",
    "                    ridx = np.random.choice(len(vals), (10000, len(vals)), replace=True)\n",
    "                    resample = vals[ridx]\n",
    "                    stats = np.median(resample, 1)\n",
    "                    lower_conf.append(np.percentile(stats, 100 - confidence_percentile))\n",
    "                    upper_conf.append(np.percentile(stats, confidence_percentile))\n",
    "                    \n",
    "            metric_per_layer = np.array(metric_per_layer)\n",
    "                \n",
    "            if ssim_reduce == 'mean':\n",
    "                ssims_reduced = metric_per_layer.mean(1)\n",
    "            elif ssim_reduce == 'median':\n",
    "                ssims_reduced = np.median(metric_per_layer, 1)\n",
    "            \n",
    "            ticks = np.arange(len(ssims_reduced))\n",
    "            ax.plot(ticks, ssims_reduced, label=name, **linestyles[name])\n",
    "            ax.fill_between(ticks, lower_conf, upper_conf, \n",
    "                            color=linestyles[name]['color'],\n",
    "                            alpha=0.25\n",
    "                           )\n",
    "            #ax.plot(ticks, lower_conf, color=linestyles[name]['color'])\n",
    "            #ax.plot(ticks, upper_conf, color=linestyles[name]['color'])\n",
    "            \n",
    "        xlabels = [layer_name for layer_name, _ in selected_layers[::-1]] \n",
    "        ax.set_ylim([0, 1.05])\n",
    "        ax.set_xticks(np.arange(len(xlabels)))\n",
    "        ax.set_xticklabels(xlabels, rotation=90)\n",
    "        ax.set_ylabel(ylabel)\n",
    "    axes[-1].legend(bbox_to_anchor=(1.0, 1.00))\n",
    "    plt.savefig('check-random-weights.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    display(IFrame('check-random-weights.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_grid(heatmaps, cols, row_labels=[], column_labels=[], fig_path=None):\n",
    "    mpl.rcParams['font.family'] = 'serif'\n",
    "    rows = len(heatmaps) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows), squeeze=False)\n",
    "    fontsize = 12\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05, top=1, bottom=0, left=0, right=1)\n",
    "    \n",
    "    for label, ax in zip(row_labels, axes[:, 0]):\n",
    "        ax.set_ylabel(label, fontsize=fontsize - 1, labelpad=55, rotation=0)\n",
    "        \n",
    "    for label, ax in zip(column_labels, axes[0, :]):\n",
    "        ax.set_title(label, fontsize=fontsize - 1)\n",
    "        \n",
    "        \n",
    "    for ax, heatmap in zip(axes.flatten(), heatmaps):\n",
    "        ax.imshow(heatmap, cmap='seismic', vmin=-1, vmax=1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    if fig_path is not None:\n",
    "        plt.savefig(fig_path, bbox_inches='tight', pad_inches=0, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_neg(x):\n",
    "    vmax = np.percentile(x, 99)\n",
    "    return np.clip(x / vmax, -1, 1)\n",
    "\n",
    "hmap_plot = []\n",
    "for attr_name in attr_names:\n",
    "    postp = hmap_postprocesisng[attr_name]\n",
    "    if attr_name ==  '$\\\\alpha=1, \\\\beta=0$-LRP':\n",
    "        postp_bar = hmap_postprocesisng[attr_name]\n",
    "        postp = lambda x: flip_hmap(normalize(postp_bar(x)))\n",
    "        \n",
    "    for img_idx in selected_img_idxs[:1]:\n",
    "        hmap_plot.append(norm_image(val_images[0][0][0]))\n",
    "        hmap_plot.append(normalize_neg(postp(hmap_original[attr_name, img_idx])))\n",
    "        for (_, layer_idx) in selected_layers[::-1]:\n",
    "            hmap_plot.append(normalize_neg(postp(hmap_random_weights[attr_name, img_idx, layer_idx])))\n",
    "\n",
    "plot_heatmap_grid(\n",
    "    hmap_plot, 2+len(selected_layers), row_labels=attr_names, \n",
    "    column_labels=['input', 'original'] + [n for (n, _) in selected_layers][::-1],\n",
    "    fig_path='heatmap_grid.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('heatmap_grid.pdf', width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop ipynb execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ssim.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'DTD'\n",
    "layer_idx = selected_layers[4][1]\n",
    "ssim_per_layer = []\n",
    "for img_idx in selected_img_idxs:\n",
    "    ssim_per_layer.append(ssim[method, img_idx, layer_idx])\n",
    "    \n",
    "ssim_per_layer = np.array(ssim_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "medians = []\n",
    "\n",
    "ridx = np.random.choice(n_selected_imgs, (10000, n_selected_imgs), replace=True)\n",
    "\n",
    "means = ssim_per_layer[ridx].mean(1)\n",
    "median = np.median(ssim_per_layer[ridx], 1)\n",
    "print(np.percentile(median, 99))\n",
    "print(np.percentile(median, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance w.r.t. Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_idx(neuron_selection, batch_size=1):\n",
    "    neuron_selection = np.asarray(neuron_selection).flatten()\n",
    "    if neuron_selection.size == 1:\n",
    "        neuron_selection = np.repeat(neuron_selection, batch_size)\n",
    "\n",
    "    # Add first axis indices for gather_nd\n",
    "    neuron_selection = np.hstack(\n",
    "        (np.arange(len(neuron_selection)).reshape((-1, 1)),\n",
    "         neuron_selection.reshape((-1, 1)))\n",
    "    )\n",
    "    return neuron_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_wrt_kernels = OrderedDict()\n",
    "for name, innv_name, innv_kwargs in analysers:\n",
    "    print(name)\n",
    "    analyzer = create_analyzer(\n",
    "        innv_name, model_wo_softmax, neuron_selection_mode=\"index\", **innv_kwargs)\n",
    "    analyzer.create_analyzer_model()\n",
    "    try:\n",
    "        analyzer_model = analyzer._analyzer_model\n",
    "    except:\n",
    "        continue\n",
    "    input_image, input_idx = analyzer_model.inputs\n",
    "\n",
    "    rel_mean = tf.reduce_sum(analyzer_model.outputs[0])\n",
    "    rel_grad_w = tf.gradients(\n",
    "        rel_mean,\n",
    "        analyzer_model.weights,\n",
    "    )\n",
    "    sess = keras.backend.get_session()\n",
    "    outs = sess.run(analyzer._analyzer_model.outputs + rel_grad_w, \n",
    "                    {input_image: ex_image, input_idx: prepare_idx(ex_target)})\n",
    "    \n",
    "    output = outs[0]\n",
    "    rel_grad_w_np = outs[1:]\n",
    "    print(len(rel_grad_w_np))\n",
    "    for i, w in enumerate(analyzer_model.weights):\n",
    "        if \"kernel\" in w.name:\n",
    "            #print(i, w.name)\n",
    "            rel_wrt_kernels[name, ex_image_idx, i, w.name.rstrip(\"/kernel:0\")] = rel_grad_w_np[i]\n",
    "\n",
    "    print(output.min(), output.max())\n",
    "    plt.imshow(normalize(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _, _) in analysers:\n",
    "    grads = []\n",
    "    percentile = 99\n",
    "    lower_percentile = []\n",
    "    upper_percentile = []\n",
    "    layer_names = []\n",
    "    for (gname, _, idx, layer_name), grad in rel_wrt_kernels.items():\n",
    "        if name == gname:\n",
    "            grads.append(np.abs(grad).mean())\n",
    "            lower_percentile.append(np.percentile(np.abs(grad), 100-percentile))\n",
    "            upper_percentile.append(np.percentile(np.abs(grad), percentile))\n",
    "            layer_names.append(layer_name)\n",
    "            \n",
    "    plt.title(name)\n",
    "    plt.semilogy(grads)\n",
    "    plt.semilogy(lower_percentile)\n",
    "    plt.semilogy(upper_percentile)\n",
    "    plt.ylim(1e2, 1e-16)\n",
    "    plt.xticks(ticks=np.arange(len(rel_wrt_kernels)), labels=layer_names, rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _, _) in analysers:\n",
    "    grads = []\n",
    "    percentile = 99\n",
    "    lower_percentile = []\n",
    "    upper_percentile = []\n",
    "    layer_names = []\n",
    "    layer_idxs = []\n",
    "    \n",
    "    n_layers = 0\n",
    "    i = 0\n",
    "    for ((gname, _, idx, layer_name), grad) in rel_wrt_kernels.items():\n",
    "        if name == gname:\n",
    "            grads.append(np.abs(grad).flatten())\n",
    "            layer_idxs.append(i*np.ones_like(grads[-1]))\n",
    "            layer_names.append(layer_name)\n",
    "            i += 1\n",
    "          \n",
    "    if len(grads) == 0:\n",
    "        continue\n",
    "     \n",
    "    layer_idxs = np.concatenate(layer_idxs)\n",
    "    grads = np.concatenate(grads)\n",
    "    print('.')\n",
    "    \n",
    "    idx = np.random.choice(len(grads), size=100000)\n",
    "    print('.')\n",
    "    \n",
    "    plt.title(name)\n",
    "    plt.scatter(layer_idxs[idx], grads[idx], marker='_', alpha=0.1)\n",
    "    plt.yscale('log')\n",
    "    plt.ylim([1000, 1e-8])\n",
    "    plt.xticks(ticks=np.arange(len(layer_names)), labels=layer_names, rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _, _) in analysers:\n",
    "    grads = []\n",
    "    layer_names = []\n",
    "    for (gname, _, idx, layer_name), grad in rel_wrt_kernels.items():\n",
    "        if name == gname:\n",
    "            grads.append((np.abs(grad) == 0).sum())\n",
    "            layer_names.append(layer_name)\n",
    "            \n",
    "    plt.title(name)\n",
    "    plt.scatter(np.arange(len(grads)), grads)\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(1e9, 1)\n",
    "    plt.xticks(ticks=np.arange(len(grads)), labels=layer_names, rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
