{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Explanations Lie: Why Modified BP Attribution fails\n",
    "\n",
    "This notebook produces the cosine similaries of the relevance vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install install packages\n",
    "# !pip install tensorflow-gpu==1.13.1\n",
    "# !pip install innvestigate seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "import innvestigate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import copy\n",
    "import contextlib\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage.measure import compare_ssim \n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "from innvestigate.analyzer.relevance_based.relevance_analyzer import LRP\n",
    "from innvestigate.analyzer.base import AnalyzerNetworkBase, ReverseAnalyzerBase\n",
    "from innvestigate.analyzer.deeptaylor import DeepTaylor\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "from when_explanations_lie import *\n",
    "from monkey_patch import custom_add_bn_rule\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def _prepare_model(self, model):\n",
    "#    return super(DeepTaylor, self)._prepare_model(model)\n",
    "#\n",
    "## otherwise DTD does not work on negative outputs\n",
    "#DeepTaylor._prepare_model = _prepare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to imagenet validation\n",
    "imagenet_val_dir = \"/mnt/ssd/data/imagenet/imagenet-raw/validation\"\n",
    "#imagenet_val_dir = \"/home/leonsixt/tmp/imagenet/imagenet-raw/validation/\"\n",
    "# path to examplary image\n",
    "ex_image_path = \"n01534433/ILSVRC2012_val_00015410.JPEG\"\n",
    "# number of images to run the evaluation\n",
    "#n_selected_imgs = 200\n",
    "n_selected_imgs = 10\n",
    "\n",
    "load_weights = True\n",
    "model_names = ['resnet50', 'vgg16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model, innv_net, color_conversion = load_model('vgg16', load_weights)\n",
    "ex_image_vgg, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "    innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model, innv_net, color_conversion = load_model('resnet50', load_weights)\n",
    "ex_image, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "    innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "\n",
    "assert ((ex_image - ex_image_vgg) == 0).all()\n",
    "\n",
    "nice_layer_names = get_nice_layer_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = {'vgg16': 22, 'resnet50': 177}\n",
    "\n",
    "replacement_layers = {\n",
    "    'vgg16':  ['fc3', 'fc1', 'conv4_3', 'conv3_3', 'conv2_2'],\n",
    "    'resnet50': ['dense', 'block5_1', 'block4_2', 'block3_4', 'block3_2', 'block2_2'],\n",
    "}\n",
    "\n",
    "output_shapes = get_output_shapes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmap_postprocess_wrapper(name):\n",
    "    return lambda x: heatmap_postprocess(name, x)\n",
    "\n",
    "input_range = (ex_image.min(), ex_image.max())\n",
    "analysers = get_analyser_params(input_range)\n",
    "\n",
    "attr_names = [n for (n, _, _, _, _) in analysers]\n",
    "    \n",
    "hmap_postprocessing = {\n",
    "    n: hmap_postprocess_wrapper(post_name) for n, _, post_name, _, _ in analysers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.9, 10).tolist() + [0.99, 0.999, 0.9999, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reversed(hidden):\n",
    "    return [h[1] for h in hidden[1:]]\n",
    "\n",
    "\n",
    "dead_neuron_mask = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    keras.backend.clear_session()\n",
    "    model, innv_net, _ = load_model(model_name, load_weights=True)\n",
    "    analyser = innvestigate.create_analyzer(\n",
    "        \"gradient\", model, reverse_keep_tensors=True)\n",
    "    \n",
    "    analyser.analyze(np.concatenate([img for (img, _) in val_images[:20]], 0))\n",
    "    \n",
    "    grad_hidden = parse_reversed(analyser._reversed_tensors) \n",
    "    dead_neuron_mask[model_name] = [(0 == np.mean(g, 0, keepdims=True)).all(-1, keepdims=True) for g in grad_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    plt.title(model_name + \" - active neurons\")\n",
    "    plt.plot([(m.sum(-1) / m.shape[-1] > 0.999999).mean() for m in dead_neuron_mask[model_name]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_layers = copy.deepcopy(replacement_layers)\n",
    "histogram_layers['vgg16'].extend(['conv1_1', 'input'])\n",
    "histogram_layers['resnet50'].extend(['conv2_1a', 'input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "histogram_layers_idx = OrderedDict()\n",
    "for model_name in model_names:\n",
    "    histogram_layers_idx[model_name] = []\n",
    "    for layer_name in histogram_layers[model_name]:\n",
    "        idx = get_layer_idx_full(model_name, nice_layer_names, layer_name)\n",
    "        histogram_layers_idx[model_name].append(idx) \n",
    "histogram_layers_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dead_neuron_mask['vgg16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from innvestigate.analyzer.relevance_based.relevance_rule import AlphaBetaRule\n",
    "\n",
    "def alpha_beta_wrapper(alpha, beta):\n",
    "    class AlphaBetaRuleWrapper(AlphaBetaRule):\n",
    "        def __init__(self, layer, state, bias=True, copy_weights=False):\n",
    "            super(AlphaBetaRuleWrapper, self).__init__(layer, state, alpha=alpha, beta=beta, \n",
    "                             bias=bias, copy_weights=copy_weights)\n",
    "            \n",
    "        def __repr__(self):\n",
    "            return \"AlphaBetaRuleWrapper(alpha={}, beta={})\".format(self._alpha, self._beta)\n",
    "        \n",
    "    return AlphaBetaRuleWrapper\n",
    "\n",
    "def get_custom_rule(innv_name, kwargs):\n",
    "    if innv_name == 'lrp.alpha_beta':\n",
    "        return alpha_beta_wrapper(kwargs['alpha'], kwargs['beta'])\n",
    "    elif innv_name == 'lrp.sequential_preset_a':\n",
    "        return alpha_beta_wrapper(1, 0)\n",
    "    elif innv_name == 'lrp.sequential_preset_b':\n",
    "        return alpha_beta_wrapper(2, 1)\n",
    "        \n",
    "for label, innv_name, _, excludes, kwargs in analysers:\n",
    "    print(innv_name, get_custom_rule(innv_name, kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_layers= {'vgg16': ['fc3'], 'resnet50': ['dense']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names, replacement_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement_layer_indices = [22]\n",
    "n_sampled_v = 5\n",
    "\n",
    "cos_sim_histograms = {}\n",
    "cos_mean = {}\n",
    "selected_percentiles = [0, 1, 5, 10, 20, 50, 100]\n",
    "cos_sim_percentiles = {}\n",
    "\n",
    "for label, innv_name, _, excludes, kwargs in tqdm.tqdm_notebook(analysers[4:]):\n",
    "    if 'exclude_cos_sim' in excludes:\n",
    "        continue\n",
    "    for model_name in model_names[:1]:\n",
    "        if 'exclude_' + model_name in excludes:\n",
    "            continue\n",
    "        keras.backend.clear_session()\n",
    "        model_wo_softmax, innv_net, _ = load_model(model_name, load_weights=load_weights)\n",
    "        if innv_name == \"pattern.attribution\":\n",
    "            kwargs['patterns'] = innv_net['patterns']\n",
    "\n",
    "        for replacement_layer in replacement_layers[model_name]:\n",
    "            replacement_layer_idx = get_layer_idx_full(model_name, nice_layer_names, replacement_layer)\n",
    "            custom_rule = get_custom_rule(innv_name, kwargs)\n",
    "            with custom_add_bn_rule(custom_rule):\n",
    "                repl_analyser, repl_shape = get_replacement_analyser(\n",
    "                    model_wo_softmax, innv_name,  \n",
    "                    replacement_layer_idx=replacement_layer_idx,\n",
    "                    **kwargs)\n",
    "                # repl_analyser.create_analyzer_model()\n",
    "                cos_per_img = OrderedDict()\n",
    "                for img_idx, (img, _) in tqdm.tqdm_notebook(zip(selected_img_idxs, val_images), \n",
    "                    desc=\"[{}.{}] {}\".format(model_name, replacement_layer, label)):\n",
    "                    channels = repl_shape[-1]\n",
    "                    if label == \"$\\\\alpha=100, \\\\beta=99$-LRP\":\n",
    "                        # a=100,b=99 sufferes numerical instabilities with std = 1\n",
    "                        std = 1 / np.sqrt(channels)\n",
    "                    else:\n",
    "                        std = 1\n",
    "\n",
    "                    relevance_v1 = std*np.random.normal(size=(1, ) + repl_shape[1:]) \n",
    "                    hmap = repl_analyser.analyze([img, relevance_v1])\n",
    "                    intermediate_values = parse_reversed(repl_analyser._reversed_tensors)\n",
    "\n",
    "                    relevance_v2 = std * np.random.normal(size=(n_sampled_v,) + repl_shape[1:]) \n",
    "                    img_tiled = np.tile(img, (n_sampled_v, 1, 1, 1))\n",
    "                    outs = repl_analyser.get_cosine(img_tiled, relevance_v2,  intermediate_values[::-1])\n",
    "                    outs = outs[::-1]\n",
    "                    for layer_idx, (o, dead_neuron) in enumerate(zip(outs, dead_neuron_mask[model_name])):\n",
    "                        cos_for_layer = np.abs(o)\n",
    "                        # we filter 0 cosine similarites as they only appear practically when the gradients are zero\n",
    "                        cos_per_img[model_name, layer_idx, img_idx] = cos_for_layer[cos_for_layer != 0]\n",
    "\n",
    "                median_for_label = []\n",
    "                percentile_for_label = OrderedDict([(p, []) for p in selected_percentiles])\n",
    "                for layer_idx in range(n_layers[model_name]):\n",
    "                    cos_per_layer = np.concatenate([cos_per_img[model_name, layer_idx, img_idx]  for img_idx in selected_img_idxs])\n",
    "                    cos_per_layer = cos_per_layer.flatten()\n",
    "\n",
    "                    idx = (label, model_name, replacement_layer_idx,  layer_idx)\n",
    "                    cos_mean[idx] = np.mean(cos_per_layer)\n",
    "\n",
    "                    perc_values = np.percentile(cos_per_layer,  selected_percentiles)\n",
    "                    for p, val in zip(selected_percentiles, perc_values):\n",
    "                        percentile_for_label[p].append(val)\n",
    "\n",
    "                    if layer_idx in histogram_layers_idx[model_name]:\n",
    "\n",
    "                        if len(cos_per_layer) > 50000:\n",
    "                            ridx = np.random.choice(len(cos_per_layer), 50000, replace=False)\n",
    "                            cos_per_layer_sel = cos_per_layer[ridx]\n",
    "                        else:\n",
    "                            cos_per_layer_sel = cos_per_layer\n",
    "\n",
    "                        cos_sim_histograms[idx] = np.histogram(cos_per_layer_sel, bins)\n",
    "\n",
    "\n",
    "                for p, values in percentile_for_label.items():\n",
    "                    cos_sim_percentiles[label, model_name, replacement_layer_idx, p] = np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with keras.backend.get_session().as_default():\n",
    "    print(1 - tf.losses.cosine_distance([0, 0], [0, 0], 0).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[0].shape, outs[-4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False\n",
    "if save_results:\n",
    "    os.makedirs('cache', exist_ok=True)\n",
    "    with open('cache/cos_sim_with_hist_random_weights.pickle', 'wb') as f:\n",
    "        pickle.dump((cos_sim_percentiles, cos_sim_histograms ), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_results = False\n",
    "if load_results:\n",
    "    os.makedirs('cache', exist_ok=True)\n",
    "    with open('cache/cos_sim_with_hist.pickle', 'rb') as f:\n",
    "        cos_sim_percentiles, cos_sim_histograms = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(U, V):\n",
    "    v_norm = V / np.linalg.norm(V, axis=0, keepdims=True)\n",
    "    u_norm = U / np.linalg.norm(U, axis=0, keepdims=True)\n",
    "    return v_norm.T @ u_norm\n",
    "\n",
    "def get_sample_cos_sim_per_layer(output_shapes):\n",
    "    values = []\n",
    "    for layer_idx, shp in output_shapes.items():\n",
    "        ch = shp[-1]\n",
    "        n_samples = 1000\n",
    "        u = np.random.normal(size=(ch, n_samples))\n",
    "        v = np.random.normal(size=(ch, n_samples))\n",
    "        cos = cosine_similarity(v, u)\n",
    "        mask = np.tri(cos.shape[0])\n",
    "        values.append(np.median(np.abs(cos[mask == 1])))\n",
    "    return np.array(values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_baseline = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    keras.backend.clear_session()\n",
    "    model, _, _ = load_model(model_name)\n",
    "    output_shapes = get_output_shapes(model)\n",
    "    print(len(output_shapes))\n",
    "    cos_sim_baseline[model_name] = get_sample_cos_sim_per_layer(output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_baseline['vgg16'].shape, cos_sim_baseline['resnet50'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = OrderedDict()\n",
    "\n",
    "os.makedirs('figures/cosine_similarity', exist_ok=True)\n",
    "for model_name in model_names[::-1]:\n",
    "    for replacement_layer in replacement_layers[model_name]:\n",
    "        repl_idx = get_layer_idx_full(model_name, nice_layer_names, replacement_layer)\n",
    "        start_layer = n_layers[model_name] - repl_idx \n",
    "        \n",
    "        layer_names = [name for idx, name in nice_layer_names[model_name].items()\n",
    "                       if idx <= repl_idx][::-1]\n",
    "        layer_idx = np.array([idx for idx, name in nice_layer_names[model_name].items()\n",
    "                       if idx < repl_idx][::-1])\n",
    "        \n",
    "        print(layer_idx, repl_idx, start_layer)\n",
    "        #layer_idx = layer_idxs\n",
    "        \n",
    "        plt.figure(figsize=(max(3, len(layer_idx) / 4), 3.5))\n",
    "        \n",
    "        for i, (label, _, _, _, _) in enumerate(analysers):\n",
    "            idx = (label, model_name, repl_idx, 50)\n",
    "            if idx not in cos_sim_percentiles:\n",
    "                warnings.warn(\"not found: \" + str(idx))\n",
    "                continue\n",
    "            print(len(cos_sim_percentiles[idx]))\n",
    "            cos_sim_per_label = cos_sim_percentiles[idx][layer_idx]\n",
    "            \n",
    "            #cos_sim_per_label = []\n",
    "            #for lidx in layer_idx:\n",
    "            #    cos_sim_per_label.append(cos_mean[label, model_name, repl_idx, lidx])\n",
    "            # try:\n",
    "            #     cos_sim_per_label = cos_sim_percentiles[idx][layer_idx]\n",
    "            # except IndexError:\n",
    "            #     cos_sim_per_label = (cos_sim_baseline[model_name][layer_idx[:1]].tolist() +\n",
    "            #                          cos_sim_percentiles[idx][layer_idx[1:]].tolist())\n",
    "                \n",
    "            plt.plot(0.5 + np.arange(len(cos_sim_per_label)), cos_sim_per_label, label=label, **mpl_styles[label])\n",
    "            \n",
    "            if label not in legend:\n",
    "                legend[label] = mpl_styles[label]\n",
    "            \n",
    "        # Random Cos Similarity\n",
    "        # Cos Similarity Base.\n",
    "        label='Cos Similarity BL'\n",
    "        style = {'color': (0.25, 0.25, 0.25)}\n",
    "        plt.plot(0.5 + np.arange(len(layer_idx)), cos_sim_baseline[model_name][layer_idx], \n",
    "                 # label='Cos. Sim. Baseline', \n",
    "                 label=label,\n",
    "                 **style)\n",
    "        if label not in legend:\n",
    "            legend[label] = style\n",
    "        \n",
    "        #plt.legend(bbox_to_anchor=(1, 1))\n",
    "        plt.ylabel('cosine similarity')\n",
    "        plt.xticks(np.arange(len(layer_names)), layer_names, rotation=90)\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid('on', alpha=0.35) #, axis=\"y\")\n",
    "        plt.savefig(\"./figures/cosine_similarity/{}_layer_{}.pdf\".format(model_name, repl_idx),  \n",
    "                    bbox_inches='tight', pad_inches=0)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cos_mean.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2.5, 3))\n",
    "for label, style in legend.items():\n",
    "    plt.plot([], label=label, alpha=1, **style)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend(loc='center')\n",
    "plt.savefig(\"./figures/cos_sim_legend.pdf\",\n",
    "            bbox_inches='tight', pad_inches=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame(\"./figures/cos_sim_legend.pdf\", 800, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr_name, model_name, layer_idx, percentile in cos_sim_percentiles.keys():\n",
    "    if attr_name == 'GuidedBP' and model_name == 'resnet50':\n",
    "        print(attr_name, model_name, layer_idx, percentile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_histograms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "attr_counts = []\n",
    "labels = []\n",
    "for (attr_name, model_name, repl_layer, layer_idx), (counts, bins) in cos_sim_histograms.items():\n",
    "    if layer_idx != 7:\n",
    "        #print(layer_idx)\n",
    "        continue\n",
    "    lower_09 = counts[bins[:-1] < 0.9].sum()\n",
    "    print(attr_name, counts.sum())\n",
    "    counts_collapsed = np.concatenate([lower_09[None], counts[bins[:-1] >= 0.9]])\n",
    "    bins_int = np.arange(len(counts_collapsed) + 1)\n",
    "    attr_counts.append(counts_collapsed)\n",
    "    labels.append(attr_name)\n",
    "plt.hist([bins_int[:-1]] * len(attr_counts), bins_int, \n",
    "         weights=attr_counts, stacked=True, label=labels)\n",
    "plt.xticks(bins_int, [\"{:.4g}\".format(b) for b in [0] + bins[bins >= 0.9].tolist()], rotation=0)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
