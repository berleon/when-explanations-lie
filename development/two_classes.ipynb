{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Explanations Lie: Why Modified BP Attribution fails\n",
    "\n",
    "This is the code to the paper [].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install install packages\n",
    "#!pip install innvestigate seaborn tqdm\n",
    "#!pip install tensorflow-gpu==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "import innvestigate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import copy\n",
    "import contextlib\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage.measure import compare_ssim \n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "from innvestigate.analyzer.relevance_based.relevance_analyzer import LRP\n",
    "from innvestigate.analyzer.base import AnalyzerNetworkBase, ReverseAnalyzerBase\n",
    "from innvestigate.analyzer.deeptaylor import DeepTaylor\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from scipy.special import softmax\n",
    "\n",
    "from when_explanations_lie import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_model(self, model):\n",
    "    return super(DeepTaylor, self)._prepare_model(model)\n",
    "\n",
    "# otherwise DTD does not work on negative outputs\n",
    "DeepTaylor._prepare_model = _prepare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to imagenet validation\n",
    "#imagenet_val_dir = \"/home/leonsixt/tmp/imagenet/imagenet-raw/validation\"\n",
    "imagenet_val_dir = \"/mnt/ssd/data/imagenet/imagenet-raw/validation\"\n",
    "# path to examplary image\n",
    "ex_image_path = \"n01534433/ILSVRC2012_val_00015410.JPEG\"\n",
    "# number of images to run the evaluation\n",
    "n_selected_imgs = 200\n",
    "\n",
    "model_names = ['resnet50', 'vgg16']\n",
    "\n",
    "assert os.path.exists(imagenet_val_dir)\n",
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_shapes = {}\n",
    "model, innv_net, color_conversion = load_model('vgg16')\n",
    "ex_image_vgg, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "    innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "output_shapes['vgg16'] = get_output_shapes(model)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model, innv_net, color_conversion = load_model('resnet50')\n",
    "ex_image, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "    innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "output_shapes['resnet50'] = get_output_shapes(model)\n",
    "\n",
    "assert ((ex_image - ex_image_vgg) == 0).all()\n",
    "\n",
    "nice_layer_names = get_nice_layer_names(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for img, target in tqdm.tqdm_notebook(val_images):\n",
    "    logits = model.predict(img)\n",
    "    top1 = logits.argmax()\n",
    "    correct.append(top1 == target)\n",
    "    \n",
    "print(\"Top-1 accuracy: \", np.mean(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = {'vgg16': 23, 'resnet50': 177}\n",
    "\n",
    "randomization_layers = {\n",
    "    'vgg16': [\"conv1_1\", \"conv2_1\",  \"conv3_1\", \"conv4_1\",  \"conv4_3\",  \"conv5_1\",\n",
    "        \"conv5_3\", \"fc1\", \"fc3\"],\n",
    "    'resnet50': ['conv1', 'block2_2', 'block3_2', 'block4_2', 'block5_2', 'dense'],\n",
    "}\n",
    "\n",
    "replacement_layers = {\n",
    "    'vgg16':  ['fc3', 'fc1', 'conv4_3', 'conv3_3', 'conv2_2'],\n",
    "    'resnet50': ['dense', 'block5_1', 'block4_2', 'block3_4', 'block3_2', 'block2_2'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('imagenet_labels.json', 'r') as f:\n",
    "    idx_to_label =  json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog = PIL.Image.open('images/cat_dog_224.jpg')\n",
    "cat  = PIL.Image.open('images/cat_224.jpg')\n",
    "dog  = PIL.Image.open('images/dog_224.jpg')\n",
    "\n",
    "zebra_elephant = PIL.Image.open('images/zebra_elephant_224.jpg')\n",
    "zebra  = PIL.Image.open('images/zebra_224.jpg')\n",
    "elephant  = PIL.Image.open('images/elephant_224.jpg')\n",
    "\n",
    "two_class_pil_imgs = [\n",
    "    # Persian cat, King Charles Spaniel \n",
    "    (cat_dog, cat, dog, 283, 156),  \n",
    "    # Zebra, African bush elephant\n",
    "    (zebra_elephant, zebra, elephant, 340, 386)\n",
    "]\n",
    "two_class_imgs = [(\n",
    "    preprocess(np.array(a_and_b).astype(np.float), innv_net)[None], \n",
    "    preprocess(np.array(a).astype(np.float), innv_net)[None], \n",
    "    preprocess(np.array(b, ).astype(np.float), innv_net)[None], \n",
    "    a_cls,\n",
    "    b_cls\n",
    ") for a_and_b, a, b, a_cls, b_cls in two_class_pil_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ab, a, b, a_cls, b_cls in two_class_imgs:\n",
    "    logit_ab = model.predict(ab)[0]\n",
    "    logit_a = model.predict(a)[0]\n",
    "    logit_b = model.predict(b)[0]\n",
    "    \n",
    "    idx_ab = np.argsort(logit_ab)[::-1]\n",
    "    idx_a = np.argsort(logit_a)[::-1]\n",
    "    idx_b = np.argsort(logit_b)[::-1]\n",
    "    \n",
    "    \n",
    "    prob_a = softmax(logit_a, -1)\n",
    "    prob_b = softmax(logit_b, -1)\n",
    "    \n",
    "    for top_class in idx_a[:5]:\n",
    "        print(prob_a[top_class], top_class, idx_to_label[int(top_class)])\n",
    "        #print()\n",
    "        \n",
    "    print()\n",
    "        \n",
    "    for top_class in idx_b[:5]:\n",
    "        print(prob_b[top_class], top_class, idx_to_label[int(top_class)])\n",
    "        \n",
    "    print()\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_neg(x):\n",
    "    vmax = np.percentile(x, 99)\n",
    "    vmin = np.percentile(x, 1)\n",
    "    x_pos = x * (x > 0)\n",
    "    x_neg = x * (x < 0)\n",
    "    x_pos = x_pos / vmax\n",
    "    x_neg = - x_neg / vmin\n",
    "    return np.abs(np.clip(x_pos + x_neg, -1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    keras.backend.clear_session()\n",
    "    model, innv_net, color_conversion = load_model(model_name)\n",
    "    ex_image_vgg, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "        innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "    analyser = innvestigate.create_analyzer('deep_taylor', model, neuron_selection_mode=\"index\")\n",
    "    \n",
    "    for i, (ab, a, b, a_cls, b_cls) in enumerate(two_class_imgs):\n",
    "        ab_explain_a = analyser.analyze(ab, a_cls)\n",
    "        ab_explain_b = analyser.analyze(ab, b_cls)\n",
    "        ab_explain_random = analyser.analyze(ab, np.random.choice([i for i in range(1000) if i not in [a_cls, b_cls]]))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
    "\n",
    "\n",
    "        axes[0].imshow(two_class_pil_imgs[i][0])\n",
    "        axes[0].set_title(\"Image\")\n",
    "        hmap_a = normalize_neg(ab_explain_a[0].sum(-1))\n",
    "        hmap_b = normalize_neg(ab_explain_b[0].sum(-1))\n",
    "\n",
    "        axes[1].imshow(hmap_a, vmin=-1, vmax=1, cmap='seismic')\n",
    "        axes[1].set_title(idx_to_label[a_cls])\n",
    "\n",
    "        axes[2].imshow(hmap_b, vmin=-1, vmax=1, cmap='seismic')\n",
    "        axes[2].set_title(idx_to_label[b_cls])\n",
    "\n",
    "        axes[3].imshow(hmap_a - hmap_b, vmin=-1, vmax=1, cmap='seismic')\n",
    "        axes[3].set_title('\"{}\" - \"{}\"'.format(idx_to_label[a_cls], idx_to_label[b_cls]))\n",
    "\n",
    "        print(np.abs(hmap_a - hmap_b).mean())\n",
    "        print(np.sqrt(((hmap_a - hmap_b)**2).mean()))\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow((ab_explain_a - ab_explain_b)[0].sum(-1), cmap='seismic')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "        #plt.imshow(normalize_neg(ab_explain_random[0].sum(-1)), vmin=-1, vmax=1, cmap='seismic')\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['vgg16']:\n",
    "    print(model_name)\n",
    "    keras.backend.clear_session()\n",
    "    model, innv_net, color_conversion = load_model(model_name)\n",
    "    ex_image_vgg, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "        innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "    analyser = innvestigate.create_analyzer('deep_taylor', model, neuron_selection_mode=\"index\")\n",
    "    \n",
    "    for i, (ab, a, b, a_cls, b_cls) in enumerate(two_class_imgs):\n",
    "        ab_explain_a = analyser.analyze(ab, a_cls)\n",
    "        ab_explain_b = analyser.analyze(ab, b_cls)\n",
    "        ab_explain_random = analyser.analyze(ab, np.random.choice([i for i in range(1000) if i not in [a_cls, b_cls]]))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "        axes[0].imshow(two_class_pil_imgs[i][0])\n",
    "        axes[0].set_title(\"Image\")\n",
    "        hmap_a = normalize_neg(ab_explain_a[0].sum(-1))\n",
    "        hmap_b = normalize_neg(ab_explain_b[0].sum(-1))\n",
    "\n",
    "        axes[1].imshow(hmap_a, vmin=-1, vmax=1, cmap='seismic')\n",
    "        axes[1].set_title(\"Why did you say: \" + idx_to_label[a_cls] + \"?\")\n",
    "        for ax in axes:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes[0].imshow(two_class_pil_imgs[i][0])\n",
    "        axes[0].set_title(\"Image\")\n",
    "        axes[1].imshow(hmap_b, vmin=-1, vmax=1, cmap='seismic')\n",
    "        axes[1].set_title(\"Why did you say: \" + idx_to_label[b_cls] + \"?\")\n",
    "\n",
    "        print(np.abs(hmap_a - hmap_b).mean())\n",
    "        print(np.sqrt(((hmap_a - hmap_b)**2).mean()))\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow((ab_explain_a - ab_explain_b)[0].sum(-1), cmap='seismic')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "        #plt.imshow(normalize_neg(ab_explain_random[0].sum(-1)), vmin=-1, vmax=1, cmap='seismic')\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zebra_test = preprocess(load_image(\"images/zebra_224.jpg\", 224), innv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(zebra_test - two_class_imgs[1][1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to imagenet validation\n",
    "# imagenet_val_dir = \"/home/leonsixt/tmp/imagenet/imagenet-raw/validation\"\n",
    "imagenet_val_dir = \"/mnt/ssd/data/imagenet/imagenet-raw/validation\"\n",
    "# path to examplary image\n",
    "ex_image_path = \"n01534433/ILSVRC2012_val_00015410.JPEG\"\n",
    "# number of images to run the evaluation\n",
    "n_selected_imgs = 200\n",
    "\n",
    "model_names = ['resnet50', 'vgg16']\n",
    "\n",
    "assert os.path.exists(imagenet_val_dir)\n",
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, innv_net, color_conversion = load_model('vgg16')\n",
    "ex_image_vgg, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "    innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_img, ex_target = val_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtd = innvestigate.create_analyzer('deep_taylor.bounded', model, low=ex_img.min(), high=ex_img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmap = dtd.analyze(ex_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "axes[0].imshow(norm_image(ex_img[0]))\n",
    "axes[0].set_title(\"Image\")\n",
    "hmap_a = normalize_neg(hmap[0].sum(-1))\n",
    "\n",
    "axes[1].imshow(hmap_a, vmin=-1, vmax=1, cmap='seismic')\n",
    "axes[1].set_title(\"Why did you say: \" + idx_to_label[ex_target] + \"?\")\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_class in idx[0][::-1][:20]:\n",
    "    print(probs[0, top_class])\n",
    "    print(top_class)\n",
    "    print(idx_to_label[int(top_class)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray((255*norm_image(ex_img[0])).astype(np.uint8), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_range = (ex_image.min(), ex_image.max())\n",
    "analysers = get_analyser_params(input_range)\n",
    "\n",
    "attr_names = [n for (n, _, _, _, _) in analysers]\n",
    "    \n",
    "hmap_postprocesisng = {\n",
    "    n: lambda x: heatmap_postprocess(post_name, x) for n, _, post_name, _, _ in analysers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(attr_names):\n",
    "    style = mpl_styles[name]\n",
    "    plt.plot(np.arange(10), [20-i] * 10, \n",
    "             #markersize=5,\n",
    "             label=name + \" m=\" + style['marker'], **style)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided BP & Deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GUIDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr_name in ['gradient', 'guided_backprop']:\n",
    "    grad_analyser = innvestigate.create_analyzer(attr_name, model, reverse_keep_tensors=True)\n",
    "    \n",
    "    for img, _ in val_images:\n",
    "        hmap = grad_analyser.analyze(img)\n",
    "\n",
    "        plots = {\n",
    "            #'abs.mean': [],\n",
    "            #'abs.median': [],\n",
    "            'ratio_negative': [],\n",
    "            'ratio_positive': [],\n",
    "            'ratio_zeros': [],\n",
    "        }\n",
    "        for (idx, val) in grad_analyser._reversed_tensors[1:]:\n",
    "            #print(idx, val.shape)\n",
    "            #print(idx, val.mean())\n",
    "            #plots['abs.mean'].append(np.abs(val).mean())\n",
    "            #plots['abs.median'].append(np.abs(val).mean())\n",
    "            plots['ratio_zeros'].append((val == 0).sum() / val.size)\n",
    "            plots['ratio_negative'].append((val < 0).sum() / val.size)\n",
    "            plots['ratio_positive'].append((val > 0).sum() / val.size)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        for name, values in plots.items():\n",
    "            values = [values[i] for i in nice_layer_names[model_name].keys()]\n",
    "            plt.plot(values[::-1], label=name)\n",
    "        plt.ylim(0, 1)\n",
    "        xticks = list(nice_layer_names[model_name].values())\n",
    "        plt.xticks(np.arange(len(xticks)), xticks[::-1], rotation=90)\n",
    "        plt.legend()\n",
    "        plt.title(attr_name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, innv_net, color_conversion = load_model('vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = model.layers[-2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsq = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = np.linalg.eigvals(wsq.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ev[np.argsort(ev)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test guided bp\n",
    "#  1. get grads\n",
    "#  2. compute max(grad)\n",
    "#  3. measure ssim before and after\n",
    "#  4. backprop further\n",
    "#  5. ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for name, values in plots.items():\n",
    "    plt.plot(values, label=name)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks: Random Parameters & Logit Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cascading, _, _ = load_model('resnet50')\n",
    "model_random, _, _ = load_model('resnet50', load_weights=False)\n",
    "model_cascading.set_weights(model.get_weights())\n",
    "out = model.predict(ex_image)\n",
    "out_cascading = model_cascading.predict(ex_image)\n",
    "print(\"mean-l1 distance of the outputs of the trained model and when weights are from trained model [should be 0]:\", np.abs(out_cascading - out).mean())\n",
    "\n",
    "n_layers = len(model_random.layers)\n",
    "copy_weights(model_cascading, model_random, range(n_layers - 3, n_layers))\n",
    "\n",
    "out = model.predict(ex_image)\n",
    "out_cascading = model_cascading.predict(ex_image)\n",
    "print( \"mean-l1 distance of the outputs of the trained model when the last 2 layers are random [should not be 0]:\", np.abs(out_cascading - out).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysers[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmaps are saved in those dicts\n",
    "hmap_original = OrderedDict()\n",
    "hmap_random_weights = OrderedDict()\n",
    "hmap_random_target = OrderedDict()\n",
    "\n",
    "recreate_analyser = False\n",
    "for model_name in tqdm.tqdm_notebook(model_names[::-1]):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    model, innv_net, _ = load_model(model_name)\n",
    "    model_cascading, _, _ = load_model(model_name)\n",
    "    model_random, _, _ = load_model(model_name, load_weights=False)\n",
    "    model_cascading.set_weights(model.get_weights())\n",
    "    get_layer_idx = lambda layer_name: get_layer_idx_full(\n",
    "        model_name, nice_layer_names, layer_name)\n",
    "    \n",
    "    for attr_name, innv_name, _, excludes, analyser_kwargs in tqdm.tqdm_notebook(\n",
    "        analysers[:6], desc=model_name):\n",
    "        \n",
    "        if \"exclude_\" + model_name in excludes:\n",
    "            continue\n",
    "        if innv_name == 'pattern.attribution':\n",
    "            analyser_kwargs['patterns'] = innv_net['patterns']\n",
    "\n",
    "        cascading_heatmaps = {}\n",
    "        cascading_outputs = {}\n",
    "        model_cascading.set_weights(model.get_weights())\n",
    "\n",
    "        original_idx = len(model.layers)\n",
    "\n",
    "        analyzer_cascading = innvestigate.create_analyzer(\n",
    "            innv_name, model_cascading, \n",
    "            neuron_selection_mode=\"index\", **analyser_kwargs)\n",
    "\n",
    "        for img_idx, (img_pp, target) in zip(selected_img_idxs, val_images):\n",
    "            random_target = get_random_target(target)\n",
    "            hmap_random_target[model_name, attr_name, img_idx] = (\n",
    "                random_target, analyzer_cascading.analyze(img_pp, neuron_selection=random_target)[0])\n",
    "        selected_layers = [('original', original_idx)] +  [\n",
    "            (name, get_layer_idx(name)) \n",
    "             for name in randomization_layers[model_name][::-1]\n",
    "        ]\n",
    "        for layer_name, layer_idx in tqdm.tqdm_notebook(selected_layers, desc=attr_name):\n",
    "            copy_weights(model_cascading, model_random, range(layer_idx, original_idx))\n",
    "            if recreate_analyser:\n",
    "                analyzer_cascading = create_analyzer(\n",
    "                    analyser, model_cascading, \n",
    "                    neuron_selection_mode=\"index\",  **analyser_kwargs)\n",
    "\n",
    "            for img_idx, (img_pp, target) in zip(selected_img_idxs, val_images):\n",
    "                hmap = analyzer_cascading.analyze(img_pp, neuron_selection=target)[0]\n",
    "                if layer_idx == original_idx:\n",
    "                    hmap_original[model_name, attr_name, img_idx] = hmap\n",
    "                else:\n",
    "                    hmap_random_weights[model_name, attr_name, img_idx, layer_idx] =  hmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_heatmaps = False\n",
    "\n",
    "if dump_heatmaps:\n",
    "    with open('heatmaps_v4.pickle', 'wb') as f:\n",
    "        pickle.dump((hmap_original, hmap_random_weights, hmap_random_target, analysers), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of dump\n",
    "! ls -lh 'heatmaps.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_heatmaps = False\n",
    "if load_heatmaps:\n",
    "    with open('heatmaps.pickle', 'rb') as f:\n",
    "        hmap_original, hmap_random_weights, hmap_random_target, analysers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attr_names = sorted(set([n for (n, _) in hmap_original.keys()]))\n",
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorting(n):\n",
    "    attr_names_sorting = {\n",
    "     'DTD': -1,\n",
    "     'GuidedBP': -1,\n",
    "     'SmoothGrad': -1,\n",
    "     '$\\\\alpha=1, \\\\beta=0$-LRP': 1,\n",
    "     '$\\\\alpha=2, \\\\beta=1$-LRP': 2,\n",
    "     '$\\\\alpha=100, \\\\beta=99$-LRP': 3,\n",
    "    }\n",
    "    if n in attr_names_sorting:\n",
    "        return attr_names_sorting[n]\n",
    "    elif \"epsilon\" in n:\n",
    "        return 0\n",
    "    elif \"cmp-LRP\" in n:\n",
    "        return 10\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "attr_names = sorted(attr_names, key=lambda x: (get_sorting(x), x))\n",
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim = OrderedDict()\n",
    "l2_random_weights = OrderedDict()\n",
    "\n",
    "last_idx = len(model.layers)\n",
    "for (name, img_idx, layer_idx), heatmap in tqdm.tqdm_notebook(hmap_random_weights.items()):\n",
    "    original_heatmap = hmap_original[(name, img_idx)]\n",
    "    postprocess = hmap_postprocesisng[name]\n",
    "    original_heatmap = postprocess(original_heatmap)\n",
    "    heatmap = postprocess(heatmap)\n",
    "    ssim[(name, img_idx, layer_idx)] = ssim_flipped(heatmap, original_heatmap)\n",
    "    l2_random_weights[(name, img_idx, layer_idx)] = l2_flipped(heatmap, original_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_random_target = OrderedDict()\n",
    "for (name, img_idx), (_, hmap_random) in tqdm.tqdm_notebook(hmap_random_target.items()):\n",
    "    if name not in ssim_random_target:\n",
    "        ssim_random_target[name] = []\n",
    "        \n",
    "    postprocess = hmap_postprocesisng[name]\n",
    "    \n",
    "    hmap = hmap_original[name, img_idx]\n",
    "    original_heatmap = postprocess(original_heatmap)\n",
    "    hmap = postprocess(hmap)\n",
    "    hmap_random = postprocess(hmap_random)\n",
    "    ssim_random_target[name].append(\n",
    "        ssim_flipped(hmap, hmap_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid', {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3.0), squeeze=True)\n",
    "    mean_ssim = [np.mean(s) for s in ssim_random_target.values()]\n",
    "    \n",
    "    names = ssim_random_target.keys()\n",
    "    bars = ax.bar(names, mean_ssim, \n",
    "           color=[linestyles[name]['color'] for name in attr_names])\n",
    "    \n",
    "    xlabels = ssim_random_target.keys()\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_xticks(np.arange(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    \n",
    "    \n",
    "    fig.savefig('check-random-logit.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    display(IFrame('check-random-logit.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_random_target.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('ticks', {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3.0), squeeze=True)\n",
    "    \n",
    "    \n",
    "    xlabels = attr_names\n",
    "    bars = ax.boxplot([ssim_random_target[n] for n in attr_names]) \n",
    "    ax.set_ylabel('SSIM')\n",
    "    #ax.set_xticks(np.arange(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    fig.savefig('check-random-logit-boxplot.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    display(IFrame('check-random-logit-boxplot.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_reduce = 'median'\n",
    "confidence_intervals = True\n",
    "confidence_percentile = 99.5\n",
    "with sns.axes_style(\"ticks\", {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    # metrics = [('SSIM', ssim), ('MSE', l2_random_weights)]\n",
    "    metrics = [('SSIM', ssim)]\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(4.5 * len(metrics), 3.5), squeeze=False)\n",
    "    axes = axes[0]\n",
    "    for ax, (ylabel, metric) in zip(axes, metrics): \n",
    "        for name in attr_names:\n",
    "            metric_per_layer = []\n",
    "            \n",
    "            lower_conf = []\n",
    "            upper_conf = []\n",
    "            for (_, layer_idx) in selected_layers[::-1]:\n",
    "                metric_per_layer.append(\n",
    "                    [metric[(name, img_idx, layer_idx)] for img_idx in selected_img_idxs]\n",
    "                )\n",
    "                if confidence_intervals:\n",
    "                    vals = np.array(metric_per_layer[-1])\n",
    "                    ridx = np.random.choice(len(vals), (10000, len(vals)), replace=True)\n",
    "                    resample = vals[ridx]\n",
    "                    stats = np.median(resample, 1)\n",
    "                    lower_conf.append(np.percentile(stats, 100 - confidence_percentile))\n",
    "                    upper_conf.append(np.percentile(stats, confidence_percentile))\n",
    "                    \n",
    "            metric_per_layer = np.array(metric_per_layer)\n",
    "                \n",
    "            if ssim_reduce == 'mean':\n",
    "                ssims_reduced = metric_per_layer.mean(1)\n",
    "            elif ssim_reduce == 'median':\n",
    "                ssims_reduced = np.median(metric_per_layer, 1)\n",
    "            \n",
    "            ticks = np.arange(len(ssims_reduced))\n",
    "            ax.plot(ticks, ssims_reduced, label=name, **linestyles[name])\n",
    "            ax.fill_between(ticks, lower_conf, upper_conf, \n",
    "                            color=linestyles[name]['color'],\n",
    "                            alpha=0.25\n",
    "                           )\n",
    "            #ax.plot(ticks, lower_conf, color=linestyles[name]['color'])\n",
    "            #ax.plot(ticks, upper_conf, color=linestyles[name]['color'])\n",
    "            \n",
    "        xlabels = [layer_name for layer_name, _ in selected_layers[::-1]] \n",
    "        ax.set_ylim([0, 1.05])\n",
    "        ax.set_xticks(np.arange(len(xlabels)))\n",
    "        ax.set_xticklabels(xlabels, rotation=90)\n",
    "        ax.set_ylabel(ylabel)\n",
    "    axes[-1].legend(bbox_to_anchor=(1.0, 1.00))\n",
    "    plt.savefig('check-random-weights.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    display(IFrame('check-random-weights.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_grid(heatmaps, cols, row_labels=[], column_labels=[], fig_path=None):\n",
    "    mpl.rcParams['font.family'] = 'serif'\n",
    "    rows = len(heatmaps) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows), squeeze=False)\n",
    "    fontsize = 12\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05, top=1, bottom=0, left=0, right=1)\n",
    "    for label, ax in zip(row_labels, axes[:, 0]):\n",
    "        ax.set_ylabel(label, fontsize=fontsize - 1, labelpad=55, rotation=0)\n",
    "        \n",
    "    print(axes.shape, column_labels, row_labels)\n",
    "    for label, ax in zip(column_labels, axes[0, :]):\n",
    "        ax.set_title(label, fontsize=fontsize - 1)\n",
    "        \n",
    "        \n",
    "    for ax, heatmap in zip(axes.flatten(), heatmaps):\n",
    "        ax.imshow(heatmap, cmap='seismic', vmin=-1, vmax=1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    if fig_path is not None:\n",
    "        plt.savefig(fig_path, bbox_inches='tight', pad_inches=0, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_neg(x):\n",
    "    vmax = np.percentile(x, 99)\n",
    "    return np.clip(x / vmax, -1, 1)\n",
    "for model_name in model_names:\n",
    "    hmap_plot = []\n",
    "    rnd_layers = randomization_layers[model_name][::-1]\n",
    "    attr_for_model = []\n",
    "    for (attr_name, _, _, excludes, _) in analysers[:4]:\n",
    "        if 'exclude_' + model_name in excludes:\n",
    "            print(attr_name)\n",
    "            continue\n",
    "        #try:\n",
    "        if True:\n",
    "            postp = hmap_postprocesisng[attr_name]\n",
    "\n",
    "            for img_idx in selected_img_idxs[:1]:\n",
    "                hmap_plot.append(norm_image(val_images[0][0][0]))\n",
    "                hmap_plot.append(normalize_neg(postp(hmap_original[model_name, attr_name, img_idx])))\n",
    "                for layer_name in rnd_layers:\n",
    "                    layer_idx = get_layer_idx_full(model_name, nice_layer_names, layer_name)\n",
    "                    hmap_plot.append(normalize_neg(postp(hmap_random_weights[model_name, attr_name, img_idx, layer_idx])))\n",
    "            attr_for_model.append(attr_name)\n",
    "        # except KeyError as e:\n",
    "        #     print(dir(e))\n",
    "        #     print(e)\n",
    "        #     pass\n",
    "    \n",
    "    plot_heatmap_grid(\n",
    "        hmap_plot, 2+len(rnd_layers), row_labels=attr_for_model, \n",
    "        column_labels=['input', 'original'] + rnd_layers,\n",
    "        fig_path='heatmap_grid.pdf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('heatmap_grid.pdf', width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hmap_original.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop ipynb execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ssim.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'DTD'\n",
    "layer_idx = selected_layers[4][1]\n",
    "ssim_per_layer = []\n",
    "for img_idx in selected_img_idxs:\n",
    "    ssim_per_layer.append(ssim[method, img_idx, layer_idx])\n",
    "    \n",
    "ssim_per_layer = np.array(ssim_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "medians = []\n",
    "\n",
    "ridx = np.random.choice(n_selected_imgs, (10000, n_selected_imgs), replace=True)\n",
    "\n",
    "means = ssim_per_layer[ridx].mean(1)\n",
    "median = np.median(ssim_per_layer[ridx], 1)\n",
    "print(np.percentile(median, 99))\n",
    "print(np.percentile(median, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance w.r.t. Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_idx(neuron_selection, batch_size=1):\n",
    "    neuron_selection = np.asarray(neuron_selection).flatten()\n",
    "    if neuron_selection.size == 1:\n",
    "        neuron_selection = np.repeat(neuron_selection, batch_size)\n",
    "\n",
    "    # Add first axis indices for gather_nd\n",
    "    neuron_selection = np.hstack(\n",
    "        (np.arange(len(neuron_selection)).reshape((-1, 1)),\n",
    "         neuron_selection.reshape((-1, 1)))\n",
    "    )\n",
    "    return neuron_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_wrt_kernels = OrderedDict()\n",
    "for name, innv_name, innv_kwargs in analysers:\n",
    "    print(name)\n",
    "    analyzer = create_analyzer(\n",
    "        innv_name, model_wo_softmax, neuron_selection_mode=\"index\", **innv_kwargs)\n",
    "    analyzer.create_analyzer_model()\n",
    "    try:\n",
    "        analyzer_model = analyzer._analyzer_model\n",
    "    except:\n",
    "        continue\n",
    "    input_image, input_idx = analyzer_model.inputs\n",
    "\n",
    "    rel_mean = tf.reduce_sum(analyzer_model.outputs[0])\n",
    "    rel_grad_w = tf.gradients(\n",
    "        rel_mean,\n",
    "        analyzer_model.weights,\n",
    "    )\n",
    "    sess = keras.backend.get_session()\n",
    "    outs = sess.run(analyzer._analyzer_model.outputs + rel_grad_w, \n",
    "                    {input_image: ex_image, input_idx: prepare_idx(ex_target)})\n",
    "    \n",
    "    output = outs[0]\n",
    "    rel_grad_w_np = outs[1:]\n",
    "    print(len(rel_grad_w_np))\n",
    "    for i, w in enumerate(analyzer_model.weights):\n",
    "        if \"kernel\" in w.name:\n",
    "            #print(i, w.name)\n",
    "            rel_wrt_kernels[name, ex_image_idx, i, w.name.rstrip(\"/kernel:0\")] = rel_grad_w_np[i]\n",
    "\n",
    "    print(output.min(), output.max())\n",
    "    plt.imshow(normalize(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _, _) in analysers:\n",
    "    grads = []\n",
    "    percentile = 99\n",
    "    lower_percentile = []\n",
    "    upper_percentile = []\n",
    "    layer_names = []\n",
    "    for (gname, _, idx, layer_name), grad in rel_wrt_kernels.items():\n",
    "        if name == gname:\n",
    "            grads.append(np.abs(grad).mean())\n",
    "            lower_percentile.append(np.percentile(np.abs(grad), 100-percentile))\n",
    "            upper_percentile.append(np.percentile(np.abs(grad), percentile))\n",
    "            layer_names.append(layer_name)\n",
    "            \n",
    "    plt.title(name)\n",
    "    plt.semilogy(grads)\n",
    "    plt.semilogy(lower_percentile)\n",
    "    plt.semilogy(upper_percentile)\n",
    "    plt.ylim(1e2, 1e-16)\n",
    "    plt.xticks(ticks=np.arange(len(rel_wrt_kernels)), labels=layer_names, rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _, _) in analysers:\n",
    "    grads = []\n",
    "    percentile = 99\n",
    "    lower_percentile = []\n",
    "    upper_percentile = []\n",
    "    layer_names = []\n",
    "    layer_idxs = []\n",
    "    \n",
    "    n_layers = 0\n",
    "    i = 0\n",
    "    for ((gname, _, idx, layer_name), grad) in rel_wrt_kernels.items():\n",
    "        if name == gname:\n",
    "            grads.append(np.abs(grad).flatten())\n",
    "            layer_idxs.append(i*np.ones_like(grads[-1]))\n",
    "            layer_names.append(layer_name)\n",
    "            i += 1\n",
    "          \n",
    "    if len(grads) == 0:\n",
    "        continue\n",
    "     \n",
    "    layer_idxs = np.concatenate(layer_idxs)\n",
    "    grads = np.concatenate(grads)\n",
    "    print('.')\n",
    "    \n",
    "    idx = np.random.choice(len(grads), size=100000)\n",
    "    print('.')\n",
    "    \n",
    "    plt.title(name)\n",
    "    plt.scatter(layer_idxs[idx], grads[idx], marker='_', alpha=0.1)\n",
    "    plt.yscale('log')\n",
    "    plt.ylim([1000, 1e-8])\n",
    "    plt.xticks(ticks=np.arange(len(layer_names)), labels=layer_names, rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _, _) in analysers:\n",
    "    grads = []\n",
    "    layer_names = []\n",
    "    for (gname, _, idx, layer_name), grad in rel_wrt_kernels.items():\n",
    "        if name == gname:\n",
    "            grads.append((np.abs(grad) == 0).sum())\n",
    "            layer_names.append(layer_name)\n",
    "            \n",
    "    plt.title(name)\n",
    "    plt.scatter(np.arange(len(grads)), grads)\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(1e9, 1)\n",
    "    plt.xticks(ticks=np.arange(len(grads)), labels=layer_names, rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
