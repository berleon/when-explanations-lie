{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Explanations Lie: Why Modified BP Attribution fails\n",
    "\n",
    "This notebook produces the heatmap figures and the ssim results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install install packages\n",
    "#!pip install tensorflow-gpu==1.13.1\n",
    "#!pip install innvestigate seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "import innvestigate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import copy\n",
    "import contextlib\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage.measure import compare_ssim \n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "from innvestigate.analyzer.relevance_based.relevance_analyzer import LRP\n",
    "from innvestigate.analyzer.base import AnalyzerNetworkBase, ReverseAnalyzerBase\n",
    "from innvestigate.analyzer.deeptaylor import DeepTaylor\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_model(self, model):\n",
    "    return super(DeepTaylor, self)._prepare_model(model)\n",
    "\n",
    "# otherwise DTD does not work on negative outputs\n",
    "DeepTaylor._prepare_model = _prepare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to imagenet validation\n",
    "# imagenet_val_dir = \"/home/leonsixt/tmp/imagenet/imagenet-raw/validation\"\n",
    "imagenet_val_dir = \"/mnt/ssd/data/imagenet/imagenet-raw/validation\"\n",
    "# path to examplary image\n",
    "ex_image_path = \"n01534433/ILSVRC2012_val_00015410.JPEG\"\n",
    "# number of images to run the evaluation\n",
    "n_selected_imgs = 10\n",
    "\n",
    "model_names = ['resnet50', 'vgg16']\n",
    "\n",
    "assert os.path.exists(imagenet_val_dir)\n",
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_paths = load_image_paths(imagenet_val_dir)\n",
    "ex_image_full_path, ex_target = [\n",
    "    (path, target) for (path, target) in val_paths \n",
    "    if path.endswith(ex_image_path)][0]\n",
    "ex_image_idx = val_paths.index((ex_image_full_path, ex_target))\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "selected_img_idxs = [ex_image_idx] + np.random.choice([idx for idx in range(len(val_paths))\n",
    "                                                       if idx != ex_image_idx], n_selected_imgs - 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, innv_net, color_conversion = load_model('vgg16')\n",
    "ex_image_vgg, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "    innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model, innv_net, color_conversion = load_model('resnet50')\n",
    "ex_image, ex_target, val_images, selected_img_idxs = load_val_images(\n",
    "    innv_net, imagenet_val_dir, ex_image_path, n_selected_imgs)\n",
    "\n",
    "\n",
    "assert ((ex_image - ex_image_vgg) == 0).all()\n",
    "\n",
    "nice_layer_names = get_nice_layer_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = {'vgg16': 23, 'resnet50': 177}\n",
    "\n",
    "randomization_layers = {\n",
    "    'vgg16': [\"conv1_1\", \"conv2_1\",  \"conv3_1\", \"conv4_1\",  \"conv4_3\",  \"conv5_1\", \"conv5_3\", \"fc1\", \"fc3\"],\n",
    "    'resnet50': ['conv1', 'block2_2', 'block3_1', 'block3_3', 'block4_1', 'block4_6', 'block5_2', 'dense'],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shapes = get_output_shapes(model)\n",
    "\n",
    "print_output_shapes = False \n",
    "if print_output_shapes: \n",
    "    print(\"{:3}{:20}{:20}{}\".format(\"l\", \"layer\", \"input_at_0\", \"output_shape\"))\n",
    "    for i in range(len(model.layers)):\n",
    "        layer = model.get_layer(index=i)\n",
    "        print(\"{:3}: {:20}  {:20}  {}\".format(\n",
    "            i, layer.name, str(layer.get_input_shape_at(0)), str(output_shapes[i])))\n",
    "        #print(\"{:3}: {:20}  {:20}  {}\".format(i, type(layer).__name__, layer.name, output_shapes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmap_postprocess_wrapper(name):\n",
    "    return lambda x: heatmap_postprocess(name, x)\n",
    "\n",
    "input_range = (ex_image.min(), ex_image.max())\n",
    "analysers = get_analyser_params(input_range)\n",
    "\n",
    "attr_names = [n for (n, _, _, _, _) in analysers]\n",
    "    \n",
    "hmap_postprocessing = {\n",
    "    n: hmap_postprocess_wrapper(post_name) for n, _, post_name, _, _ in analysers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(attr_names):\n",
    "    style = mpl_styles[name]\n",
    "    plt.plot(np.arange(10), [20-i] * 10, \n",
    "             #markersize=5,\n",
    "             label=name + \" m=\" + style['marker'], **style)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks: Random Parameters & Logit Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cascading, _, _ = load_model('resnet50')\n",
    "model_random, _, _ = load_model('resnet50', load_weights=False)\n",
    "model_cascading.set_weights(model.get_weights())\n",
    "out = model.predict(ex_image)\n",
    "out_cascading = model_cascading.predict(ex_image)\n",
    "print(\"mean-l1 distance of the outputs of the trained model and when weights are from trained model [should be 0]:\", np.abs(out_cascading - out).mean())\n",
    "\n",
    "n_layers = len(model_random.layers)\n",
    "copy_weights(model_cascading, model_random, range(n_layers - 3, n_layers))\n",
    "\n",
    "out = model.predict(ex_image)\n",
    "out_cascading = model_cascading.predict(ex_image)\n",
    "print( \"mean-l1 distance of the outputs of the trained model when the last 2 layers are random [should not be 0]:\", np.abs(out_cascading - out).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmaps are saved in those dicts\n",
    "hmap_original = OrderedDict()\n",
    "hmap_random_weights = OrderedDict()\n",
    "hmap_random_target = OrderedDict()\n",
    "\n",
    "recreate_analyser = False\n",
    "for model_name in tqdm.tqdm_notebook(model_names[::-1]):\n",
    "    get_layer_idx = lambda layer_name: get_layer_idx_full(\n",
    "        model_name, nice_layer_names, layer_name)\n",
    "    \n",
    "    for i, (attr_name, innv_name, _, excludes, analyser_kwargs) in enumerate(tqdm.tqdm_notebook(\n",
    "        analysers, desc=model_name)):\n",
    "        if i % 5 == 0:\n",
    "            # clear session from time to time to not OOM\n",
    "            keras.backend.clear_session()\n",
    "\n",
    "            model, innv_net, _ = load_model(model_name)\n",
    "            model_cascading, _, _ = load_model(model_name)\n",
    "            model_random, _, _ = load_model(model_name, load_weights=False)\n",
    "            model_cascading.set_weights(model.get_weights())\n",
    "            \n",
    "        if \"exclude_\" + model_name in excludes:\n",
    "            continue\n",
    "        if innv_name == 'pattern.attribution':\n",
    "            analyser_kwargs['patterns'] = innv_net['patterns']\n",
    "\n",
    "        cascading_heatmaps = {}\n",
    "        cascading_outputs = {}\n",
    "        model_cascading.set_weights(model.get_weights())\n",
    "\n",
    "        original_idx = len(model.layers)\n",
    "\n",
    "        analyzer_cascading = innvestigate.create_analyzer(\n",
    "            innv_name, model_cascading, \n",
    "            neuron_selection_mode=\"index\", **analyser_kwargs)\n",
    "\n",
    "        for img_idx, (img_pp, target) in zip(selected_img_idxs, val_images):\n",
    "            random_target = get_random_target(target)\n",
    "            hmap_random_target[model_name, attr_name, img_idx] = (\n",
    "                random_target, analyzer_cascading.analyze(img_pp, neuron_selection=random_target)[0])\n",
    "        selected_layers = [('original', original_idx)] +  [\n",
    "            (name, get_layer_idx(name)) \n",
    "             for name in randomization_layers[model_name][::-1]\n",
    "        ]\n",
    "        for layer_name, layer_idx in tqdm.tqdm_notebook(selected_layers, desc=attr_name):\n",
    "            copy_weights(model_cascading, model_random, range(layer_idx, original_idx))\n",
    "            if recreate_analyser:\n",
    "                analyzer_cascading = create_analyzer(\n",
    "                    analyser, model_cascading, \n",
    "                    neuron_selection_mode=\"index\",  **analyser_kwargs)\n",
    "\n",
    "            for img_idx, (img_pp, target) in zip(selected_img_idxs, val_images):\n",
    "                hmap = analyzer_cascading.analyze(img_pp, neuron_selection=target)[0]\n",
    "                if layer_idx == original_idx:\n",
    "                    hmap_original[model_name, attr_name, img_idx] = hmap\n",
    "                else:\n",
    "                    hmap_random_weights[model_name, attr_name, img_idx, layer_idx] =  hmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_heatmaps = False\n",
    "\n",
    "if dump_heatmaps:\n",
    "    with open('heatmaps_v4.pickle', 'wb') as f:\n",
    "        pickle.dump((hmap_original, hmap_random_weights, hmap_random_target, analysers), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of dump\n",
    "! ls -lh 'heatmaps.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_heatmaps = False\n",
    "if load_heatmaps:\n",
    "    with open('heatmaps.pickle', 'rb') as f:\n",
    "        hmap_original, hmap_random_weights, hmap_random_target, analysers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attr_names = sorted(set([n for (n, _) in hmap_original.keys()]))\n",
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorting(n):\n",
    "    attr_names_sorting = {\n",
    "     'DTD': -1,\n",
    "     'GuidedBP': -1,\n",
    "     'SmoothGrad': -1,\n",
    "     '$\\\\alpha=1, \\\\beta=0$-LRP': 1,\n",
    "     '$\\\\alpha=2, \\\\beta=1$-LRP': 2,\n",
    "     '$\\\\alpha=100, \\\\beta=99$-LRP': 3,\n",
    "    }\n",
    "    if n in attr_names_sorting:\n",
    "        return attr_names_sorting[n]\n",
    "    elif \"epsilon\" in n:\n",
    "        return 0\n",
    "    elif \"cmp-LRP\" in n:\n",
    "        return 10\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "attr_names = sorted(attr_names, key=lambda x: (get_sorting(x), x))\n",
    "attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim = OrderedDict()\n",
    "l2_random_weights = OrderedDict()\n",
    "\n",
    "last_idx = len(model.layers)\n",
    "for (name, img_idx, layer_idx), heatmap in tqdm.tqdm_notebook(hmap_random_weights.items()):\n",
    "    original_heatmap = hmap_original[(name, img_idx)]\n",
    "    postprocess = hmap_postprocesisng[name]\n",
    "    original_heatmap = postprocess(original_heatmap)\n",
    "    heatmap = postprocess(heatmap)\n",
    "    ssim[(name, img_idx, layer_idx)] = ssim_flipped(heatmap, original_heatmap)\n",
    "    l2_random_weights[(name, img_idx, layer_idx)] = l2_flipped(heatmap, original_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_random_target = OrderedDict()\n",
    "for (name, img_idx), (_, hmap_random) in tqdm.tqdm_notebook(hmap_random_target.items()):\n",
    "    if name not in ssim_random_target:\n",
    "        ssim_random_target[name] = []\n",
    "        \n",
    "    postprocess = hmap_postprocesisng[name]\n",
    "    \n",
    "    hmap = hmap_original[name, img_idx]\n",
    "    original_heatmap = postprocess(original_heatmap)\n",
    "    hmap = postprocess(hmap)\n",
    "    hmap_random = postprocess(hmap_random)\n",
    "    ssim_random_target[name].append(\n",
    "        ssim_flipped(hmap, hmap_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid', {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3.0), squeeze=True)\n",
    "    mean_ssim = [np.mean(s) for s in ssim_random_target.values()]\n",
    "    \n",
    "    names = ssim_random_target.keys()\n",
    "    bars = ax.bar(names, mean_ssim, \n",
    "           color=[linestyles[name]['color'] for name in attr_names])\n",
    "    \n",
    "    xlabels = ssim_random_target.keys()\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_xticks(np.arange(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    \n",
    "    \n",
    "    fig.savefig('check-random-logit.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    display(IFrame('check-random-logit.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_random_target.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('ticks', {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3.0), squeeze=True)\n",
    "    \n",
    "    \n",
    "    xlabels = attr_names\n",
    "    bars = ax.boxplot([ssim_random_target[n] for n in attr_names]) \n",
    "    ax.set_ylabel('SSIM')\n",
    "    #ax.set_xticks(np.arange(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    fig.savefig('check-random-logit-boxplot.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    display(IFrame('check-random-logit-boxplot.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_reduce = 'median'\n",
    "confidence_intervals = True\n",
    "confidence_percentile = 99.5\n",
    "with sns.axes_style(\"ticks\", {\"axes.grid\": True, 'font.family': 'serif'}):\n",
    "    # metrics = [('SSIM', ssim), ('MSE', l2_random_weights)]\n",
    "    metrics = [('SSIM', ssim)]\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(4.5 * len(metrics), 3.5), squeeze=False)\n",
    "    axes = axes[0]\n",
    "    for ax, (ylabel, metric) in zip(axes, metrics): \n",
    "        for name in attr_names:\n",
    "            metric_per_layer = []\n",
    "            \n",
    "            lower_conf = []\n",
    "            upper_conf = []\n",
    "            for (_, layer_idx) in selected_layers[::-1]:\n",
    "                metric_per_layer.append(\n",
    "                    [metric[(name, img_idx, layer_idx)] for img_idx in selected_img_idxs]\n",
    "                )\n",
    "                if confidence_intervals:\n",
    "                    vals = np.array(metric_per_layer[-1])\n",
    "                    ridx = np.random.choice(len(vals), (10000, len(vals)), replace=True)\n",
    "                    resample = vals[ridx]\n",
    "                    stats = np.median(resample, 1)\n",
    "                    lower_conf.append(np.percentile(stats, 100 - confidence_percentile))\n",
    "                    upper_conf.append(np.percentile(stats, confidence_percentile))\n",
    "                    \n",
    "            metric_per_layer = np.array(metric_per_layer)\n",
    "                \n",
    "            if ssim_reduce == 'mean':\n",
    "                ssims_reduced = metric_per_layer.mean(1)\n",
    "            elif ssim_reduce == 'median':\n",
    "                ssims_reduced = np.median(metric_per_layer, 1)\n",
    "            \n",
    "            ticks = np.arange(len(ssims_reduced))\n",
    "            ax.plot(ticks, ssims_reduced, label=name, **linestyles[name])\n",
    "            ax.fill_between(ticks, lower_conf, upper_conf, \n",
    "                            color=linestyles[name]['color'],\n",
    "                            alpha=0.25\n",
    "                           )\n",
    "            #ax.plot(ticks, lower_conf, color=linestyles[name]['color'])\n",
    "            #ax.plot(ticks, upper_conf, color=linestyles[name]['color'])\n",
    "            \n",
    "        xlabels = [layer_name for layer_name, _ in selected_layers[::-1]] \n",
    "        ax.set_ylim([0, 1.05])\n",
    "        ax.set_xticks(np.arange(len(xlabels)))\n",
    "        ax.set_xticklabels(xlabels, rotation=90)\n",
    "        ax.set_ylabel(ylabel)\n",
    "    axes[-1].legend(bbox_to_anchor=(1.0, 1.00))\n",
    "    plt.savefig('check-random-weights.pdf',  bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    display(IFrame('check-random-weights.pdf', 800, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_grid(heatmaps, cols, row_labels=[], column_labels=[], fig_path=None):\n",
    "    mpl.rcParams['font.family'] = 'serif'\n",
    "    rows = len(heatmaps) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows), squeeze=False)\n",
    "    fontsize = 12\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05, top=1, bottom=0, left=0, right=1)\n",
    "    for label, ax in zip(row_labels, axes[:, 0]):\n",
    "        ax.set_ylabel(label, fontsize=fontsize + 1, labelpad=55, rotation=0)\n",
    "        \n",
    "    print(axes.shape, column_labels, row_labels)\n",
    "    for label, ax in zip(column_labels, axes[0, :]):\n",
    "        ax.set_title(label, fontsize=fontsize)\n",
    "        \n",
    "        \n",
    "    for ax, heatmap in zip(axes.flatten(), heatmaps):\n",
    "        ax.imshow(heatmap, cmap='seismic', vmin=-1, vmax=1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    if fig_path is not None:\n",
    "        plt.savefig(fig_path, bbox_inches='tight', pad_inches=0, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_neg(x):\n",
    "    vmax = np.percentile(x, 99)\n",
    "    vmin = np.percentile(x, 1)\n",
    "    vmax\n",
    "    x_pos = x * (x > 0)\n",
    "    x_neg = x * (x < 0)\n",
    "    \n",
    "    x_pos = x_pos / vmax\n",
    "    x_neg = - x_neg / vmin\n",
    "    return np.clip(x_pos + x_neg, -1, 1)\n",
    "\n",
    "for model_name in model_names:\n",
    "    hmap_plot = []\n",
    "    rnd_layers = randomization_layers[model_name][::-1]\n",
    "    attr_for_model = []\n",
    "    for (attr_name, _, _, excludes, _) in analysers:\n",
    "        if 'exclude_' + model_name in excludes:\n",
    "            print(attr_name)\n",
    "            continue\n",
    "        try:\n",
    "            if attr_name in ['GuidedBP', 'Deconv']:\n",
    "                postp = hmap_postprocess_wrapper('sum')\n",
    "            else:\n",
    "                postp = hmap_postprocessing[attr_name]\n",
    "\n",
    "            for img_idx in selected_img_idxs[:1]:\n",
    "                hmap_plot.append(norm_image(val_images[0][0][0]))\n",
    "                hmap_plot.append(normalize_neg(postp(hmap_original[model_name, attr_name, img_idx])))\n",
    "                for layer_name in rnd_layers:\n",
    "                    layer_idx = get_layer_idx_full(model_name, nice_layer_names, layer_name)\n",
    "                    hmap_plot.append(normalize_neg(postp(hmap_random_weights[model_name, attr_name, img_idx, layer_idx])))\n",
    "            attr_for_model.append(attr_name)\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    plot_heatmap_grid(\n",
    "        hmap_plot, 2+len(rnd_layers), row_labels=attr_for_model, \n",
    "        column_labels=['input', 'original'] + rnd_layers,\n",
    "        fig_path='figures/heatmap_grid_{}.pdf'.format(model_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('figures/heatmap_grid_vgg16.pdf', width=1000, height=600))\n",
    "display(IFrame('figures/heatmap_grid_resnet50.pdf', width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_name = 'LRP-$\\\\alpha=5, \\\\beta=4$'\n",
    "attr_name = 'PatternAttr.'\n",
    "hmap = hmap_random_weights['vgg16', attr_name, 673, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
